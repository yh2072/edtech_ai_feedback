# The association between groups' interactions with the Visual-GenAI learning analytics feedback and student engagement in CSCL

Xinghan Yin a, Junmin Ye b,\*, Shuang Yu a, Honghui Li c, Qingtang Liu a,d, Gang Zhao a

a Faculty of Artificial Intelligence in Education, Central China Normal University, Wuhan, Hubei, 430079, China  b School of Computer, Central China Normal University, Wuhan, Hubei, 430079, China  c School of Educational Technology, Beijing Normal University, Beijing, 100091, China  d Key Laboratory of Digital Education of Hubei Province, Central China Normal University, Wuhan, 430079, China

# ARTICLEINFO

# ABSTRACT

Keywords: Cooperative/collaborative learning  Distance education and online learning  Data science applications in education  Evaluation methodologies

Promoting student engagement has long been a vital subject in the research of Computer- Supported Collaborative Learning (CSCL). Previous research has indicated the potential of AI- based visual learning analytics feedback and generative AI (GenAI) feedback in this context. However, there is currently a lack of definitive research on the combined impact of these two types of intelligent feedback in CSCL. Additionally, limited attention has been paid to how groups utilize these tools in CSCL practice and the differences that may exist. In this study, we developed an Visual- GenAI learning analytics feedback tool that integrates AI- based visual learning analytics feedback and GenAI- based feedback. We then evaluated the differences in groups' interactions with this Visual- GenAI learning analytics feedback and its association with student engagement and academic performance. The study employed a mixed- methods approach, combining quantitative analysis of feedback interaction log data, content analysis of group discussion data, and qualitative analysis of students' perceptions of different feedback tools through surveys. Our results show that groups exhibit four distinct levels of feedback interaction behavior patterns with the Visual- GenAI learning analytics feedback. These four patterns exhibit significant differences in behavioral engagement, emotional engagement, cognitive engagement, and academic performance. This study's significance lies in its potential contribution to future research on examining group behavior and optimizing learning using AI- based visual learning analytics feedback and GenAI- based feedback.

# 1. Introduction

Identifying and changing learning environments in traditional classrooms is challenging (Kao & Ruan, 2022). In contrast, Computer- Supported Collaborative Learning (CSCL), one of the most promising innovations backed by modern computer- mediated information and communication technologies, improves student collaboration and learning activities. In CSCL, group members focus on unstructured problems, express and integrate views to promote knowledge construction and provide solutions or create

products (Gu et al., 2015; Molinillo et al., 2018). Recently, CSCL has been widely used in higher education, ranging from general collaborative settings like forums to domain- specific tools (Kaliisa et al., 2025; Lock & Redmond, 2021). During the COVID- 19 pandemic, blended teaching with CSCL showed more flexible learning approaches than traditional classrooms, enabling students to complement classroom experiences through online interaction (Ortega- Arranz et al., 2024), and enhancing students' academic performance (Ng et al., 2022; Northey et al., 2018; Qureshi et al., 2023). However, not all students actively participate in CSCL as maintaining effective collaboration requires high intrinsic and extrinsic cognitive loads (Kirschner, 2009). High cognitive loads can disrupt teamwork and cause students to struggle with engagement (Liu et al., 2018). For example, students may "free- riding" (Van den Bossche, Gijselaers, Segers, & Kirschner, 2006), have low cognitive engagement levels (Lin et al., 2014), or lack emotional engagement (Capdeferro & Romero, 2012), all of which can affect collaborative learning and academic performance (Hu et al., 2025). Meanwhile, with educational technology advancement, students now expect technology- enhanced flexibility, personalized support, and instant feedback regardless of delivery mode, expectations traditional methods find hard to meet at scale (Regnier et al., 2024). Educational institutions thus face growing pressure to develop adaptive CSCL environments while maintaining quality and student engagement (Lee & Wu, 2025).

In this evolving edtech landscape, artificial intelligence (AI) and Learning analysis (LA) are driving CSCL research forward through innovative learning strategies and environments (Kaliisa et al., 2025). At the core of LA lies the systematic collection, processing, analysis, and reporting of learning- process data to enhance understanding and optimize learning and its environment (Long et al., 2011). Recent LA progress, especially visual learning analytics feedback and similar tools, has effectively boosted student engagement in CSCL (Ouyang & Zhang, 2024; Zheng, Long, et al., 2023), improving group awareness and academic performance (Bao et al., 2021; Kong et al., 2025; Peng et al., 2022). AI, with its ability to handle complex data, identify patterns, and enable automation, is the key driver of current visual learning analytics feedback (Khosravi et al., 2021). These AI- based tools automatically collect, analyze, and visualize collaborative metrics and participation levels, offering students insights into their learning behaviors and outcomes, and having a far- reaching impact on collaborative learning (Han et al., 2021; Jivet et al., 2021). However, recent studies indicate that presenting analytical outputs in a student- friendly manner is a major challenge in designing visual learning analytics feedback tools (Alfredo et al., 2024). Most students have low data visualization literacy, and without interpretive support, such feedback may increase cognitive load (Moraes Bueno Rodrigues, Diniz Junqueira Barbosa, Cortes Vieira Lopes, Diniz, & Barbosa, 2021; Cheng et al., 2024). As LA technologies develop, the information complexity presented by these tools is increasing, further intensifying these challenges (Vieira et al., 2018; Tretow- Fish & Khalid, 2023). Therefore, there is a growing need for support to ensure students can benefit from visual learning analytics feedback and unlock its educational potential (Yan et al., 2025).

The rise of Generative AI (GenAI) offers a promising solution to the above challenges and brings new educational potential to visual learning analytics feedback (Yan, Martinez- Maldonado, & Gasevic, 2024). Unlike traditional AI models, GenAI, such as large language models like GPT, is pretrained on massive internet data. It exceeds in deep semantic understanding and provides user- friendly interfaces that allow language as both input and output (Cress & Kimmerle, 2023). GenAI can parse unstructured data like collaborative dialogues and text records based on customized prompts, offering more detailed and targeted explanations that make complex learning data easier for students to understand (Hutt et al., 2024; Tang et al., 2024; Tsai et al., 2024). More importantly, GenAI can dynamically generate specific and actionable learning suggestions or instructional strategies based on real- time analysis of learning process data (Deng et al., 2024). The feedback from GenAI is expected to bridge the gap between data and useful insights, making visual learning analytics feedback more meaningful and actionable (Yan et al., 2025). Some empirical studies also indicate that GenAI- based learning analytics feedback, such as that from ChatGPT, significantly enhances student engagement (Teng, 2024) and academic performance (Guo et al., 2024; Tsai et al., 2024).

While GenAI offers promising opportunities to address the limitations of visual learning analytics feedback (Susnjak, 2024), it also has certain limitations. One key barrier is the hallucination phenomenon, where GenAI tends to generate clear but inaccurate content due to training data bias and probabilistic generation mechanisms (Huang et al., 2025). Meanwhile, students' expectations for technology- enhanced learning have fundamentally changed (Yilmaz et al., 2024; Lee & Wu, 2025). Students have higher expectations for GenAI feedback, and when there is a mismatch between GenAI outputs and their expectations, they interact with the feedback less frequently (Yilmaz et al., 2024; Jin, Yang, et al., 2025). In addition, there are variations in students' continuous access to and use of visual learning analytics feedback tools (Sansom et al., 2020; Zamecnik et al., 2022). Differences in technical characteristics may further intensify this divergence in interaction. GenAI feedback can provide direct learning guidance based on data (Susnjak, 2024), but it is inferior to visual learning analytics feedback in intuitive data visualization (Ye et al., 2024). However, empirical evidence on student engagement with the integration of visual learning analytics feedback and GenAI feedback remains limited (Yan, Zhao, et al., 2024). The latest progress comes from Yan et al.'s (2025) quasi- experimental study, which shows that GenAI can effectively support students in using visual learning analytics feedback. Yet, their focus is not on students' feedback engagement levels. There is still a lack of research on collecting and tracking data to evaluate differences in student interaction with feedback and its potential impacts on CSCL (Kaliisa et al., 2024; Zamecnik et al., 2022). Therefore, there is an urgent need for more research to differentiate groups' interaction behaviors with learning- analytics feedback and explore the associations between these behavioral differences and student engagement and academic performance in CSCL. This can not only help optimize the design of learning analytics feedback but also enhance the effectiveness of relevant tools (Park et al., 2023; Roberts et al., 2017).

To address these research gaps, this study developed an Visual- GenAI learning analytics feedback tool that integrates AI- based visual learning analytics feedback with GenAI feedback. The AI- based visual learning analytics feedback offers students a direct view of the collaborative learning process (Ouyang & Zhang, 2024), while GenAI feedback gives more detailed learning suggestions and guidance (Lee et al., 2024). We then focused on collecting tracking data to evaluate the patterns of groups' interactions with this Visual- GenAI learning analytics feedback and its association with student engagement and academic performance. Based on empirical

results, we enriched the theoretical and practical underpinnings of GenAI in optimizing collaborative learning analytics feedback.

# 2. Literature review

# 2.1. Student engagement in CSCL

The core of CsCL resides in the interactions among group members, with successful engagement in this format being integral to the learning process (Northey et al., 2018; Perez- Lopez et al., 2020). Particularly in complex learning tasks, the complementary engagement of group members can enhance the efficiency of task completion (Dowell et al., 2020; He et al., 2023). Student engagement refers to the level of students' commitment and active involvement in learning activities (Coates, 2006; Ng et al., 2022). Fredricks et al. (2004) identified three widely accepted dimensions of engagement: behavioral engagement, emotional engagement, and cognitive engagement. Previous research has conceptualized behavioral engagement in CsCL as the specific actions students undertake in relation to collaborative task activities (Paneth et al., 2024). Emotional engagement is manifest in students' affective responses to the learning content, experiences, and peers, as well as in the maintenance of a harmonious group atmosphere (Hu et al., 2025; Peng et al., 2022). Cognitive engagement is reflected in the level of students' investment in cognitive activities such as knowledge construction, problem- solving, and argumentation around shared goals (Sinha et al., 2015; Zhou & Ye, 2024). Prior research on student engagement in CsCL has provided evidence of its impact on academic performance. For example, Khosa and Volet (2014) found a positive association between effective student engagement in group collaborative learning activities and conceptual understanding. Zhao et al. (2024) discovered that student engagement in CsCL significantly influences their academic performance.

Although student engagement is central to academic performance in CsCL, not every group member can sustain interaction with others or maintain high engagement over time (Li et al., 2025). CsCL is a complex system in which engagement depends on the extent to which all members participate in collaborative activities to achieve intended outcomes, and group members often struggle to maintain behavioral, cognitive, and emotional engagement (Zheng, Long, et al., 2023). Behaviorally, communication among students is frequently ineffective when solving collaborative tasks because many lack the necessary regulatory skills (Rojas et al., 2022), and some members exhibit social loafing, with unequal participation leading to free riding or disengagement (Van den Bossche et al., 2006; Loughry et al., 2007; Zheng et al., 2022). Emotionally, negative emotions or confusion that are not identified and addressed in time can impair group climate and the overall learning process, becoming barriers to collaboration (Tömmlen et al., 2021; Ye & Zhou, 2022). Students may also experience loneliness and anxiety in CsCL, resulting in relatively lower cognitive engagement in collaborative knowledge construction (Lin et al., 2014; Hou et al., 2015; Ma et al., 2023). Many students often post off- topic messages or get lost in online discussions, making it difficult to reach consensus toward shared goals (Chen et al., 2021; Wu, 2022). To enhance student engagement, groups usually require instructional support (Rojas et al., 2022), yet teachers find it difficult to track multiple groups simultaneously and provide timely feedback and support (Bikowski & Vithanage, 2016; Chen et al., 2022; Schwarz et al., 2021). Recent work on socially shared regulation of learning argues for raising group awareness so that members understand individual contributions and the overall group process, and then co- regulate their collaborative engagement (Jarvela & Hadwin, 2024; Li et al., 2025; Schnaubert & Bodemer, 2022). Group awareness is defined as students' knowledge about the current state of their peers and the group (Buder et al., 2021; Janssen et al., 2011). Prior studies show that supporting group awareness helps coordinate and improve collaborative activities (Miller & Hadwin, 2015), promotes equal participation (Pifarré et al., 2014), and enhances groups' performance (Melanie & Bodemer, 2019). Therefore, researchers advocate developing intelligent tools that provide group awareness support in CsCL to foster internal regulation and reduce teachers' workload (Hu et al., 2025).

# 2.2. Utilizing AI to provide visual learning analytics feedback for supporting CSCL

Visual learning analytics feedback, serving as the interface between students and learning analytics, combines learning analytics and visualization technology to decode large amounts of educational data into intuitive visual forms, enhancing students' understanding of their learning status and conditions (Park et al., 2023). With the involvement of artificial intelligence technology, it can process complex data, identify patterns, and achieve a certain degree of automation (Fernandez- Nieto et al., 2021; Lim et al., 2021). In CsCL, AI- based visual learning analytics feedback tools are effective group awareness support tools that use interaction data from the collaborative learning process as a source. Through artificial intelligence, learning analytics, and visualization, they present information about the group in terms of behavior, cognition, and social aspects, helping students understand each other's individual or collective student engagement status or process to promote the collaborative learning process (Bodemer & Dehler, 2011; Sedrakyan et al., 2020). For example, visual learning analytics dashboards (Echeverria et al., 2025; Han et al., 2021; Li et al., 2022) and group awareness tools (Chen et al., 2023; Su et al., 2024) are widely used in CsCL. In recent years, a series of studies have successively shown that these tools can provide beneficial support for CsCL (Bao et al., 2021; Peng et al., 2022). Recent studies have sequentially shown that these tools can provide beneficial support for CsCL activities (Bao et al., 2021; Peng et al., 2022). For example, Li, Li, et al. (2021) developed a AI- based visual learning analytics feedback tool that used natural language processing to analyze online collaborative discussion content in real- time, providing the student group with keyword clouds, interaction frequency, and social network graphs related to cognitive aspects. Zheng, Zheng, and Fan (2023) developed a AI- based visual learning analytics feedback tool based on BERT, presenting thematic analyses of interaction content during collaborative learning, and their quasi- experimental study found that the tool significantly enhanced student engagement and academic performance. Similarly, Kong et al. (2025) developed a BERT- based visual learning analytics feedback tool that analyzed group collaborative interaction content data, offering AI- based visual learning analytics feedback and reports on behavioral and cognitive aspects. Their research also indicated that the tool could significantly

improve student engagement and academic performance.

improve student engagement and academic performance.Although AI- based visual learning analytics feedback tools have shown potential in enhancing student engagement in CSCL, several studies have also highlighted their limitations. For example, traditional tools tend to provide basic feedback on aspects such as participation or task compliance, and most feedback is generated through rule- based AI algorithms and structured data, lacking deep understanding and interpretation of students' cognitive and emotional states (Ba et al., 2025; Maier & Klotz, 2022). Moreover, the scalability of rich and tailored visual learning analytics feedback in modern educational environments is limited (Mejeh et al., 2024; Pozdniakov et al., 2023). Additionally, many students lack sufficient data visualization literacy, making it difficult for them to derive beneficial strategies and insights from AI- based visual learning analytics feedback to improve their learning (Corrin et al., 2015; Fernandez- Nieto et al., 2024). In the absence of support to help students understand visualization information, this may lead to increased cognitive load (Matcha et al., 2019; Moraes Bueno Rodrigues et al., 2021; Ramaswami et al., 2023). Although recent studies have found that scaffolding approaches, such as providing structured annotations and prompts for visualization data, are effective in enhancing students' understanding of complex visual learning analytics feedback (Milesi et al., 2025; Shao et al., 2024), traditional scaffolding methods still face challenges in achieving real- time responsive support and personalized customization (Fernandez- Nieto et al., 2024; Yan et al., 2025). Therefore, more research is needed to explore methods for enhancing visual learning analytics feedback in CSCL and to investigate how students can benefit from such new approaches (Jin, Yang, et al., 2025).

# 2.3. Providing learning feedback with generative AI

Recent developments in generative AI (GenAI) have brought new opportunities for visual learning analytics feedback (Yan, Zhao, et al., 2024). Compared to earlier AI that focused on simple task completion, GenAI can perceive contextual situations and dynamically generate personalized content and feedback, with the ability to produce coherent outputs comparable in quality to human input (Floden, 2025; Gencer & Gencer, 2024; Niloy et al., 2024; Su et al., 2023). The superior abilities of GenAI in data processing, analysis, and generation have led to its widespread application in scenarios such as computer programming code analysis (Jukiewicz, 2024; Wilson & Nishimoto, 2024), automatic writing analysis (Parker et al., 2023), and text encoding analysis (Siiman et al., 2023). For instance, Hutt et al. (2024) utilized ChatGPT to analyze the quality of peer feedback and generate explanations, finding that ChatGPT's analysis in terms of cognitive, emotional, and social engagement was second only to human coders. Phung et al. (2024) used GPT- 4 to provide learners with personalized assessments and feedback on their academic performance. Tang et al. (2024) discovered that ChatGPT excelled in assessing key indicators of critical thinking based on student peer feedback text data. Surveys on students' use of GenAI feedback show that GenAI feedback is widely welcomed by most students (Alghamli, 2024; Jiang, 2025; Polakova & Ivenz, 2024). Meanwhile, some empirical studies have also continuously demonstrated that GenAI- based learning analytics feedback (such as ChatGPT) shows significant beneficial effects in enhancing student engagement (Bu et al., 2025; Teng, 2024) and academic performance (Guo et al., 2024; Tsai et al., 2024). For instance, Lu and Ba (2025) found that providing GenAI feedback support in online collaborative discussions can enhance student behavioral engagement and knowledge construction without increasing additional cognitive load. This progress points to the potential of GenAI to enhance AI- based visual learning analytics feedback tools, providing more detailed contextual explanations and learning guidance (Yan, Martinez- Maldonado, & Gasevic, 2024; Milesi et al., 2024). The latest progress comes from Yan et al. (2025), who found that GenAI can effectively support students in using visual learning analytics feedback. However, direct evidence on how this approach specifically benefits student engagement in CSCL remains scarce.

While GenAI offers promising opportunities to enhance AI- based visual learning analytics feedback, its limitations cannot be overlooked (Zhan et al., 2025). A significant barrier is the hallucination phenomenon of GenAI, which tends to generate clear but inaccurate content (Huang et al., 2025). Meanwhile, researchers have recognized that understanding student interaction with technology is equally crucial for effective implementation, especially in the post- pandemic educational environment where student expectations for technology- enhanced learning have fundamentally changed (Lee & Wu, 2025; Yilmaz et al., 2024). For example, some studies have found that students' interactions with GenAI feedback is not high, and there is a mismatch between GenAI outputs and student expectations (Jin, Yang, et al., 2025). However, current research on AI- based visual learning analytics feedback (Kong et al., 2025; Zheng, Zhong, & Fan, 2023) or GenAI feedback (Guo et al., 2024; Tsai et al., 2024) mainly focuses on validating their effectiveness through quasi- experimental study designs, lacking collection of tracking data to assess differences in student interaction with them and their potential impact on CSCL (Zamecnik et al., 2022). Therefore, more empirical evidence is needed to provide insights into students' interactions with the combination of AI- based visual learning analytics feedback and GenAI feedback in CSCL (An et al., 2025).

# 2.4. Student interaction with learning analysis feedback

Researchers in the LA field claim that learning improvements through learning analytics feedback interventions rely on how students interact with the feedback, and a better understanding of learning processes from the learner's perspective can enhance the reliability and validity of assessment tools while strengthening the link between assessment and learning theories (Gasevic et al., 2022; Park et al., 2023). However, there is a lack of studies on how students interact with these tools in real- world learning environments, with most research focusing on students' perceptions rather than their actual usage patterns and engagement with the tools (Lucas & Euan, 2024; Jung & Wise, 2025). Even in studies with strict experimental designs where some students have access to learning analytics feedback and others do not, it is important to note that not all students in the access group may actually use the feedback (Kaliisa et al., 2024). Students vary in their adoption of feedback tools, often freely choosing to accept, ignore, or reject feedback and integrating it into their learning behavior based on personal interests (Matcha et al., 2020; Mejeh et al., 2024). For instance, Zamecnik

et al. (2022) found different interaction patterns between group roles and visual learning analytics feedback. Surveys on student use of intelligent feedback tools also show significant differences in how students access and use learning feedback tools, with only a few consistently engaging with them (Sanson et al., 2020). Research by Van Horne et al. (2018) indicates that only a minority of students frequently use intelligent feedback, with most using it rarely or not at all, and those who frequently access intelligent feedback tend to perform better in courses. Similar findings were reported by Hu (2022), who observed that students who frequently use learning platforms with visual learning analytics feedback generally achieve better academic performance. However, studies by Kim et al. (2016) and Kia et al. (2020) present inconsistent conclusions. The former found no significant relationship between interaction frequency with visual learning analytics feedback and academic performance, while the latter even reported lower academic performance among students with high interaction frequencies. These inconsistent findings highlight the importance of investigating differences in groups' interactions with AI- based visual learning analytics feedback and GenAI feedback, as well as their association with student engagement and academic performance. This importance has garnered increasing research interest. For example, the human- AI shared regulation of learning model introduced by Jarvela et al. (2023) suggests that students should be active participants when learning and using AI tools, and they will benefit from their reactions and interactions with the AI tools. Additionally, with the rapid development of AI technology and its growing impact on learning, recent studies have further emphasized the importance of investigating interaction patterns between humans and AI to better inform and support the learning process (Dang et al., 2025; Echeverria et al., 2025). These studies indicate that understanding students' interaction with AI- based learning analysis feedback is crucial for improving educational outcomes in the increasingly popular AI- integrated learning environments. Yet, empirical evidence in this area remains scarce (Jung & Wise, 2025).

To address these research gaps, this study designed an Visual- GenAI learning analytics feedback tool that integrates AI- based visual learning analytics feedback with GenAI feedback. Then, we focused on how groups interact with this hybrid feedback during different tasks, mined potential behavioral patterns, and investigated their association with student engagement and academic performance in CSCL. To achieve this, the study focuses on the following three research questions:

RQ1. What patterns of interaction behavior are exhibited by groups with the Visual- GenAI learning analytics feedback?

RQ2. What is the association between the feedback interaction behavior patterns and student engagement (behavioral, emotional, cognitive)?

RQ3. What is the association between the feedback interaction behavior patterns and academic performance?

Drawing on empirical evidence, we posit three assumptions corresponding to the three research questions, which together address the interaction differences between groups and Visual- GenAI learning analytics feedback in CSCL.

H1. There are different interaction behavior patterns between groups and Visual- GenAI learning analytics feedback.

Previous studies have shown that students interact differently with AI- based visual learning analytics feedback (Zamecnik et al., 2022) and GenAI feedback (Jin, Yang, et al., 2025). So, it is reasonable to assume that groups will also interact differently with Visual- GenAI learning analytics feedback. Prior studies also indicate that factors like different usage attitudes (Granić & Marangunić, 2019) and data visualization literacy (Fernandez- Nieto et al., 2024) may lead to such differences. For example, while AI- based visual learning analytics feedback offers groups a representation of collaborative learning processes through intuitive visualizations (Ouyang & Zhang, 2024), groups that cannot gain valuable insights from it may interact more with the learning feedback and suggestions provided by GenAI (Yan et al., 2025).

H2. Groups with different feedback interaction behavior patterns will show differences in student engagement.

This assumption is supported by studies indicating that AI- based visual learning analytics feedback (Kong et al., 2025; Zheng, Zhong, & Fan, 2023) and GenAI feedback (Ba et al., 2025) can enhance student engagement. Although these studies compared the effects of having versus not having feedback, they also implied that the extent of groups' interaction with feedback can influence student engagement. Moreover, GenAI feedback provides more direct learning suggestions and guidance than visual feedback, which is more conducive to promoting socially shared regulation within groups and thereby improving student engagement (Ng et al., 2024; Shao et al., 2024). Thus, groups that interact more with GenAI feedback are likely to benefit more in terms of student engagement.

H3. Groups with different feedback interaction behavior patterns will show differences in academic performance.

This assumption is based on studies showing that students' interaction with feedback can affect their academic performance (Hu, 2022). Also, following on from H2 which suggests differences in student engagement among groups with different feedback interaction patterns, these student engagement differences may in turn lead to varied academic performance (An et al., 2025; Luo et al., 2023). These results will significantly advance the ongoing academic discussion about differences in student interaction with learning analytics feedback in CSCL and their impacts. They will offer a key empirical foundation for understanding and optimizing the role of learning analytics feedback in CSCL environments (Lahza et al., 2023; Roberts et al., 2017), and provide practical guidance for designing feedback systems that better match learners' needs and promote deep collaboration and cognitive development.

# 3. Methods

# 3.1. Research context and participants

This study was conducted in a blended course named "Software Architecture (including software architecture, software design

patterns, etc.)" at a comprehensive university in Central China during the spring semester of 2024. This 12- week compulsory course for undergraduate software engineering majors aims to deepen students' understanding of core software analysis and development methods, and to equip them with the ability to apply this knowledge in real- world software development projects. These skills are essential for becoming a qualified software analyst or architect. During the course, students attended a 90- min face- to- face class weekly, taught by a professor with over 20 years of teaching experience in software engineering. The offline class focused on explaining core concepts and architecture evaluation methods. To enhance their understanding of software architecture and design patterns, students also engaged in online learning, which involved group discussions on 24 software architecture and design pattern cases through an online collaborative learning system with Visual- GenAI learning analytics feedback. Fifty- two third- year software engineering students (18 females and 34 males, aged 19- 23) from the same class participated in the course. They had previously taken core courses such as "Introduction to Software Engineering" and "Operating Systems". We assured all students that the collected data would be confidential and used solely for research. Informed consent was obtained from all participants, and the study was approved by the university's research ethics committee.

# 3.2. Online collaborative learning system with Visual-GenAI learning analytics feedback

In this study, an online collaborative learning system anchored in Visual- GenAI learning analytics feedback for student groups was developed. This integrated platform combines rule- based AI and generative AI (GenAI) technologies, featuring five core modules: task information module, group discussion module, data module, analysis module, and feedback module. The system architecture, visualized in Fig. 1, demonstrates how these components interoperate to create a dynamic feedback.

In the task information module, instructors or teaching assistants post group collaborative task information within the platform, whereafter group members, upon reviewing the task information and understanding the requirements, proceed to the group discussion module, as illustrated in Fig. 2. Within the group discussion module, students engage in group discussions with their peers regarding the problem- solving strategies, processes, and outcomes for the learning tasks, as shown in Fig. 3. Their participation in online discussions is logged in the form of logs within the database of the data module. During group discussions, group members can click the "Learning Analytics" button at any time to enter the feedback module page and view formative feedback on group collaboration.

The feedback module combines visual learning analytics and GenAI- generated explanatory text. The visual learning analytics feedback, produced by rules- based AI, encompassing the group's basic information, behavioral engagement, cognitive engagement, and social engagement, as depicted in Fig. 4. Basic information includes the group name and the number of members; behavioral engagement comprises the total number of group messages, the distribution of message timing, and the number of messages contributed by each member; cognitive engagement presents a historical message feed and a keyword contribution network. Social engagement is measured using the Gini coefficient to assess the presence of unequal participation within the group, with the Gini coefficient calculated as follows: for a group, sum the deviations of each member's discussion participation from the average participation of the group members during actual activities, and then divide this sum by the maximum possible value of these deviations (Janssen et al., 2011).

Detailed textual feedback is generated through the analysis by GenAI, an advanced API application service offered by the generative artificial intelligence model Sparkdesk (Leng et al., 2024). This study follows the approach of Suraworachet et al. (2024) who utilized GenAI (GPT- 4) to analyze challenging moments in students' online group discussions. GenAI was assigned a specific role to analyze the content of group discussions and produce feedback, with the following prompt engineering designed:

"Group members are engaged in a collaborative discussion focusing on the <collaborative task content>, through which they formulate a solution. You are a teaching assistant, tasked with observing the students' discussion to assist the instructor in analyzing the group's engagement in terms of emotional engagement, social engagement, and knowledge construction. emotional engagement involves assessing the emotional states and commitment of group members during the discussion, including the presence of positive emotional expressions and mutual support and encouragement. Social engagement examines the interaction and collaboration among group members. Knowledge construction is the examination of the depth of thought and problem- solving exhibited by group members during the discussion. I would like you to generate learning feedback and

![](images/32623ced4ef9a74aba74f885ea0c805bdebb2dc2955a9eaa9e694a96c8021c40.jpg)  
Fig. 1. The architecture of the online collaborative learning system with Visual-GenAI learning analytics feedback.

![](images/ebf0434500d5186b8f0d2e3aad7123b4eb36fa17706244e7d75cfa7ca2c4dc2c.jpg)  
Fig. 2. Task information page.

![](images/8b8346838f1f9b315072ee9dd54ef2615ff088a60c04b9cd9eaea652ac5a9e6e.jpg)  
Fig. 3. Group discussion page.

suggestions for the student group based on the analysis of the group discussion. Below are the records of the group's discussion: <group discussion records>."

The GenAI feedback is depicted in Fig. 5. Upon each visit to the feedback page, group members clicked the "View Analysis" button, whereupon GenAI analyzed the textual data of the group discussion to generate detailed explanatory feedback. This feedback was designed to assist group members in gaining a better understanding of their performance and directions for improvement during the collaborative process. The feedback encompassed not only an evaluation of the group's overall performance but also provided individualized suggestions based on the members' participation. Through this approach, GenAI offered profound insights to enhance the group's level of collaborative engagement.

# 3.3. Research procedure

The research procedure is outlined in Fig. 6. During the initial preparation phase (week 1- 4), the instructor first introduced to the students the course teaching format, learning content, and schedule, and encouraged them to believe in their ability to enhance software design skills through study and practice to accomplish complex learning tasks. Researchers joined the course as teaching assistants. Initially, students freely formed 13 groups of four. Following An et al. (2025), to evaluate consistency among student groups, the researchers pre- tested students using a self- efficacy questionnaire. Based on the principle of homogeneity between groups, after communicating with some group members and obtaining their consent, the researchers appropriately adjusted group members to make differences between groups insignificant. Additionally, the researchers trained students to use the online collaborative learning environment, namely the online collaborative learning system with Visual- GenAI learning analytics feedback, ensuring that every student could operate it proficiently.

During the implementation phase (Week 5- 11), in addition to participating in face- to- face offline courses each week, students were

![](images/e1da2ce3c9522002a1d28864e91eafc9e51af427996c2473901e107abde7fb3a.jpg)  
Fig. 4. Collaborative learning analysis feedback page.

![](images/60dd46cc37806ca27590bc6570c3110694b0e6b70c4a927354a43de65143eb77.jpg)  
Fig. 5. Learning analytics feedback provided by GenAI.

required to complete six periodic online collaborative learning tasks. Each online collaborative learning task lasted for one week, during which groups focused on four software architecture and design pattern cases (a total of 24 cases discussed over the period) for online discussions. Groups conducted online discussions through an online collaborative learning system incorporating Visual- GenAI learning analytics feedback. Each group engaged in in- depth discussions on the four cases assigned for the week within the system, focusing on analyzing case understanding, implementation ideas, and methods. For detailed case content, please refer to Appendix A. During the collaborative discussion, group members could click the "Learning Analytics" button on the current discussion page to view the Visual- GenAI learning analytics feedback, as shown in Fig. 3. After the collaborative discussion, each group was required to summarize the discussion content into a collaborative report (with no format or word- count restrictions) and submit it.

![](images/ad502b19612aaf20860abe3f3114eef5101e2d1e4edb64293fc7fb37e88ae60d.jpg)  
Fig. 6. Research process.

This study concluded with the data collection phase (Week 12). During this week, all students took the final course academic performance test. Upon course completion, researchers gathered feedback interaction data and online discussion data from the learning platform to evaluate differences in learning groups' interactions with Visual- GenAI learning analytics feedback and its association with student engagement. Also, to explore students' perceptions of this feedback tool, researchers randomly selected two participants from each feedback interaction pattern for interviews, based on data analysis. To ensure representativeness, selected students had participated in all collaborative tasks and came from different learning groups.

# 3.4. Instruments

# 3.4.1. Self-efficacy questionnaire for grouping

To ensure the robustness of the research findings, we assessed students' self- efficacy prior to the study. Previous research has indicated that self- efficacy influences student engagement (Bedi, 2023) and academic performance (Liu, Zaigham, et al., 2022; Wang & Lin, 2007). In this study, we utilized a self- efficacy questionnaire developed by Pintrich et al. (1991) and modified it to suit the context of CSCL activities. The questionnaire consists of 8 items and employs a 5- point Likert scale, which is widely used in the educational domain and has been validated by numerous studies (Chang et al., 2022; Cheng et al., 2023). The questionnaire demonstrated high reliability, with a Cronbach's alpha value of 0.95. To validate the rationality of the groupings, we conducted a Kruskal- Wallis test to examine intergroup differences in self- efficacy. The results revealed no significant differences in self- efficacy among the student groups (Kruskal- Wallis  $\mathrm{H} = 11.761$ ,  $df = 12$ ,  $p = .465$ ), indicating a consistent homogeneity across the groups.

# 3.4.2. Extract feedback interaction behaviors from the data files of the learning system

In this study, researchers exported students' behavior data on viewing AI- based visual learning analytics feedback and GenAI feedback from the learning- platform behavior- log- data files, which recorded behavior names, timestamps, task names, and usernames, etc. To ensure data accuracy, two researchers separately counted each group's behaviors regarding viewing visual learning analytics feedback and GenAI feedback in each task. They then verified discrepancies until their counts matched. Ultimately, they collected 78 feedback interaction behavior data units from 13 groups across six discussions for analysis.

# 3.4.3. Semi-structured interviews are conducted to understand students' perception

In the semi- structured interviews, students were asked about their perceptions and experiences of using the AI- GenAI intelligent feedback tool. This helped explore the reasons behind the four types of differential interaction patterns between learning groups and the Visual- GenAI learning analytics feedback. Example interview questions were "What benefits do you think the visual learning analytics feedback and GenAI feedback brought to your group?" and "Why did your group use the visual learning analytics feedback/ GenAI feedback more/less in the tasks?" We collected semi- structured interview data from students via an online conference system (Tencent Conference system). Before recording the interviews, we ensured students knew about and agreed to the recording. After the interviews, two researchers transcribed the audio data into text and checked the accuracy of the text data.

# 3.4.4. The coding framework of student engagement

In traditional learning environments, student engagement is often measured through self- report methods such as surveys (Liu et al.,

2015; Qureshi et al., 2023). However, self- reporting is susceptible to learners' subjectivity and overlooks the temporal and dynamic nature of student engagement (Li, Lajoie, et al., 2021). In contrast, CSCL offers a richer data source for studying student engagement. Researchers advocate for the use of log data analysis and coding of interaction content to gain insights into student engagement during the CSCL process (Li et al., 2021; Liu, Liu, et al., 2022; Ouyang et al., 2021). Thus, this study gathered discussion data from six tasks of 13 student groups via group discussion data logs in the learning system. Here, the smallest meaningful unit was the individual student utterance, so 5, 599 utterance records in total were collected for analyzing behavioral, emotional, and cognitive engagement.

Behavioral engagement was analyzed based on the behavioral log data recorded by the online collaborative learning system. Drawing on previous research on behavioral engagement in online collaborative student group discussions (Su et al., 2024; Hu et al., 2025), behavioral engagement was assessed in terms of the quantity of chat frequencies and the duration of online discussion.

Emotional engagement was assessed through content analysis by encoding and analyzing textual data from group discussions. We utilized an emotional engagement coding scheme that was previously applied by Hu et al. (2025) to online collaborative student group discussions, which was derived from the research conducted by Gao et al. (2023). In the context of the current study, we made slight modifications to the coding scheme, which includes the categories Positive, Negative, Confused, and Neutral, as presented in Table 1.

Cognitive engagement was also assessed through content analysis by encoding and analyzing textual data from group discussions. The coding scheme employed was adapted from the cognitive engagement coding scheme for online collaborative student group discussions previously used by Hu et al. (2025), which originated from the research of Peng et al. (2022). The research context of Peng et al. (2022) is somewhat similar to the present study. The coding scheme comprised four main dimensions: Questioning, Statement, Monitoring, and Others. To better align with the context of this study, we made slight adjustments to the coding scheme, as detailed in Table 2.

# 3.4.5. Academic performance test

The academic performance test was developed by a professor with over 20 years of teaching experience in software engineering. It aims to assess students' grasp of theoretical knowledge in the "Software Architecture" course, their software architecture design skills, and their ability to analyze and solve problems. The test includes five true- or- false questions (2 points each), four short- answer questions (7 points each), two design questions (15 points each), and two essay questions (16 points each), with a total score of 100. An example of a true- or- false question is: "In the B/S architecture, component connections are message- driven." A short- answer question example is: "List the main activities in development based on software architecture." An example of a design question is: "Design the requirements specification for a virtual learning community system, including its design objectives and software architecture." An essay question example is: "Using the Architecture Tradeoff Analysis Method (ATTAM), analyze the software architecture style of an online learning analytics system based on short texts." All students took the 120- min test in the classroom to ensure fairness. Two software engineering professors evaluated the written responses. The Pearson correlation between their scores was significant  $(r = .969, p < .001)$ , with a maximum score difference of eight points. The final academic performance score was the average of the two evaluators' scores.

# 3.5. Data analysis

This study used mixed methods with a data analysis process shown in Fig. 7, involving five main steps: (1) Hierarchical clustering classified 78 data units from 13 learning groups' interaction behavior data with two AI feedback types in six collaborative tasks; (2) Interview content was organized to extract student views; (3) Behavioral engagement was measured by counting student chat frequencies and online durations, while cognitive and emotional engagement were encoded via content analysis of discussion texts; (4) Differences in behavioral, emotional, and cognitive engagement were analyzed among groups with different feedback interaction behavior patterns across six tasks; (5) Academic performance differences were analyzed among students with different feedback interaction behavior patterns across six tasks.

To address RQ1 (What patterns of interaction behavior are exhibited by groups with the Visual- GenAI learning analytics feedback?), we conducted the first and second steps of analysis. In step one, hierarchical clustering classified 78 data units from 13 groups across six collaborative discussion tasks. The frequency of groups' interactions with visual learning analytics feedback and GenAI feedback served as variables. The agglomerative hierarchical clustering approach operates in a bottom- up manner, starting with each data point as a separate cluster and progressively merging the closest clusters until a single cluster is formed (Li, Lu, et al., 2023). To

Table 1 Coding scheme of emotional engagement.  

<table><tr><td>Dimensions</td><td>Descriptions</td><td>Examples</td></tr><tr><td>Positive</td><td>Expression encompasses positive feelings towards collaboration or tasks like surprise, interest, enjoyment, and pride, as well as praising, encouraging, or comforting group members while proactively offering assistance and demonstrating confidence in task completion.</td><td>e.g., &quot;Your analysis is comprehensive!&quot;</td></tr><tr><td>Negative</td><td>Expressing unfavorable sentiments regarding the collaboration or tasks, like worry, dissatisfaction, monotony, irritation, hopelessness, and embarrassment.</td><td>e.g., &quot;It&#x27;s so hard. I can&#x27;t understand it.&quot;</td></tr><tr><td>Confused</td><td>Expressing uncertainty about the collaboration or tasks, including doubts, suspicions, guesses, and bewilderment.</td><td>e.g., &quot;Can you explain it in more detail?&quot;</td></tr><tr><td>Neutral</td><td>Expressing facts or opinions in a calm or unemotional way.</td><td>e.g., &quot;Let&#x27;s look at this problem first.&quot;</td></tr></table>

Table 2 Coding scheme of cognitive engagement.  

<table><tr><td>Dimensions</td><td>Sub-dimensions</td><td>Descriptions</td><td>Examples</td></tr><tr><td>Questioning</td><td>Seeking information</td><td>Inquiry with a direct or definitive answer.</td><td>e.g., &quot;What is the Facade Pattern?&quot;</td></tr><tr><td rowspan="4">Statement</td><td>Inquiring Informative</td><td>Inquiry without a direct or definitive answer. A statement related to the discussion topic that either offers opinions/ideas without elaboration or justification, or restates previous degrees with others&#x27; ideas.</td><td>e.g., &quot;Do you have any other suggestions?&quot; e.g., &quot;I understand it in the same way.&quot;</td></tr><tr><td>Explanatory</td><td>A statement that elaborates on previous opinions/ideas by adding details, further explanation, or examples.</td><td>e.g., &quot;The Bridge Pattern is a structural design pattern.&quot;</td></tr><tr><td>Analytical</td><td>A statement offering an in-depth analysis, logical reasoning, or a detailed elaboration of an opinion supported by reasons.</td><td>e.g., &quot;The Singleton pattern utilizes a list to maintain records, so the maintained data blocks are capable of supporting simultaneous modification of multiple blocks.&quot;</td></tr><tr><td>Synthesizing</td><td>A statement summarizing or encapsulating the key points of a discussion.</td><td>e.g., &quot;Overall, the design patterns adopted in both the basic and extended experiments provide excellent support for system design.&quot;</td></tr><tr><td rowspan="3">Monitoring</td><td>Planning</td><td>A statement outlining the group task plan, delineating clear work assignments, and furnishing strategies beneficial to the group&#x27;s objective.</td><td>e.g., &quot;I plan to find some information about this problem.&quot;</td></tr><tr><td>Process managing</td><td>A statement tracking time or progress of tasks.</td><td>e.g., &quot;Next, let&#x27;s continue to discuss the second issue.&quot;</td></tr><tr><td>Reflecting</td><td>A statement that evaluates the work of a group or individual and highlighting shortcomings.</td><td>e.g., &quot;I found issues with my UML class diagram.&quot;</td></tr><tr><td>Others</td><td>Irrelevant</td><td>A statement unrelated to learning tasks.</td><td>e.g., &quot;What&#x27;s for lunch today?&quot;</td></tr></table>

![](images/5f5f36cf55f0c1a98f915ff539ed406540690b65f107b5d3913d952315be284f.jpg)  
Fig. 7. Data collection and analysis process.

clearly illustrate the hierarchical relationships among different clusters during the merging process, the computational procedure can be visually represented by a dendrogram. Concurrently, we employed the silhouette coefficient to evaluate the clustering outcomes, thereby determining the most appropriate number of clusters (Higgins et al., 2024; Rousseauw, 1987). After identifying interaction behavior patterns, a Kruskal- Wallis test checked differences in feedback interaction between patterns, with Mann- Whitney U tests and Holm- Bonferroni adjustments for multiple comparisons. In step two, to explore why learning groups interact differently with hybrid feedback, two researchers analyzed student interviews. They extracted views on the two feedback types and reasons for continued or discontinued use.

To address RQ2 (What is the association between the feedback interaction behavior patterns and student engagement?), we conducted the third and fourth steps of our analysis. In step three, we analyzed behavioral, emotional, and cognitive engagement based on group discussion data. For behavioral engagement, we counted chat frequencies and online participation times from discussion logs. For emotional engagement, two coders independently coded  $30\%$  of discussion texts as per the coding scheme. Their Cohen's Kappa was  $86.31\%$  indicating strong agreement. And they discussed any discrepancies until consensus was reached. Similarly, for cognitive engagement, two coders independently coded  $30\%$  of discussion texts as per the coding scheme. Their Cohen's Kappa was  $89.17\%$  indicating strong agreement. They also discussed discrepancies until consensus was reached. After consensus, the remaining  $70\%$  of texts were coded separately by the two coders. In fourth step, we applied the Kruskal- Wallis test (with Holm- Bonferroni adjustment for multiple comparisons) to compare behavioral engagement differences among feedback interaction patterns in each task, and the Chi- square test to compare emotional and cognitive engagement differences. Subsequently, we employed ordered

network analysis (ONA) to visually analyze the cognitive engagement connection characteristics and differences of each feedback interaction behavior pattern. ONA is a technique based on epistemic network analysis (ENA) that identifies and quantifies directed connections between data elements through the calculation of event sequences, thereby visualizing the frequency of transitions between data elements within a dataset (Tan et al., 2023). ONA employs a sliding window method to identify and accumulate direct connections between data elements (Zhao et al., 2023). In this study, we utilized an R language package to perform the ONA visualization for cognitive engagement. Based on a review of existing research (An et al., 2025; Li, Fan, et al., 2023), we determined the optimal sliding window length for analyzing the current cognitive engagement dataset to be 7. In ONA networks, the four codes of cognitive engagement are represented as distinct nodes, with the size of the nodes reflecting the frequency of occurrence. The colored circles within each node represent the frequency of self- connections related to the node code, with the size and saturation of the colored circles indicating the strength of the self- connections. Additionally, directed connections between two nodes are represented by a pair of edges composed of triangles, with a dark chevron (enhanced by a red arrow) indicating the direction of the connection. Notably, edges that are thicker and more saturated imply a stronger connection between two nodes (Kong et al., 2025). Moreover, the small squares in the ONA network structure diagram represent the centroid of cognitive engagement (An et al., 2025).

To address RQ3 (What is the association between the feedback interaction behavior patterns and academic performance?), we conducted the fifth step of the analysis in sequence. In this step, we used the Kruskal- Wallis test to determine the differences in academic performance among students with different feedback behavioral interaction patterns in each task. This non- parametric method was appropriate for the analysis as the observed academic performance data violated the normality assumption. When the Kruskal- Wallis test indicated statistically significant differences, we subsequently performed Mann- Whitney U tests to explore post hoc pairwise differences between independent interaction behavior patterns. The Holm- Bonferroni method was applied to adjust for multiple comparisons.

# 4. Results

4.1. What patterns of interaction behavior are exhibited by groups with the Visual- GenAI learning analytics feedback?

To clearly illustrate the hierarchical relationships between different clusters during the merging process, the clustering procedure can be visually represented by a dendrogram, as shown in Fig. 8. The horizontal axis indicates the 78 data units, with labels such as "1- 01" representing group 4 of task 1. In this study, the optimal number of clusters was determined by calculating the silhouette coefficient. Fig. 9 shows the variation of the silhouette coefficient with the number of clusters, with the decision to select four clusters as a balance between the silhouette coefficient and the clustering distance. In Fig. 8, different colors are used to encode the four identified clusters. Additionally, Fig. 10 illustrates the behavior patterns of each group's interaction with the AI- GenAI hybrid intelligence feedback during each task, with "G- 1" denoting group 1.

Table 3 shows the descriptive statistics for the two feedback interaction behaviors across four patterns. To further investigate whether there were differences in the two feedback behaviors among the four patterns, we conducted a non- parametric Kruskal- Wallis test due to the non- normal distribution of the feedback behavior data. Results indicated significant differences in viewing GenAI feedback (Kruskal- Wallis  $\mathrm{H} = 33.885$ ,  $df = 3$ ,  $p = .000$ ) and AI- based visual learning analytics feedback (Kruskal- Wallis  $\mathrm{H} = 49.735$ ,  $df = 3$ ,  $p = .000$ ). Specifically, Cluster 1 had the fewest views on both feedback types and was labeled as "Low Feedback Interaction (LFI)". It occurred most frequently in every task, with over half of the groups being in the LFI mode in all tasks. Cluster 2 had the most views but occurred least frequently and only in Task 1, so it was named "High Feedback Interaction (HFI)". Cluster 3, with more views on GenAI feedback and the third- highest views on AI- based visual learning analytics feedback, was called "High GenAI Feedback Interaction (HGFI)". Cluster 4, which had more views on AI- based visual learning analytics feedback and the third- highest views on GenAI feedback, was labeled as "High AI- based Visual learning analytics feedback Interaction (HAFI)".

![](images/0fef1f86325d3594189d68c8fb9f19a355adad0a34a95dc44f6bd29f7e5e00f3.jpg)  
Fig. 8. Dendrogram of the first clustering result.

![](images/c30e6a6995b685d0bba9900325128b2f54f8f12bdd165260ec571958561e0741.jpg)  
Fig. 9. Number of clusters selected in the cluster.

![](images/3d0ce2fc9b550da03167c44770943b8e46370a4569c76823cb2999f9ba335d3a.jpg)  
Fig. 10. The type of each group on each task.

Table 3 Descriptive statistics for four patterns.  

<table><tr><td>Interaction</td><td>Pattern</td><td>N</td><td>Mean</td><td>SD</td><td>Min</td><td>Max</td><td>H</td><td>p</td><td>Post-hoc test</td></tr><tr><td rowspan="4">Viewing GenAI Feedback</td><td>LFI</td><td>55</td><td>0.29</td><td>0.458</td><td>0</td><td>1</td><td>33.885</td><td>.000 ***</td><td>HFI &amp;gt; LFI **</td></tr><tr><td>HFI</td><td>3</td><td>3.67</td><td>1.155</td><td>3</td><td>5</td><td></td><td></td><td>HGFI &amp;gt; LFI ***</td></tr><tr><td>HGFI</td><td>5</td><td>2.40</td><td>0.548</td><td>2</td><td>3</td><td></td><td></td><td></td></tr><tr><td>HAFI</td><td>15</td><td>0.73</td><td>0.458</td><td>0</td><td>1</td><td></td><td></td><td></td></tr><tr><td rowspan="4">Viewing AI-based visual learning analytics feedback</td><td>LFI</td><td>55</td><td>1.40</td><td>1.862</td><td>0</td><td>7</td><td>49.735</td><td>.000 ***</td><td>HFI &amp;gt; LFI **</td></tr><tr><td>HFI</td><td>3</td><td>18.00</td><td>2.646</td><td>16</td><td>21</td><td></td><td></td><td>HGFI &amp;gt; LFI*</td></tr><tr><td>HGFI</td><td>5</td><td>7.20</td><td>2.775</td><td>4</td><td>10</td><td></td><td></td><td>HAFI &amp;gt; LFI ***</td></tr><tr><td>HAFI</td><td>15</td><td>13.53</td><td>3.482</td><td>9</td><td>20</td><td></td><td></td><td></td></tr></table>

Note:  $^\ast p < .05$ $^{**}p < .01$ $^{**}p < .001$

To explore the reasons behind the four distinct interaction patterns between groups and Visual- GenAI learning analytics feedback, semi- structured interviews were conducted with students at the end of the course. Table 4 presents student perceptions on using Visual- GenAI learning analytics feedback, with eight students from different groups. Specifically, students from groups G- 8 and G- 12 (LFI) reported no habitual use of learning analytics feedback, highlighting how learning motivation and metacognitive strategy differences affect feedback engagement. Students from groups G- 4 and G- 6, representing HFI, expressed positive perceptions of both feedback types. However, student from G- 4 noted that the degree of group collaboration influenced their subsequent adoption of the feedback, while student from G- 6 highlighted the impact of GenAI feedback's response speed on their adoption. This indicates that sustained feedback use also depends on contextual factors like collaboration quality and technology- related perceptions. Students from groups G- 1 and G- 3, representing HGFI, reported difficulties in extracting meaningful information from visual feedback. Student from G- 1 was drawn to the novelty of GenAI feedback, while student from G- 3 affirmed its value but also noted that academic stress affected their sustained use of learning feedback. Lastly, students from groups G- 5 and G- 7, representing HAFI, placed greater value on visual feedback for group collaboration compared to GenAI feedback. Overall, these four interaction patterns are not random but result from dynamic interactions between students' internal factors (learning motivation, metacognition, data visualization and AI literacy) and external factors (collaboration quality, technology perception, academic stress).

4.2. What is the association between the feedback interaction behavior patterns and student engagement (behavioral, emotional, cognitive)?

4.2.1. What is the association between the feedback interaction behavior patterns and behavioral engagement?

Table 5 presents the analysis of behavioral engagement across different feedback interaction patterns in six tasks. Kruskal- Wallis

Table 4 Students' perceptions on the experience of using the Visual-GenAI learning analytics feedback.  

<table><tr><td>Group</td><td>Perceptions of visual feedback</td><td>Perceptions of GenAI feedback</td></tr><tr><td>G-8</td><td colspan="2">Our group prioritized task completion over discussion quality optimization, and lacked a habit of shared reflection, leading to infrequent consultation of the visualization and GenAI feedback.</td></tr><tr><td>G-12</td><td colspan="2">Our group is accustomed to meeting task deadlines. We tend to directly focus on the course task requirements and perceive the learning feedback tools as less useful. Also, we lack habits to effectively utilize feedback to enhance group collaboration.</td></tr><tr><td>G-4</td><td colspan="2">We found word clouds can identify key discussion terms and indicate if we stray from the topic. However, as our group cohesion strengthened and discussions became more seamless, we sometimes checked feedback based on discussion progress.</td></tr><tr><td>G-6</td><td colspan="2">We find visualization feedback can intuitively display participation and speaking time in group discussions. It offers an immediate overview of participation, facilitates team leaders&#x27; communication with members, and helps encourage less active students.</td></tr><tr><td>G-1</td><td colspan="2">We find that interpreting the charts in the visual feedback is time-consuming. With the increasing workload from other courses, we tend to focus on completing tasks quickly without much attention to the quality of task completion.</td></tr><tr><td>G-3</td><td colspan="2">We find the charts in the visual feedback offer an intuitive display of our participation. However, we haven&#x27;t realized where these charts can provide specific help.</td></tr><tr><td>G-5</td><td colspan="2">I find group member viewing of visual feedback useful for intuitively gauging participation levels. Word clouds visuals highlight key discussion themes and focal points.</td></tr><tr><td>G-7</td><td colspan="2">I think visual feedback more truthfully reflects our group&#x27;s participation, feeling more practical. So, we tend to focus on it more.</td></tr></table>

tests reveal significant differences in chat frequency and participation duration among these patterns across all tasks (all  $p$ - values  $<.05$ ). Notably, HFI, appearing only in Task 1, has higher chat frequency and participation duration than LFI, HGFI, and HAFI, suggesting a possible association with high behavioral engagement. However, its single- task occurrence also highlights the challenge of sustaining such intense engagement. In contrast, LFI shows lower chat frequency and participation duration than HGFI and HAFI, indicating the lowest behavioral engagement. Moreover, HGFI has higher chat frequency and participation duration than HAFI across all tasks, implying slightly higher behavioral engagement in HGFI. Overall, these results indicate that differences in how groups interact with AI- based visual learning analytics feedback and GenAI feedback can affect behavioral engagement.

4.2.2. What is the association between the feedback interaction behavior patterns and emotional engagement?

Table 6 presents the descriptive statistics for emotional engagement coding frequencies and the Chi- square test results. The Chi- square tests show that in each task, there were statistically significant differences in emotional engagement codes among the different patterns (all  $p$ - values  $<.01$ ). The data shows that HFI has a higher proportion of "positive", "negative", and "confused" codes compared to the other three patterns, and a lower proportion of "neutral" codes. This suggests that HFI may be associated with high

Table 5 Results for behavioral engagement.  

<table><tr><td rowspan="2">Task</td><td rowspan="2">Pattern</td><td rowspan="2">N</td><td colspan="5">Chat frequency</td><td colspan="5">Participation duration(Minutes)</td></tr><tr><td>Mean</td><td>SD</td><td>H</td><td>p</td><td>Post-hoc test</td><td>Mean</td><td>SD</td><td>H</td><td>p</td><td>Post-hoc test</td></tr><tr><td rowspan="4">Task 1</td><td>LFI</td><td>28</td><td>4.69</td><td>6.794</td><td>32.451</td><td>.000 ***</td><td>HFI &amp;gt; LFI ***</td><td>4.64</td><td>6.338</td><td>32.461</td><td>.000 ***</td><td>HFI &amp;gt; LFI ***</td></tr><tr><td>HFI</td><td>12</td><td>43.50</td><td>24.001</td><td></td><td></td><td>HGFI &amp;gt; LFI **</td><td>58.33</td><td>36.380</td><td></td><td></td><td>HGFI &amp;gt; LFI **</td></tr><tr><td>HGFI</td><td>8</td><td>27.38</td><td>16.080</td><td></td><td></td><td></td><td>40.88</td><td>36.380</td><td></td><td></td><td></td></tr><tr><td>HAFI</td><td>4</td><td>7.00</td><td>2.160</td><td></td><td></td><td></td><td>20.00</td><td>6.782</td><td></td><td></td><td></td></tr><tr><td rowspan="3">Task 2</td><td>LFI</td><td>36</td><td>10.31</td><td>11.102</td><td>18.855</td><td>.000 ***</td><td>HGFI &amp;gt; LFI **</td><td>8.19</td><td>11.732</td><td>27.042</td><td>.000 ***</td><td>HGFI &amp;gt; LFI ***</td></tr><tr><td>HGFI</td><td>4</td><td>58.25</td><td>11.172</td><td></td><td></td><td>HAFI &amp;gt; LFI **</td><td>59.5</td><td>17.577</td><td></td><td></td><td>HAFI &amp;gt; LFI ***</td></tr><tr><td>HAFI</td><td>12</td><td>24.00</td><td>14.996</td><td></td><td></td><td></td><td>35.33</td><td>17.312</td><td></td><td></td><td></td></tr><tr><td rowspan="3">Task 3</td><td>LFI</td><td>36</td><td>15.64</td><td>16.355</td><td>9.425</td><td>.009 **</td><td>HGFI &amp;gt; LFI *</td><td>12.33</td><td>13.611</td><td>16.024</td><td>.000 ***</td><td>HGFI &amp;gt; LFI **</td></tr><tr><td>HGFI</td><td>4</td><td>43.25</td><td>25.46</td><td></td><td></td><td></td><td>46.00</td><td>18.184</td><td></td><td></td><td>HAFI &amp;gt; LFI **</td></tr><tr><td>HAFI</td><td>12</td><td>19.67</td><td>10.748</td><td></td><td></td><td></td><td>30.33</td><td>17.375</td><td></td><td></td><td></td></tr><tr><td rowspan="2">Task 4</td><td>LFI</td><td>36</td><td>16.72</td><td>20.770</td><td>8.641</td><td>.003 **</td><td>-</td><td>11.81</td><td>14.160</td><td>14.115</td><td>.000 ***</td><td>-</td></tr><tr><td>HAFI</td><td>16</td><td>25.63</td><td>13.114</td><td></td><td></td><td></td><td>26.13</td><td>13.281</td><td></td><td></td><td></td></tr><tr><td rowspan="2">Task 5</td><td>LFI</td><td>44</td><td>18.57</td><td>22.142</td><td>4.244</td><td>.039 *</td><td>-</td><td>11.86</td><td>14.439</td><td>5.608</td><td>.018 *</td><td>-</td></tr><tr><td>HAFI</td><td>8</td><td>23.75</td><td>10.209</td><td></td><td></td><td></td><td>23.63</td><td>6.479</td><td></td><td></td><td></td></tr><tr><td rowspan="3">Task 6</td><td>LFI</td><td>40</td><td>13.75</td><td>17.418</td><td>7.057</td><td>.029 *</td><td>-</td><td>12.35</td><td>18.085</td><td>10.279</td><td>.006 **</td><td>HGFI &amp;gt; LFI *</td></tr><tr><td>HGFI</td><td>4</td><td>34.25</td><td>32.438</td><td></td><td></td><td></td><td>59.50</td><td>0.577</td><td></td><td></td><td></td></tr><tr><td>HAFI</td><td>8</td><td>16.13</td><td>3.044</td><td></td><td></td><td></td><td>35.33</td><td>17.312</td><td></td><td></td><td></td></tr></table>

Note:  $^\ast p < .05$ $^{**}p < .01$ $^{**}p < .001$

emotional engagement. In contrast, LFI had the highest proportion of "neutral" codes and the lowest proportion of "confused" codes across all tasks, indicating the lowest level of emotional engagement. Moreover, in most tasks, HGFI had higher proportions of "positive", "negative", and "confused" codes than LFI, with slightly higher "positive" and "negative" code proportions than HAFI. HGFI also had lower "neutral" code proportions than LFI and HAFI across all tasks, showing richer emotional engagement than the latter two. In all tasks, HAFI had lower "neutral" code proportions but higher "confused" code proportions than LFI, indicating better emotional engagement than LFI. These results indicate that the differences in the way groups interact with AI- based visual learning analytics feedback and GenAI feedback can lead to variations in emotional engagement.

# 4.2.3. What is the association between the feedback interaction behavior patterns and cognitive engagement?

Table 7 presents descriptive statistics and Chi- square test results for cognitive engagement coding frequencies. Chi- square tests for cognitive engagement across six tasks show significant differences among patterns in each task (all  $p$ - values  $<.001$ ). Specifically, LFI has the highest frequencies and percentages of "others" coding and the lowest percentages of "monitoring" coding in each task, indicating the lowest cognitive engagement. HFI shows significantly lower frequencies and percentages of "others" coding than LFI and HGFI, and similar cognitive engagement across dimensions to HAFI in Task 1. HGFI has higher percentages of "monitoring" coding than HAFI in Task 2 and 6, and much lower percentages of "statement" coding than HAFI in all tasks. HAFI has lower frequencies and percentages of "others" coding than other patterns in each task, and slightly higher percentages of "questioning" coding than HGFI in most tasks (Task 1, 2, 3). Overall, HGFI and HAFI show relatively small differences in cognitive engagement.

The ONA networks reveal cognitive engagement connectivity characteristics and differences across feedback interaction behavior patterns. Fig. 11 presents the ONA networks and pairwise subtraction comparison networks for different patterns in Task 1. LFI demonstrates stronger connections from "others" to "statement". HFI and HAFI both show stronger connections from "statement" to "questioning" and from "statement" to "monitoring". HGFI exhibits stronger connections from "questioning" to "statement" and from "monitoring" to "statement". The subtraction comparison networks further reveal differences in directed cognitive engagement connectivity. Compared to other patterns, LFI displays higher transition frequencies from "others" to "statement" but lower frequencies among "questioning", "monitoring", and "statement". HFI shows higher transition frequencies from "statement" to "questioning" and from "statement" to "monitoring" than HGFI, and higher frequencies from "statement" to "questioning" than HAFI. HGFI demonstrates higher transition frequencies from "statement" to "others" than both HFI and HAFI. Full ONA network details for six tasks appear in Appendix B. All patterns show more "statement" self- connections. LFI has more "others" self- connections than others, while other patterns show more "questioning" and "monitoring" self- connections than LFI. Across tasks, LFI's centroid is closer to the top- right and nearer to "others", indicating stronger "others" association. HFI, HGFI, HAFI have centroids nearer to "statement", suggesting greater focus on "statement" engagement. Additional details on subtraction comparison networks across tasks appear in Appendix C. Throughout tasks, LFI maintains significantly lower transition frequencies between "statement" and both "questioning" and "monitoring" compared to other patterns throughout all tasks, while HGFI and HAFI display minimal differences in nodal transition frequencies with largely similar ONA structures.

Table 6 Frequency and percentage of emotional engagement.  

<table><tr><td rowspan="2">Task</td><td rowspan="2">Pattern</td><td colspan="2">Positive</td><td colspan="2">Negative</td><td colspan="2">Confused</td><td colspan="2">Neutral</td><td colspan="2">Total</td><td rowspan="2">λ²</td><td rowspan="2">p</td></tr><tr><td>(F)</td><td>(%)</td><td>(F)</td><td>(%)</td><td>(F)</td><td>(%)</td><td>(F)</td><td>(%)</td><td>(F)</td><td>(%)</td></tr><tr><td rowspan="4">Task 1</td><td>LFI</td><td>9a</td><td>6.9</td><td>1a</td><td>0.8</td><td>11a</td><td>8.4</td><td>110a</td><td>85</td><td>131</td><td>100</td><td>25.177</td><td>.003**</td></tr><tr><td>HFI</td><td>60a</td><td>11.5</td><td>18a</td><td>3.4</td><td>101b</td><td>19.3</td><td>343b</td><td>65.7</td><td>322</td><td>100</td><td></td><td></td></tr><tr><td>HGFI</td><td>23a</td><td>10.5</td><td>6a</td><td>2.7</td><td>24a</td><td>11</td><td>166a</td><td>75.8</td><td>219</td><td>100</td><td></td><td></td></tr><tr><td>HAFI</td><td>1a</td><td>3.6</td><td>0a</td><td>0</td><td>5a,b</td><td>17.9</td><td>22a,b</td><td>78.6</td><td>28</td><td>100</td><td></td><td></td></tr><tr><td rowspan="3">Task 2</td><td>LFI</td><td>13a</td><td>3.5</td><td>13a</td><td>3.5</td><td>15a</td><td>4.0</td><td>330a</td><td>88.9</td><td>371</td><td>100</td><td>30.138</td><td>.000***</td></tr><tr><td>HGFI</td><td>19b</td><td>8.2</td><td>12a</td><td>5.2</td><td>24b</td><td>10.3</td><td>178b</td><td>76.4</td><td>233</td><td>100</td><td></td><td></td></tr><tr><td>HAFI</td><td>12a,b</td><td>4.2</td><td>2b</td><td>3.7</td><td>51a</td><td>10.8</td><td>243a,b</td><td>84.4</td><td>288</td><td>100</td><td></td><td></td></tr><tr><td rowspan="3">Task 3</td><td>LFI</td><td>73a</td><td>13</td><td>2a</td><td>0.4</td><td>24a</td><td>4.3</td><td>464a</td><td>82.4</td><td>563</td><td>100</td><td>51.318</td><td>.000***</td></tr><tr><td>HGFI</td><td>38b</td><td>22</td><td>5b</td><td>2.9</td><td>21b</td><td>12.1</td><td>109b</td><td>63</td><td>173</td><td>100</td><td></td><td></td></tr><tr><td>HAFI</td><td>16c</td><td>6.8</td><td>2a,b</td><td>0.8</td><td>27b</td><td>11.4</td><td>191a</td><td>80.9</td><td>236</td><td>100</td><td></td><td></td></tr><tr><td rowspan="2">Task 4</td><td>LFI</td><td>72a</td><td>12</td><td>9a</td><td>1.5</td><td>18a</td><td>3</td><td>503a</td><td>83.6</td><td>602</td><td>100</td><td>43.464</td><td>.000***</td></tr><tr><td>HAFI</td><td>41a</td><td>10</td><td>26b</td><td>6.3</td><td>43b</td><td>10.5</td><td>300b</td><td>73.2</td><td>410</td><td>100</td><td></td><td></td></tr><tr><td rowspan="2">Task 5</td><td>LFI</td><td>67a</td><td>8.2</td><td>28a</td><td>3.4</td><td>18a</td><td>2.2</td><td>704a</td><td>86.2</td><td>817</td><td>100</td><td>20.280</td><td>.000***</td></tr><tr><td>HAFI</td><td>21a</td><td>11.1</td><td>2a</td><td>1.1</td><td>15b</td><td>7.9</td><td>155b</td><td>80.0</td><td>190</td><td>100</td><td></td><td></td></tr><tr><td rowspan="3">Task 6</td><td>LFI</td><td>51a</td><td>9.3</td><td>8a</td><td>1.5</td><td>22a</td><td>4.0</td><td>469a</td><td>85.3</td><td>550</td><td>100</td><td>22.790</td><td>.001**</td></tr><tr><td>HGFI</td><td>15a</td><td>10.9</td><td>5a</td><td>3.6</td><td>17b</td><td>12.4</td><td>100b</td><td>73</td><td>137</td><td>100</td><td></td><td></td></tr><tr><td>HAFI</td><td>15a</td><td>11.6</td><td>0a</td><td>0</td><td>6a,b</td><td>4.7</td><td>108a,b</td><td>83.7</td><td>129</td><td>100</td><td></td><td></td></tr></table>

Note:  $^{**}p < .01$ $^{**}p < .001$  Superscript letters (a, b, c) indicate significant differences between groups based on post-hoc pairwise comparisons after Chi-square test with Bonferroni correction. Groups sharing the same letter (e.g., a, a) are not significantly different from each other; Groups with different letters (e.g., a vs. b) are significantly different; Combined letters (e.g., a,b) denote groups with no significant pairwise difference but differences from other groups may exist.

Table 7 Frequency and percentage of cognitive engagement.  

<table><tr><td rowspan="2">Task</td><td rowspan="2">Pattern</td><td colspan="2">Questioning</td><td colspan="2">Statement</td><td colspan="2">Monitoring</td><td colspan="2">Others</td><td colspan="2">Total</td><td rowspan="2">λ²</td><td rowspan="2">p</td></tr><tr><td>(F)</td><td>(%)</td><td>(F)</td><td>(%)</td><td>(F)</td><td>(%)</td><td>(F)</td><td>(%)</td><td>(F)</td><td>(%)</td></tr><tr><td rowspan="4">Task 1</td><td>LFI</td><td>6a</td><td>4.6</td><td>92a</td><td>70.2</td><td>21a</td><td>16.0</td><td>12a</td><td>9.2</td><td>131</td><td>100</td><td>46.654</td><td>.000***</td></tr><tr><td>HFI</td><td>77b</td><td>14.8</td><td>346a</td><td>66.3</td><td>94a</td><td>18.0</td><td>5b</td><td>1.0</td><td>522</td><td>100</td><td></td><td></td></tr><tr><td>HGFI</td><td>20a,b</td><td>9.1</td><td>139a</td><td>63.5</td><td>43a</td><td>19.6</td><td>17a</td><td>7.8</td><td>219</td><td>100</td><td></td><td></td></tr><tr><td>HAFI</td><td>4a,b</td><td>14.3</td><td>18a</td><td>64.1</td><td>6a</td><td>21.4</td><td>0a,b</td><td>0.0</td><td>28</td><td>100</td><td></td><td></td></tr><tr><td rowspan="3">Task 2</td><td>LFI</td><td>17a</td><td>4.6</td><td>204a</td><td>55.0</td><td>28a</td><td>7.5</td><td>122a</td><td>32.9</td><td>271</td><td>100</td><td>205.338</td><td>.000***</td></tr><tr><td>HGFI</td><td>19a,b</td><td>8.2</td><td>150a</td><td>67</td><td>57b</td><td>24.5</td><td>1b</td><td>0.4</td><td>233</td><td>100</td><td></td><td></td></tr><tr><td>HAFI</td><td>28b</td><td>9.7</td><td>212b</td><td>73.6</td><td>46a</td><td>16.0</td><td>2b</td><td>0.7</td><td>288</td><td>100</td><td></td><td></td></tr><tr><td rowspan="3">Task 3</td><td>LFI</td><td>34a</td><td>6.0</td><td>370a</td><td>65.7</td><td>38a</td><td>6.7</td><td>121a</td><td>21.5</td><td>563</td><td>100</td><td>90.768</td><td>.000***</td></tr><tr><td>HGFI</td><td>24b</td><td>13.9</td><td>123a,b</td><td>71.1</td><td>19a</td><td>11.0</td><td>7b</td><td>4.0</td><td>173</td><td>100</td><td></td><td></td></tr><tr><td>HAFI</td><td>36b</td><td>15.3</td><td>170b</td><td>75.8</td><td>10a</td><td>9.1</td><td>2b</td><td>0.8</td><td>226</td><td>100</td><td></td><td></td></tr><tr><td rowspan="2">Task 4</td><td>LFI</td><td>31a</td><td>5.1</td><td>375a</td><td>62.3</td><td>53a</td><td>8.8</td><td>143a</td><td>23.8</td><td>602</td><td>100</td><td>24.327</td><td>.000***</td></tr><tr><td>HAFI</td><td>45b</td><td>11.0</td><td>269a</td><td>65.6</td><td>40a</td><td>9.8</td><td>56b</td><td>13.7</td><td>410</td><td>100</td><td></td><td></td></tr><tr><td rowspan="2">Task 5</td><td>LFI</td><td>55a</td><td>6.7</td><td>475a</td><td>58.1</td><td>53a</td><td>6.5</td><td>234a</td><td>28.6</td><td>817</td><td>100</td><td>75.146</td><td>.000***</td></tr><tr><td>HAFI</td><td>15a</td><td>7.9</td><td>142b</td><td>74.7</td><td>31b</td><td>16.3</td><td>2b</td><td>1.1</td><td>190</td><td>100</td><td></td><td></td></tr><tr><td rowspan="3">Task 6</td><td>LFI</td><td>27a</td><td>4.9</td><td>354a</td><td>64.4</td><td>44a</td><td>8.0</td><td>125a</td><td>22.7</td><td>550</td><td>100</td><td>74.186</td><td>.000***</td></tr><tr><td>HGFI</td><td>17b</td><td>12.4</td><td>76a</td><td>55.5</td><td>30b</td><td>21.9</td><td>14b</td><td>10.2</td><td>137</td><td>100</td><td></td><td></td></tr><tr><td>HAFI</td><td>4a</td><td>3.1</td><td>107b</td><td>82.9</td><td>17a,b</td><td>13.2</td><td>1c</td><td>0.8</td><td>129</td><td>100</td><td></td><td></td></tr></table>

Note:  $^{**}p < .001$  . Subscript letters (a, b, c) indicate significant differences between groups based on post-hoc pairwise comparisons after Chi-square test with Bonferroni correction. Groups sharing the same letter (e.g., a, a) are not significantly different from each other; Groups with different letters (e.g., a vs. b) are significantly different; Combined letters (e.g., a,b) denote groups with no significant pairwise difference but differences from other groups may exist.

# 4.3. What is the association between the feedback interaction behavior patterns and academic performance?

4.3. What is the association between the feedback interaction behavior patterns and academic performance?Table 8 shows the differences in academic performance among students with different feedback interaction behavior patterns across six tasks. The Kruskal- Wallis test results indicate that in each task, there were significant differences in academic performance among students with different feedback interaction behavior patterns (all  $p$ - values  $<.05$ ). Specifically, students in the HFI, HGFI, and HAFI patterns had higher academic performance than those in the LFI pattern. This suggests that HFI, HGFI, and HAFI are more likely to be associated with high academic performance, while LFI is more likely to be associated with low academic performance. Moreover, the differences in academic performance among students in the HFI, HGFI, and HAFI patterns were relatively small. In Task 1, a significant difference in academic performance was found between the HFI and LFI patterns. Additionally, in Tasks 2- 6, there were differences in academic performance between the HAFI and LFI patterns. These results indicate that the differences in the way groups interact with AI- based visual learning analytics feedback and GenAI feedback can lead to variations in academic performance.

# 5.Discussion

# 5.1. The interactive behavior patterns of student groups with Visual-GenAI learning analytics feedback

5. Discussion5.1. The interactive behavior patterns of student groups with Visual- GenAI learning analytics feedbackRegarding the RQ1, the results show there are four types of differential feedback interaction patterns between groups and Visual- GenAI learning analytics feedback. Specifically, HFI only occurred during the first task. This suggests a novelty effect in students' use of Visual- GenAI learning analytics feedback, meaning the technological novelty can stimulate learner interest initially, but sustained engagement with the feedback is limited after the first use, consistent with Sansom et al. (2020). This is confirmed in interview responses like, "We had no prior experience with GenAI feedback. Out of curiosity, we initially looked into the suggestions GenAI could provide." The LFI pattern is the most frequent, reflecting low feedback engagement among most students, similar to previous studies (Jin, Maheshi, et al., 2025). This pattern reminds us not to overestimate students' engagement with new technology. Factors such as learning motivation (Meek et al., 2017), feedback experience (Zhan et al., 2022), and AI literacy (Jin, Yang, et al., 2025) influence feedback use. From the Technology Acceptance Model (TAM) perspective, students' attitudes are key in deciding to use or reject a system (Granić & Marangunić, 2019). If feedback tools are seen as effective and easy to use, students may continue using them. This could explain the HGFI and HAFI interaction patterns. HAFI might occur because GenAI feedback requires active switching from AI- based visual learning analytics feedback, but its interaction frequency is lower than HFI. In this study, the emergence of HAFI may be attributed to the requirement for additional clicks to access GenAI feedback on the AI- based visual learning analytics feedback page. However, groups exhibiting the HFI pattern consulted GenAI feedback at a significantly higher frequency compared to HAFI. Feedback theory distinguishes internal and external feedback (Narciss, 2013; Narciss et al., 2022). When AI- based visual learning analytics feedback, as an external source, effectively stimulates and transforms into internal feedback processes, aiding students in adapting, selecting, and utilizing regulatory strategies, additional GenAI feedback support may be unnecessary (Patchan Melissa & Puranik Cynthia, 2016; Mejeh et al., 2024). Furthermore, from a cognitive- load theory viewpoint, extra information can increase cognitive load (Sweller, 1988). This finding is significant as it reveals different groups have varying needs and preferences for learning analytics feedback tools, leading to differences in their interaction with feedback. This variation further validates the human- centered learning

![](images/a2097098bb98cd42f630a88d93cea1ca52ab73f237b1a4662a54195087a00a11.jpg)  
Fig. 11. The cognitive engagement network of different pattern in Task 1.

Table 8 The differences in students' academic performance.  

<table><tr><td>Task</td><td>Pattern</td><td>N</td><td>Mean</td><td>SD</td><td>H</td><td>p</td><td>Post-hoc test</td></tr><tr><td rowspan="4">Task 1</td><td>LFI</td><td>28</td><td>70.05</td><td>9.644</td><td>11.326</td><td>.010*</td><td>HFI &amp;gt; LFI *</td></tr><tr><td>HFI</td><td>12</td><td>80.71</td><td>7.448</td><td></td><td></td><td></td></tr><tr><td>HGFI</td><td>8</td><td>74.81</td><td>11.511</td><td></td><td></td><td></td></tr><tr><td>HAFI</td><td>4</td><td>79.50</td><td>7.958</td><td></td><td></td><td></td></tr><tr><td rowspan="3">Task 2</td><td>LFI</td><td>36</td><td>70.96</td><td>10.066</td><td>10.208</td><td>.006 **</td><td>HAFI &amp;gt; LFI *</td></tr><tr><td>HGFI</td><td>4</td><td>80.88</td><td>5.266</td><td></td><td></td><td></td></tr><tr><td>HAFI</td><td>12</td><td>80.71</td><td>7.685</td><td></td><td></td><td></td></tr><tr><td rowspan="3">Task 3</td><td>LFI</td><td>36</td><td>71.11</td><td>10.113</td><td>9.456</td><td>.009 **</td><td>HAFI &amp;gt; LFI *</td></tr><tr><td>HGFI</td><td>4</td><td>79.50</td><td>7.083</td><td></td><td></td><td></td></tr><tr><td>HAFI</td><td>12</td><td>80.708</td><td>7.685</td><td></td><td></td><td></td></tr><tr><td rowspan="2">Task 4</td><td>LFI</td><td>36</td><td>71.11</td><td>10.113</td><td>9.451</td><td>.002 **</td><td>-</td></tr><tr><td>HAFI</td><td>16</td><td>80.41</td><td>7.324</td><td></td><td></td><td></td></tr><tr><td rowspan="2">Task 5</td><td>LFI</td><td>44</td><td>72.45</td><td>9.954</td><td>6.502</td><td>.011*</td><td>-</td></tr><tr><td>HAFI</td><td>8</td><td>82.31</td><td>7.769</td><td></td><td></td><td></td></tr><tr><td rowspan="3">Task 6</td><td>LFI</td><td>40</td><td>71.75</td><td>9.90</td><td>9.053</td><td>.011*</td><td>HAFI &amp;gt; LFI *</td></tr><tr><td>HGFI</td><td>4</td><td>79.50</td><td>7.083</td><td></td><td></td><td></td></tr><tr><td>HAFI</td><td>8</td><td>82.31</td><td>7.769</td><td></td><td></td><td></td></tr></table>

Note:  $^\ast p < .05$ $^{**}p < .01$ $^{**}p < .001$

analytics perspective, emphasizing the need to consider student differences and requirements when designing learning analytics feedback, and highlights the importance of creating adaptive and customizable learning analytics feedback (Wiley et al., 2024).

5.2. The association between feedback interaction behavior patterns with behavioral, emotional, and cognitive engagement

Regarding the RQ2, the results indicate that there are significant differences in behavioral engagement across different feedback interaction behavior patterns. Specifically, HAFI demonstrates higher- level behavioral engagement than LFI, which shows the lowest level of behavioral engagement. This supports prior research that groups' interactions with learning analytics feedback can promote behavioral engagement in CSCL (Lin et al., 2016; Ma et al., 2023; Ollesch et al., 2022). Also, HGFI shows higher behavioral engagement than HAFI. Although AI- based visual learning analytics feedback offers fixed overall group information condition data (Lin et al., 2021), students need to make extra effort to translate it into beneficial learning strategies (Sahin & Ifenthaler, 2021). GenAI feedback can directly offer instructional recommendations to groups like a real- life teacher, encouraging metacognitive reflection to adjust learning behavior (Floden, 2025). This resonates with recent advocacy (Fernandez- Nieto et al., 2021, 2024) that in CSCL, visual learning analytics feedback with additional scaffolding helps groups to reflect on data and gain useful insights, such as interpreting and applying data. However, our results add to this understanding by revealing a key difference: the HFI pattern shows higher behavioral engagement than the HGFI and HAFI patterns. This suggests the effect of additional scaffolding may also relate to students' active interaction with the scaffolding, which hasn't been fully explored in previous research.

Secondly, the results reveal significant differences in emotional engagement among different feedback interaction behavior patterns. HFI shows the richest emotional engagement, with more positive, negative, and confused emotions, whereas LFI exhibits the most superficial, mostly neutral emotions. This aligns with prior findings that groups' participation in learning analytics feedback in CSCL can enhance emotional engagement (Peng et al., 2022; Su et al., 2024). Additionally, HGFI demonstrates more emotional engagement than HAFI, with much lower neutral emotions across most tasks. Research on socially shared regulation of learning helps explain this. It indicates that boosting students' awareness of collaborative learning processes supports their involvement in group shared regulation, helping resolve or avoid cognitive, social, or emotional conflicts for more effective and efficient learning (Noroozi et al., 2019). In this study, GenAI provided groups with more direct guidance and suggestions, assisting them in examining and handling key phases of the regulation process. Students could take action directly based on this information (such as promoting equal participation), thereby enhancing emotional engagement. Also, another possible reason is that students' perception of GenAI feedback triggers more emotional responses (Hamid et al., 2023; Lo et al., 2024). Interview results show that students in HGFI patterns have positive attitudes toward GenAI. To sum up, this study offers empirical evidence supporting the use of GenAI feedback as an enhancer of visual learning analytics feedback to promote higher- level student engagement. This supports and extends recent research on using GenAI to comprehend visual learning analytics feedback (Yun et al., 2025). Future research can further explore the differences in the effects of visual learning analytics feedback when GenAI directly provides learning guidance versus when it supports data comprehension.

The results also indicate significant differences in cognitive engagement among distinct feedback interaction behavior patterns. HFI, HGFI, and HAFI patterns engage more frequently in questioning, statement, and monitoring, with less off- topic discussion than LFI. Similarly, the ONA graphs show that HFI, HGFI, and HAFI have more connections from lower- order cognition (statement) to higher- order cognition (questioning and monitoring), while LFI has stronger connections within lower- order cognition. This is in line with prior studies that groups' participation in learning analytics feedback in CSCL is beneficial for achieving higher levels of cognitive engagement (Peng et al., 2022; Zheng, Long, et al., 2023). Such differences suggest that skilled use of feedback scaffolding can promote students' cognitive development, supporting the concept of the "Zone of Proximal Development" (Vygotsky, 1978). Notably, HAFI and HGFI have comparable cognitive engagement and smaller ONA- structural differences across multiple tasks. Also, HAFI has less off- topic discussion than HGFI, which is inconsistent with their engagement differences in behavioral and emotional aspects. One possible explanation is that AI- based visual learning analytics feedback enhances students' intuitive perception of discussion content. For instance, word- cloud visualizations may alert them to stay on topic (Kong et al., 2025). This might also explain the minimal cognitive engagement differences between HFI and HAFI in the first task, where their ONA structures are also similar. However, HGFI slightly surpasses HAFI in process monitoring. This might be because GenAI feedback offers more direct learning suggestions and guidance than AI- based visual learning analytics feedback, thereby promoting metacognitive regulation (Ng et al., 2024; Shao et al., 2024). Similarly, Hu et al. (2025) found that GenAI feedback can enhance process monitoring in CSCL.

# 5.3. The association between feedback interaction behavior patterns with academic performance

Regarding the RQ3, our findings reveal significant differences in academic performance among students with different feedback interaction behavior patterns. Specifically, substantial differences in academic performance exist between LFI and the other patterns (HFI, HGFI, HAFI). This echoes the findings of previous studies that the interaction between groups and learning analytics feedback in CSCL has an important impact on academic performance (Zheng, Gao, Huang, Shi, & Zhou, 2025; Kong et al., 2025). This may stem from HFI, HGFI, and HAFI patterns being more adept at leveraging learning analytics feedback than LFI, coupled with their higher student engagement levels. Existing evidence indicates that active student engagement in CSCL positively influences academic performance (An et al., 2025; Luo et al., 2023). In contrast, the differences in their impact on academic performance among HFI, HGFI and HAFI were relatively small, despite their student engagement differences. One possible reason is that HFI only occurred in the first task, and its long- term impact on learning was limited as the novelty effect faded (Jeno et al., 2019; Wu & Yu, 2024). As for the smaller differences between HGFI and HAFI, it might be due to the smaller differences in their cognitive engagement. Because prior research has indicated that a certain level of cognitive engagement in CSCL is associated with academic performance (Galikyan & Admiraal,

2019). Moreover, the groups exhibiting the HGFI pattern demonstrated more LFI patterns in other tasks. This might indicate that these groups had higher student engagement when the HGFI pattern occurred. However, their overall academic engagement across the entire learning cycle may not differ significantly from groups with the HAFI pattern. Based on this finding, future research could explore how to sustain high-level feedback engagement in groups and its long-term impact on CSCL.

# 6. Implications

This study makes key theoretical and practical contributions to understanding how visual learning analytics feedback and GenAI feedback influence CSCL. By systematically collecting tracking data to evaluate behavioral differences in groups' interactions with Visual- GenAI learning analytics feedback and their association with student engagement and performance, our findings address a critical research gap: there are differences in how groups interact with learning analytics feedback within CSCL.

Theoretically, this study makes several key contributions. First, it contributes to learning analytics feedback research by uncovering group- level differences in interacting with AI- based visual learning analytics feedback and GenAI feedback in CSCL, and by exploring the causes of these differences. This work strengthens the foundation for understanding group engagement with learning analytics feedback and offers insights into the models of successful feedback use (Garino, 2020) and the contextual model of learning analytics feedback use (Jung & Wise, 2025), showing that these differences result from the interplay of multiple factors. Second, it advances research on generative AI literacy by showing that students' attitudes toward GenAI affect their interaction with GenAI feedback. Positive attitudes enhance this interaction and help students benefit from the feedback, as emphasized by Marengo et al. (2025). We also highlights the importance of cultivating positive student attitudes toward GenAI and effectively integrating this technology into learning environments (Yilmaz & Yilmaz, 2025). Third, it extends the traditional technology acceptance model by showing that interaction with Visual- GenAI learning analytics feedback varies over time, suggesting that evaluations of learning analytics tools should include a temporal dimension (Lee & Wu, 2025). Notably, this study advances the design of tightly coupled human- AI interactions. Our findings show that differences in how groups interact with AI- based visual learning analytics feedback and GenAI feedback can lead to varying student engagement and academic performance. This challenges the assumption that AI support alone automatically enhances learning. Additionally, students interacting frequently with only one feedback type do not necessarily achieve better learning gains. This indicates that learning gains are a shared responsibility between humans and AI, rather than being solely the responsibility of one or the other (Cukunova, 2025). We also found that groups interacting more with both feedback types show better monitoring of cognitive engagement and have superior academic performance, highlighting the promise of hybrid human- AI regulation (Molenaar, 2022; Jarvelä et al., 2023). This hybrid approach offers new ways to address risks from students' direct use of GenAI, such as reduced self- regulated learning ability (Darvishi et al., 2024) and "metacognitive laziness" (Fan et al., 2024).

This study offers empirical evidence to guide the design and implementation of AI- enhanced learning environments. For technology developers, our results stress that developing learning analytics feedback tools requires considering students' needs and participation, in line with Wiley et al.'s (2024) human- centered learning analytics design approach. For researchers, we provide a template to track groups' differing interactions with various learning analytics feedback types (such as AI- based visual learning analytics feedback and GenAI feedback) and their impact on CSCL. For educators, we recommend providing both AI- based visual learning analytics feedback and GenAI feedback in CSCL to meet diverse group needs and promote educational equity. Groups weak in data visualization can gain insights from GenAI feedback, while those with low GenAI literacy or acceptance can benefit from visual learning analytics feedback. Our findings also highlight that educators should not just give groups learning analytics feedback but also recognize groups' differing use of it. They should focus on groups that underutilize such feedback, identify reasons for this underuse, and help these groups use the feedback effectively to prevent negative learning cycles.

# 7. Conclusion, limitations, and future directions

In this study, we explored the behavioral differences in groups' interactions with Visual- GenAI learning analytics feedback and their association with student engagement and academic performance. All groups were provided with an Visual- GenAI learning analytics feedback tool that combines AI- based visual learning analytics feedback and GenAI feedback to assist them in monitoring and enhancing their CSCL learning process. Previous research indicated variations in students' interactions with AI- based visual learning analytics feedback (Zamecnik et al., 2022) or GenAI feedback (Jin, Yang, et al., 2025). Our study further evaluated the differences in groups' interactions with AI- GenAI hybrid feedback and verified their impact on CSCL by systematically collecting tracking data. For RQ1, we used hierarchical cluster analysis to cluster the interaction data of 13 groups with the two feedback types across six collaborative discussion tasks and identified four distinct patterns of feedback interaction behavior. We also conducted interviews to uncover the key factors behind these differences, including group collaboration quality, technology perception, and academic stress. For RQ2, we collected group discussion data, measured behavioral engagement levels, and encoded emotional and cognitive engagement based on existing frameworks. Using Kruskal- Wallis tests and Chi- square tests, we found significant differences in these engagement aspects among the four interaction patterns. For RQ3, we found via Kruskal- Wallis tests that students with different feedback interaction patterns significantly differed in academic performance, and longitudinal data from six tasks reinforced the robustness of these differences. These findings provide solid empirical evidence for learning analytics feedback research in CSCL, surpassing many prior studies that only compared using versus not using such feedback. And these findings especially confirm a key difference: the level of groups' interactions with learning analytics feedback. This can deepen our understanding of how groups' interactions with visual learning analytics feedback and GenAI feedback enhances student engagement and academic performance in CSCL.

Despite providing innovative insights, this study has several limitations that need to be addressed in future work. First, students were not involved in the design of the feedback tool. This lack of user participation may affect their perception and use of the feedback tool to some extent, and may also hinder the comparability with other studies that develop feedback tools using human- centered design approaches to interpret research findings. Future research should incorporate human- centered design principles. Second, in the Visual- GenAI learning analytics feedback tool, GenAI feedback needs to be accessed by jumping from the AI- based visual learning analytics feedback, which may have affected students' interaction with both types of feedback. Future research could present the two types of feedback as separate entities to more clearly distinguish the interaction differences between groups and them. Meanwhile, applying the same intervention conditions to all groups in this study limited the assessment of GenAI feedback's true capacity to support visual learning analytics feedback. Future research should conduct ablation studies or add control groups to more clearly identify the unique role of GenAI feedback in supporting visual learning analytics feedback. In addition, although this study was conducted in a real- world course over a semester, the participants were mainly students from software engineering backgrounds with a relatively small group sample size, which limits the generalizability of the findings and may lead to potential biases. For future research, it is essential to include a more diverse group of participants to ensure the broader applicability of the results. Lastly, this study only used group discussion text data to assess student engagement, which may not be sufficient to comprehensively and accurately reflect students' emotional states. Therefore, future research could consider collecting more diverse data, such as physiological data (heart rate, skin conductance, etc.), to more comprehensively assess student engagement. Moving forward, we plan to continue in- depth research on Visual- GenAI learning analytics feedback, explore more effective feedback strategies, and understand their mechanisms to further enhance the effectiveness and quality of CSCL. Meanwhile, we will also focus on key issues such as technological ethics and privacy protection to ensure that AI applications in education are both efficient and secure.

# CRediT authorship contribution statement

Xinghan Yin: Writing - original draft, Investigation, Data curation. Junmin Ye: Writing - review & editing, Supervision, Conceptualization. Shuang Yu: Writing - review & editing, Data curation. Honghui Li: Writing - review & editing, Validation. Qingtang Liu: Validation, Software. Gang Zhao: Validation.

# Ethical approval

The procedures for human participants involved in this study were consistent with the ethical standards of Central China Normal University and the 1975 Helsinki Declaration. The approval number is CCNU- IRB- 202201015b.

# Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

# Acknowledgments

This work was supported by a project funded by the National Natural Science Foundation of China (No.62377020; No.62277021). This work was also conducted as part of a Research Project Supported by the Fundamental Research Funds for the Central Universities (No.2024CXZZ047).

# Appendix A. The 24 software architecture and design pattern cases that the group needs to discuss

Case 1: Focus on the design of "Simulating a dual- system boot selection interface." The group should discuss how to apply the Abstract Factory Pattern to implement the selection mechanism between Windows and Unix operating systems. The group should analyze the applicability of this pattern and discuss the functional goals of the design: the final prototype should present a selection interface with options "1. Windows" and "2. Unix," and the discussion may extend to the potential of this pattern in other similar selection scenarios.

Case 2: Focus on the design of "Building a unified mechanism for describing faculty and student evaluation profiles." The group should discuss how to use the Factory Pattern to support teachers and students in using the same method for filling out and printing evaluation content. During the discussion, the group should analyze the applicability of this pattern in handling the creation of different objects and discuss solutions to meet the following functional goals: the ability to construct and initialize evaluation profile objects for both teachers and students separately, ensuring that the same mechanism can be used to output the evaluation profile descriptions for these two different types of objects.

Case 3: Focus on the design of "Building a unified reuse mechanism for functional modules in graphical modeling software." The group should discuss how to apply the Prototype Pattern to support users in efficiently copying and modifying pre- configured functional modules (e.g., sine wave, square wave, triangular wave signal generators, and math operation modules). The discussion should analyze the applicability of this pattern in handling the cloning of existing objects (rather than creating new ones) and address solutions to meet the following functional goals: the ability to quickly create and initialize a functional module object identical to the prototype through cloning, ensuring users can use the same cloning mechanism to replicate any functional module object and modify it

afterward.

Case 4: Focus on the design of "Building a unified generation mechanism for functional modules to source code." The group should discuss how to combine the Abstract Factory Pattern with the Prototype Pattern to support users in automatically generating executable source code in different target languages (e.g., C or Python) using the same design process. The discussion should analyze the applicability of this combined pattern in handling the dynamic creation of multi- language code generators and the reuse of core module designs, addressing solutions to meet the following functional goals: the ability to construct and initialize language- specific code generator objects based on user language selection (Abstract Factory Pattern) while leveraging the prototype pattern to reuse core module designs, ensuring the same generation interface can output source code files in these two different languages that carry the same business logic and are directly compilable and executable.

Case 5: Focus on the design of "Building a unified access mechanism for collaborative multi- user documents." The group should discuss how to use the Singleton Pattern to support two developers (Developer A and Developer B) in using the same document operation interface for real- time reading and writing of shared content. The discussion should analyze the applicability of this pattern in ensuring a globally unique document instance (e.g., a software design document) and address solutions to meet the following functional goals: the ability to construct a unique document object via the Singleton Pattern for initialization and loading of historical content when different accounts (A/B) log in, ensuring both developers use the same document access mechanism to achieve real- time data synchronization and strictly maintain cross- scenario document data consistency.

Case 6: Focus on the design of "Building a unified composition mechanism for ui components." The group should discuss how to apply the Composite Pattern to support developers in using the same operation interface to handle simple controls (e.g., checkboxes) and composite containers (e.g., setting panels). The discussion should analyze the applicability of this pattern in implementing a "part- whole" hierarchy (e.g., a wireframe property panel composed of checkboxes for color, thickness, and line type) and address solutions to meet the following functional goals: the ability to uniformly construct basic components (individual checkboxes) and composite components (dialogs) via the Composite Pattern, allowing them to share the same behavioral interfaces (e.g., rendering, event handling), ensuring developers can use the exact same mechanism to assemble and manipulate UI elements of any complexity.

Case 7: Focus on the design of "Building a unified collaboration mechanism for legacy and new functional modules." The group should discuss how to apply the Adapter Pattern to support developers in retaining the original text editor module's interface while uniformly integrating new functional modules via adapters. The discussion should analyze the applicability of this pattern in converting legacy class interfaces to client target interfaces and address solutions to meet the following functional goals: the ability to construct font color adapters and print adapters via the Adapter Pattern, initializing the binding of the legacy class's (text editor) output behavior to the adapters, ensuring the client class (new functional modules) can use the exact same invocation mechanism to operate the adapters.

Case 8: Focus on the design of "Building a unified mechanism for dynamic replacement of storage subsystem implementations." The group should discuss how to apply the Bridge Pattern to support the development team in transparently switching between different storage implementations (e.g., stub, file system, database) using the same system interface. The discussion should analyze the applicability of this pattern in separating the abstraction layer from the implementation layer and address solutions to meet the following functional goals: the ability to construct stub storage implementations, file- based storage implementations, and relational database storage implementations via the Bridge Pattern, initializing the binding of the storage subsystem's core functionality to a unified abstract interface, ensuring the system can use the exact same invocation mechanism to dynamically switch between these three different implementations.

Case 9: Focused on the design of "Building a unified extension mechanism for communication encryption and verification," the group should discuss how to apply the Decorator Pattern to support users in dynamically combining encryption and verification functions through the same interface. The discussion should analyze the pattern's applicability in avoiding class float while achieving transparent function stacking, and propose solutions to meet these functional goals: being able to construct basic communication objects and nested encryption/verification decoration layers using the Decorator Pattern, initialize core communication data at the start of the decoration chain, and ultimately ensure users can trigger any combined functions using identical calling mechanisms, completely resolving the conflict between code duplication and dynamic expansion.

Case 10: Focused on the design of "Building a unified proxy mechanism for movie ticket purchasing services," the group should discuss how to apply the Proxy Pattern to support users in indirectly accessing real cinema ticketing services through online platforms using a unified interface. The discussion should analyze the pattern's applicability in reducing access costs while maintaining operational transparency, and propose solutions to meet these functional goals: being able to construct real ticketing objects (cinemas) and proxy objects (ticketing platforms) using the Proxy Pattern, initialize real ticketing data (such as movie names and prices) to the proxy, and ultimately ensure users can use identical ticket- purchasing mechanisms to achieve seamless cross- channel purchasing experiences and optimized resource access.

Case 11: Focused on the design of "Building a unified sharing mechanism for scenic spot tickets," the group should discuss how to apply the Flyweight Pattern to support tour guides in enabling multiple tourists to reuse the same scenic resources through group reservation forms. The discussion should analyze the pattern's applicability in separating intrinsic ticket states (fixed scenic information) from extrinsic states (dynamic tourist information), and propose solutions to meet these functional goals: being able to construct shareable group ticket objects and tourist- specific contexts (tourist lists) using the Flyweight Pattern, initialize scenic booking logic by binding it to group ticket flyweights, and ultimately ensure guides can output single group tickets using identical issuing mechanisms, completely eliminating resource redundancy caused by duplicate bookings.

Case 12: Focused on the design of "Building a unified simplification mechanism for home theater device operations," the group should discuss how to apply the Facade Pattern to support users in uniformly managing complex subsystems (audio systems/

projectors/speakers/lighting) through a single controller. The discussion should analyze the pattern's applicability in encapsulating multi- device coordination logic, and propose solutions to meet these functional goals: being able to construct controller objects (unified interface) and associate initialized device instances using the Facade Pattern, and ultimately ensure users can coordinate all devices with one click (such as pressing a "Movie Mode" button) using identical triggering mechanisms, completely hiding subsystem complexity and achieving ultimate operational simplification.

Case 13: Focused on the design of "Building a unified delegation mechanism for report approvals," the group should discuss how to apply the Chain of Responsibility Pattern to support employees in triggering multi- level leadership approval workflows through a single submission interface. The discussion should analyze the pattern's applicability in decoupling requesters (employees) from handlers (leadership chain) while dynamically delegating approval authority, and propose solutions to meet these functional goals: being able to construct leadership objects at different levels and initialize their approval permissions and superior relationships using the Chain of Responsibility Pattern, and ultimately ensure employees can trigger approvals using identical report submission mechanisms - with reports automatically passing along the responsibility chain- achieving low- coupling, extensible approval processes.

Case 14: Focused on the design of "Building a unified notification mechanism for event responses," the group should discuss how to apply the Observer Pattern to support subject objects in linking multiple observers using the same event notification interface. The discussion should analyze the pattern's applicability in decoupling subjects from observers (such as email systems and users) while supporting dynamic subscription/notification, and propose solutions to meet these functional goals: being able to construct subject objects and observer objects using the Observer Pattern, register observers to the subject's subscription list, and ultimately ensure subjects can automatically trigger synchronized responses from all observers using identical state- change notification mechanisms, achieving a low- coupling event- driven architecture.

Case 15: Focused on the design of "Building a unified coordination mechanism for multi- department collaboration," the group should discuss how to apply the Mediator Pattern to support the design department, production department, warehouse department, sales department, and customer service department in achieving cross- department interaction through a mediator using the same message- passing interface. The discussion should analyze the pattern's applicability in eliminating direct inter- department dependencies while centralizing workflow control, and propose solutions to meet these functional goals: being able to construct department objects and initialize their registration to the mediator using the Mediator Pattern, and ultimately ensure all departments use identical message- forwarding mechanisms, completely solving data inconsistency and communication blockage problems to achieve low- coupling efficient collaboration.

Case 16: Focused on the design of "Building a unified execution mechanism for cooking strategies," the group should discuss how to apply the Strategy Pattern to support chefs in dynamically switching between different dish preparation logics through the same cooking interface. The discussion should analyze the pattern's applicability in decoupling chefs from specific cooking algorithms while supporting runtime strategy replacement, and propose solutions to meet these functional goals: being able to construct hotpot strategy classes and sushi strategy classes using the strategy pattern, initialize the chef object as the strategy invoker, and ultimately ensure chefs can trigger different strategies using identical execution mechanisms, achieving low- coupling, extensible customized dish production.

Case 17: Focused on the design of "Building a unified encapsulation mechanism for dish ordering behavior," the group should discuss how to apply the Command Pattern to support customers in triggering ordering operations for different dishes through a standardized interface. The discussion should analyze the pattern's applicability in decoupling customers from chefs while supporting behavior extension, and propose solutions to meet these functional goals: being able to construct specific command objects for hamburgers, pizzas, and hot dry noodles using the command pattern, initialize the chef as the command receiver, and ultimately ensure customers can use identical ordering mechanisms to achieve unified encapsulation and flexible management of action requests and execution logic.

Case 18: Focused on the design of "Building a unified recovery mechanism for work state backtracking," the group should discuss how to apply the Memento Pattern to support secretaries in saving and rolling back work states through the same interface. The discussion should analyze the pattern's applicability in separating state capture from state management while ensuring data encapsulation, and propose solutions to meet these functional goals: being able to construct originators, mementos, and caretakers using the memento pattern, initialize editing progress (such as text content and cursor position) by binding it to the originator, and ultimately ensure secretaries can use identical triggering mechanisms to completely solve progress loss issues caused by task interruptions.

Case 19: Focused on the design of "Building a unified framework mechanism for cooking processes," the group should discuss how to apply the Template Method Pattern to support chefs in dynamically customizing specific step implementations for different dishes using the same cooking algorithm skeleton. The discussion should analyze the pattern's applicability in fixing basic processes while delaying derived class implementations, and propose solutions to meet these functional goals: being able to define cooking process templates in abstract classes using the template method pattern, initialize basic cooking logic by binding it to abstract templates, and ultimately ensure chefs can trigger differentiated cooking using identical execution mechanisms.

Case 20: Focused on the design of "Building a unified traversal mechanism for snack aggregate objects," the group should discuss how to apply the Iterator Pattern to support users in sequentially processing different types of snacks in a grocery basket through the same access interface. The discussion should analyze the pattern's applicability in decoupling aggregate structures (grocery basket) from traversal logic (sequential tasting) while supporting dynamic element expansion, and propose solutions to meet these functional goals: being able to construct aggregate objects and iterator objects using the iterator pattern, bind snack elements to the aggregate in predetermined order, and ultimately ensure users can use identical retrieval mechanisms.

Case 21: Focused on the design of "Building a unified extension mechanism for stamp classification access," the group should discuss how to apply the Visitor Pattern to support users in dynamically processing seven heterogeneous elements in a stamp album

through the same operation interface. The discussion should analyze the pattern's applicability in separating element storage from new operations without modifying the stamp album structure, and propose solutions to meet these functional goals: being able to construct stamp element objects and initialize their binding to aggregate structures using the visitor pattern, and ultimately ensure users can use identical access mechanisms to achieve decoupling between new operations and element structures while enabling standardized processing.

Case 22: Focused on the design of "Building a unified execution mechanism for fitness state responses," the group should discuss how to apply the State Pattern to support dynamically switching wang's corresponding activities in different fitness phases through the same behavior triggering interface. The discussion should analyze the pattern's applicability in decoupling state objects from behavior implementation while supporting runtime state switching, and propose solutions to meet these functional goals: being able to construct preparation state, exercise state, and relaxation state objects using the state pattern, initialize wang as the state context holder, and ultimately ensure users can use identical triggering mechanisms to achieve flexible adaptation and seamless transitions of state- driven behaviors.

Case 23: Focused on the design of "Building a unified interpretation mechanism for coffee- making instructions," the group should discuss how to apply the Interpreter Pattern to support users in triggering standardized parsing processes of coffee machines through natural language instructions. The discussion should analyze the pattern's applicability in handling domain- specific grammar while dynamically converting instructions into machine operations, and propose solutions to meet these functional goals: being able to construct terminal expressions using the interpreter pattern, initialize grammar rules by binding them to abstract syntax trees, and ultimately ensure users can use identical instruction input mechanisms to achieve precise mapping from domain language to execution actions.

Case 24: Focused on the design of "Building a unified generation mechanism for exercise data reports," the group should discuss how to comprehensively apply the Factory Pattern and Visitor Pattern to support fitness bands in automatically generating daily exercise reports through the same data processing interface. The discussion should analyze the factory pattern's applicability in uniformly encapsulating the creation and initialization of heterogeneous exercise data objects (such as step counts and calories), as well as the visitor pattern's advantages in decoupling data storage (exercise records) from report operations (analysis logic) while supporting expansion of new analysis dimensions, and propose solutions to meet these functional goals: being able to dynamically construct exercise data objects like step counts and calories and initialize their values using the factory pattern, then bind report generators to data aggregate structures using the Visitor pattern, and ultimately ensure fitness bands can use identical triggering mechanisms.

Appendix B. The cognitive engagement network of different pattern in six learning tasks

![](images/5b383e1d8fe1001cc8e5b2ea4da3298738ab278b035b3c076f1506672cf6be15.jpg)

Appendix C. Detailed comparison of different patterns on the directed cognitive engagement network in six learning tasks

![](images/f7af45caa53fcdc6b8774f6e587c9640cd04e7c82adad3277fe2ce10ae2e27e0.jpg)

# Appendix D. Supplementary data

Supplementary data to this article can be found online at https://doi.org/10.1016/j.compedu.2025.105434.

# Data availability

Data will be made available on request.

# References

Alfredo, R, ehrria, , Jin, Y, Yan, , Swicki, Z, Gavi, D, & Mrtin- Maldonado, R. (2024). Hman- entred leang anlytics and AI in edcatio A systematic literature review. Computers and Education: Artificial Intelligence. , Article 100215. https://doi.org/10.1016/j.caeal.2024.100215 Alghami, (2024) pting the mpact of ChatG- gted fedback on technical writing skil of computing stdnts: A blindd stdy. dction and Information Technologies, 29(14), 18901- 18926. https://doi.org/10.1007/s10639- 024- 12594- 2 An, S, Zhang, S, Guo, T, Lu, S, Zhang, W., & Cai, Z. (2025). Impacts of generative AI on student teachers' task performance and collaborative knowledge construction process in mind mapping- based collaborative environment. Computers & Education, 227, Articles 105227. htps:/doi.org/10.1016/j. compedu.2024.105227 Ba, S, Zhan, Y, Huang, L, & Lu, G. (2025). Investigating the impact of ChatGPT- asssted feedback on the dynamics and outcomes of online inquiry- based discussion. British Journal of Educational Technology. http://doi.org/10.1111/bjet.13605 Bao, H, Li, Y, Su, Y, Xing, S, Chen, N. S., & Rose, C. P. (2021). The effects of a learning analytics dashboard on teachers' diagnosis and intervention in computer. supported collaborative learning. Technology, Pedagogy and Education, 30(2), 287- 303. https://doi.org/10.1080/1475939X.2021.1902383 Bedi, A. (2023). Keep learning: Student engagement in an online environment. Online Learning, 27(2), 119- 136. https://doi.org/10.24059/olj.v27i2.3287 Bikowski, D, & Vithanage, R. (2016). Effects of web- based collaborative writing on individual L2 writing development. Language, Leaming and Technology, 20(1), 79- 99. http://lt.msu.edu/issues/february2016/bikowskivithanage. pdf. Bodemer, D, & Denier, J. (2011). Group awareness in CSCL environments. Computers in Human Behavior, 27(3), 1043- 1045. https://doi.org/10.1016/j. chb.2010.07.014 Buder, J, Bodemer, D, & Ogata, H. (2021). Group awareness. In U. Cress, C. Rose, A. F. Wise, & J. Oshima (Eds.), International handbook of computer- supported collaborative learning (pp. 295- 313). Springer. https://doi.org/10.1007/978- 3- 030- 65291- 3_16. Capdeferr,  R. 2012)  n  t  i  n  d  t Learning, 13(2), 26- 44. https://doi.org/10.19473/irrodl.v13i2.1127 Chang, C. Y, Hwang, G. J, & Gau, M. L. (2022). Promoting students learning achievement and self- efficacy: A mobile chatbot approach for nursing training. British Journal of Educational Technology, 53(1), 171- 188. https://doi.org/10.1111/bjet.13158 Chen, C. M., Li, M. C., Chang, W. C., & Chen, X. K. (2021). Developing a topic analysis instant feedback system to facilitate asynchronous online discussion effectiveness. Computers & Education, 163, Article 104095. https://doi.org/10.1016/j.compedu.2020.104095 Chen, C. M., Li, M. C., & Liao, C. K. (2023). Developing a collaborative writing system with visualization interaction network analysis to facilitate online learning performance. Interactive Learning Environments, 31(9), 6054- 6073. https://doi.org/10.1080/10494820.2022.1028851 Chen, S., Ouyang, F, & Jiao, P. (2022). Promoting student engagement in online collaborative writing through a student- facing social learning analytics tool. Journal of Computer Assisted Learning, 38(1), 192- 208. https://doi.org/10.1111/jcal.12604 Cheng, Z., Wang, H., Zhu, X., West, R. E., Zhang, Z., & Xu, Q. (2023). Open badges support goal setting and self- efficacy but not self- regulation in a hybrid learning environment. Computers & Education, 197, Article 104744. https://doi.org/10.1016/j.compedu.2023.104744 Cheng, N., Zhao, W., Xu, X., Liu, H., & Tao, J. (2024). The influence of learning analytics dashboard information design on cognitive load and performance. Education and Information Technologies, 29(15), 19729- 19752. https://doi.org/10.1007/s10639- 024- 12606- 1 Coates, H. (2006). Student engagement in campus- based and online education: University connections. In Student engagement in campus- based and online education. University Connections. https://doi.org/10.424/9780203969465. Corrin, L, & De Barba, P. (2015). How do students interpret feedback delivered via dashboards?. In Proceedings of the fifth international conference on learning analytics and knowledge. https://doi.org/10.1145/2723576.2723662 Cress, U., & Kimerle, J. (2023). Co- constructing knowledge with generative AI tools: Reflections from a CSCL perspective. International Journal of Computer- Supported Collaborative Learning, 10(9), 907- 914. https://doi.org/10.1007/s1142- 023- 0919- 94 Cukurova, M. (2025). The interplay of learning, analytics and artificial intelligence in education: A vision for hybrid intelligence. British Journal of Educational Technology, 56(2), 469- 488. https://doi.org/10.1111/bjet.13514 Dang, B., Huynh, L, Gul, F, Rose, C., Jarvela, S., & Nguyen, A. (2025). Human- AI collaborative learning in mixed reality: Examining the cognitive and socioemotional interactions. British Journal of Educational Technology. https://doi.org/10.1111/bjet.13607 Darvishi, A., Khosravi, H., Sadiq, S., Gasevic, D., & Siemens, G. (2024). Impact of AI assistance on student agency. Computers & Education, 210, Article 104967. https://doi.org/10.1016/j.compedu.2023.104967 Deng, R., Jiang, M., Yu, X., Lu, Y., & Liu, S. (2024). Does ChatGPT enhance student learning? A systematic review and meta- analysis of experimental studies. Computers & Education, Article 105224. https://doi.org/10.1016/j.compedu.2024.105224 Dowell, N. M., Lin, Y., Godfrey, A., & Brooks, C. (2020). Exploring the relationship between emergent Sociocognitive roles, collaborative problem- solving skills, and outcomes: A group communication analysis. Journal of Learning Analytics, 7(1), 38- 57. https://doi.org/10.18508/jla.2020.71.4 Echeverria, V., Nieto, G. F., Zhao, L, Palominos, E, Srivastava, N., Gasevic, D, .. Martinez- Maldonado, R. (2025). A learning analytics dashboard to support students reflection on collaboration. Journal of Computer Assisted Learning, 41(1), Article e13088. https://doi.org/10.1111/jcal.13088 Fan, Y., Tang, L, Le, H., Shen, K., Tan, S., Zhao, Y, .. Gasevic, D. (2024). Beware of metacognitive laziness: Effects of generative artificial intelligence on learning motivation, processes, and performance. British Journal of Educational Technology, 56(2), 489- 530. https://doi.org/10.1111/bjet.13544 Fernandez- Nieto, G. M., Echeverria, V., Shum, S. B., Mangaroska, K., Kitto, K., Palominos, E, .. Martinez- Maldonado, R. (2021). Storytelling with learner data: Guiding student reflection on multimodal team data. IEEE Transactions on Learning Technologies, 14(5), 695- 708. https://doi.org/10.1109/TLT.2021.3131842 Fernandez- Nieto, G. M., Martinez- Maldonado, R., Gheverria, V., Kitto, K., Gasevic, D., & Buckingham Shum, S. (2024). Data storytelling editor: A teacher- centred tool for customising learning analytics dashboard narratives. In Proceedings of the 14th learning analytics and knowledge conference. https://doi.org/10.1145/ 3636555.3636930 Floden, J. (2025). Grading exams using large language models: A comparison between human and AI grading of exams in higher education using ChatGPT. British Educational Research Journal, 51(1), 201- 224. https://doi.org/10.1002/berj.4069 Fredricks, J. A., Blumenfeld, P. C., & Paris, A. H. (2004). School engagement: Potential of the concept, state of the evidence. Review of Educational Research, 74, 59- 109. https://doi.org/10.2307/3516061 Galikyan, I., & Admiraal, W. (2019). Students' engagement in asynchronous online discussion: The relationship between cognitive presence, learner prominence, and academic performance. The Internet and Higher Education, 43, Article 100692. https://doi.org/10.1016/j.ibeduc.2019.100692 Gao, L, Li, X, Li, Y, & Hu, W. (2023). Capturing temporal and sequential patterns of socio- emotional interaction in high- and low- performing collaborative argumentation groups. The Asia- Pacific Education Researcher, 32(6), 817- 831. https://doi.org/10.1007/s40259- 022- 00698- 7 Garino, A. (2020). Ready, willing and able: A model to explain successful use of feedback. Advances in Health Sciences Education, 25(2), 337- 361. https://doi.org/ 10.1007/s10459- 019- 09924- 2 Gasevic, D., Greiff, S., & Shaffer, D. W. (2022). Towards strengthening links between learning analytics and assessment: Challenges and potentials of a promising new bond. Computers in Human Behavior, 134, Article 107304. https://doi.org/10.1016/j.chb.2022.107304 Gencer, G., & Gencer, K. (2024). A comparative analysis of ChatGPT and medical faculty graduates in medical specialization exams: Uncovering the potential of artificial intelligence in medical education. Cureus, 16(8), Article e66517. https://doi.org/10.7759/cureus.66517 Granic, A., & Marangunic, N. (2019). Technology acceptance model in educational context: A systematic literature review. British Journal of Educational Technology, 50 (5), 2572- 2593. https://doi.org/10.1111/bjet.12864 Gu, X., Shao, Y., Guo, X., & Lim, C. P. (2015). Designing a role structure to engage students in computer- supported collaborative learning. The Internet and Higher Education, 24, 13- 20. https://doi.org/10.1016/j.ibeduc.2014.09.002 Guo, K., Zhang, E. D., Li, D., & Yu, S. (2024). Using AI- supported peer review to enhance feedback literacy: An investigation of students' revision of feedback on peers' essays. British Journal of Educational Technology. https://doi.org/10.1111/bjet.13540

Hamid, H., Zulkifli, K., Naimat, F., Yaacob, N. L. C., & Ng, K. W. (2023). Exploratory study on student perception on the use of chat AI in process- driven problem- based learning. Currents in Pharmacy Teaching and Learning, 15(12), 1017- 1025. https://doi.org/10.1016/j.cptl.2023.10.001Han, J., Kim, K. H., Rhee, W. & Cho, Y. H. (2023). Learning analytics dashboards for adaptive support in face- to- face collaborative argumentation. Computers & Education, 163, Article 104041. https://doi.org/10.1016/j.compedu.2020.104041He, S., Shi, X., Choi, T. H., & Zhai, J. (2023). How do students' roles in collaborative learning affect collaborative problem- solving competency? A systematic review of research. Thinking Skills and Creativity, Article 101423. https://doi.org/10.1016/j.tsc.2023.101423Higgins, S., Dutta, S., & Kakar, R. S. (2024). Machine learning for lumbar and pelvis kinematics clustering. Computer Methods in Biomechanics and Biomedical Engineering, 27(10), 1332- 1345. https://doi.org/10.1080/10255842.2023.2241593Hou, H. T., Wang, S. M., Lin, P. C., & Chang, K. E. (2015). Exploring the learner's knowledge construction and cognitive patterns of different asynchronous platforms: Comparison of an online discussion forum and facebook. Innovations in Education & Teaching International, 52(6), 610- 620. https://doi.org/10.1080/14703297.2013.847381Hu, Y. H. (2022). Effects and acceptance of precision education in an AI- supported smart learning environment. Education and Information Technologies, 27(2), 2013- 2037. https://doi.org/10.1007/s10639- 021- 10664- 3Hu, W., Tian, J., & Li, Y. (2025). Enhancing student engagement in online collaborative writing through a generative AI- based conversational agent. The Internet and Higher Education, 65, Article 100979. https://doi.org/10.1016/j.iheduc.2024.100979Huang, L. Yu, W. Ma, W., Zhong, W., Feng, Z., Wang, H., Liu, T. (2025). A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2), 1- 55. https://doi.org/10.1145/3703155Hutt, S., DePiro, A., Wang, J., Rhodes, S., Baker, R. S., Hieb, G., ... Mills, C. (2024). Feedback on feedback: Comparing classic natural language processing and generative AI to evaluate peer feedback. In Proceedings of the 14th learning analytics and knowledge conference. https://doi.org/10.1145/3636555.3636850Janssen, J., Erkens, G., & Kirschner, P. A. (2011). Group awareness tools: It's what you do with it that matters. Computers in Human Behavior, 27(3), 1046- 1058. https://doi.org/10.1016/j.chb.2010.06.002Järvelä, S., & Hadwin, A. (2024). Triggers for self- regulated learning: A conceptual framework for advancing multimodal research about srl. Learning and Individual Differences, 115, Article 102526. https://doi.org/10.1016/j.lindif.2024.102526Järvelä, S., Nguyen, A., & Hadwin, A. (2023). Human and artificial intelligence collaboration for socially shared regulation in learning. British Journal of Educational Technology, 54(5), 1057- 1076. https://doi.org/10.1111/bjet.13325Jeno, L. M., Vandvik, V., Eliassen, S., & Grytnes, J. A. (2019). Testing the novelty effect of an m- learning tool on internalization and achievement: A self- Determination theory approach. Computers & Education, 128, 398- 413. https://doi.org/10.1016/j.compedu.2018.10.008Jiang, Y. (2025). Interaction and dialogue: Integrating application of artificial intelligence in blended mode writing feedback. The Internet and Higher Education, 64, Article 100975. https://doi.org/10.1016/j.iheduc.2024.100975Jin, F. J. Y., Maheshi, B., Lai, W., Li, Y., Gasevic, D., Chen, G., ... Tsai, Y. S. (2025). Students' perceptions of generative AI- Powered learning analytics in the feedback process: A feedback literacy perspective. Journal of Learning Analytics, 12(1), 152- 168. https://doi.org/10.18608/jla.2025.8609Jin, Y., Yang, K., Yan, L., Echeverria, V., Zhao, L., Alfredo, R., ... Martinez- Maldonado, R. (2025). Chatting with a learning analytics dashboard: The role of generative AI literacy on learner interaction with conventional and scaffolding chatbots. In Proceedings of the 15th international learning analytics and knowledge conference. https://doi.org/10.1145/3706468.370645Jivet, I., Wong, J., Scheffel, M., Valle Torre, M., Specht, H., & Drachsler, H. (2021). Quantum of choice: How learners' feedback monitoring decisions, goals and self- regulated learning skills are related. In LAK21: 11th international learning analytics and knowledge conference. https://doi.org/10.1145/3448139.3448179Jukiewicz, M. (2024). The future of grading programming assignments in education: The role of ChatGPT in automating the assessment and feedback process. Thinking Skills and Creativity, 52, Article 101522. https://doi.org/10.1016/j.tsc.2024.101522Jung, Y., & Wise, A. F. (2025). How students engage with learning analytics: Access, action- Taking, and learning Routines with message- based information to support collaborative annotation. Computers & Education, Article 102980. https://doi.org/10.1016/j.compedu.2025.105280Kaliisa, R., Lopez- Pernas, S., Misiejuk, K., Damsa, C., Sobocinski, M. L., Jarvela, S., & Saq, M. (2015). A topical review of research in computer- supported collaborative learning: Questions and Possibilities. Computers & Education, Article 105246. https://doi.org/10.1016/j.compedu.2025.105246Kaliisa, R., Misiejuk, K., Lopez- Pernas, S., Khalil, M., & Saq, M. (2024). Have learning analytics dashboards lived up to the hype? A systematic review of impact on students' achievement, motivation, participation and attitude. In Proceedings of the 14th learning analytics and knowledge conference (pp. 295- 304). https://doi.org/10.1145/3636555.3636884Kao, G. Y. M., & Ruan, C. A. (2022). Designing and evaluating a high interactive augmented reality system for programming learning. Computers in Human Behavior, 132, Article 107245. https://doi.org/10.1016/j.chb.2022.107245Khosas, D. K., & Volet, S. E. (2014). Productive group engagement in cognitive activity and metacognitive regulation during collaborative learning: Can it explain differences in students' conceptual understanding? Metacognition and Learning, 9, 287- 307. https://doi.org/10.1007/s11409- 014- 9117- zkHosravi, H., Shabaninejad, S., Bakharia, A., Sadiq, S., Indulskia, M., & Gasevic, D. (2021). Intelligent learning analytics dashboards: Automated Drill- Down recommendations to support teacher data Exploration. Journal of Learning Analytics, 8(3), 133- 154. https://doi.org/10.18608/jla.2021.7279Kia, F. S., Teasley, S. D., Hatala, M., Karabenick, S. A., & Kay, M. (2020). How patterns of students' dashboard use are related to their achievement and self- regulatory engagement. In Proceedings of the tenth international conference on learning analytics & knowledge. https://doi.org/10.1145/3375462.3375472Kim, J., Jo, I. H., & Park, Y. (2016). Effects of learning analytics dashboard: Analyzing the relations among dashboard utilization, satisfaction, and learning achievement. Asia Pacific Education Review, 17, 13- 24. https://doi.org/10.1007/s12564- 015- 9403- 8Kirschner, F., Paas, F., & Kirschner, P. A. (2009). A cognitive load approach to collaborative learning: United brains for complex tasks. Educational Psychology Review, 21, 31- 42. https://doi.org/10.1007/s10648- 008- 9095- 2Kong, X., Liu, Z., Chen, C., Liu, S., Xu, Z., & Tang, Q. (2020). Exploratory study of an AI- supported discussion representational tool for online collaborative learning in a Chinese university. The Internet and Higher Education, 64, Article 100973. https://doi.org/10.1016/j.iheduc.2019.100973Lahza, H., Khosravi, H., & Demartini, G. (2023). Analytics of learning tactics and strategies in an online learning environment. Journal of Computer Assisted Learning, 39(1), 94- 112. https://doi.org/10.1111/jcal.12729Lee, H. Y., Chen, P. H., Wang, W. S., Huang, Y. M., & Wu, T. T. (2024). Empowering ChatGPT with guidance mechanism in blended learning: Effect of self- regulated learning, higher- order thinking skills, and knowledge construction. International Journal of Educational Technology in Higher Education, 21(1), 16. https://doi.org/10.1186/s41239- 024- 0047- 4Lee, H. Y., & Wu, T. T. (2025). Enhancing blended learning discussions with a scaffolded knowledge integration- based ChatGPT mobile instant messaging system. Computers & Education, Article 105375. https://doi.org/10.1016/j.compedu.2025.105375Leng, G., Zhang, G., Xiong, Y. J., & Chen, J. (2024). CODP- 1200: An AIGC based benchmark for assisting in child language acquisition. Displays, 82, Article 102627. https://doi.org/10.1016/j.displa.2023.102627Li, S., Lajoie, S. P., Zheng, J., Wu, H., & Cheng, F. (2021). Automated detection of cognitive engagement to inform the art of staying engaged in problem- solving. Computers & Education, 163, 104114. https://doi.org/10.1016/j.compedu.2020.104114Li, T., Fan, Y., Tan, Y., Wang, Y., Singh, S., Li, X., ... Gasevic, D. (2023). Analytics of self- regulated learning scaffolding: Effects on learning processes. Frontiers in Psychology, 14, Article 1206696. https://doi.org/10.3389/fpsyg.2023.1206696Li, X., Li, Y., Mao, Z., & Gong, R. (2025). The effects of group awareness support combined with collaboration scripts on adaptive social regulation in promoting students' challenge awareness and social regulation strategies. Computers & Education, Article 105324. https://doi.org/10.1016/j.compedu.2025.105324Li, Y., Chen, K., Su, Y., & Yue, X. (2021). Do social regulation strategies predict learning engagement and learning outcomes? A study of English language learners in wiki- supported literature circles activities. Educational Technology Research & Development, 69, 917- 943. https://doi.org/10.1007/s11423- 020- 09934- 7Li, F., Lu, Y., Ma, Q., Gao, J., Wang, Z., & Bai, L. (2023). SPOC online video learning clustering analysis: Identifying learners' group behavior characteristics. Computer Applications in Engineering Education, 31(4), 1059- 1077. https://doi.org/10.1002/cae.22624

Li, Y., Li, X., Zhang, Y., & Li, X. (2021). The effects of a group awareness tool on knowledge construction in computer- supported collaborative learning. British Journal of Educational Technology, 52(3), 1178- 1196. https://doi.org/10.1111/bjet.13066Li, Y., Zhang, M., Su, Y., Bao, H., & Xing, S. (2022). Examining teachers' behavior patterns in and perceptions of using teacher dashboards for facilitating guidance in CSCL. Educational Technology Research & Development, 70(3), 1035- 1058. https://doi.org/10.1007/s11423- 022- 10102- 2Lim, L. A., Gasevic, D., Matcha, W., Ahmad Uzir, N. A., & Dawson, S. (2021). Impact of learning analytics feedback on self- regulated learning: Triangulating behavioural logs with students' recall. In LAK21: 11th international learning analytics and knowledge conference. https://doi.org/10.1145/3448139.3448174Lin, P. C., Hou, H. T., Wu, S. Y., & Chang, K. E. (2014). Exploring college students' cognitive processing patterns during a collaborative problem- solving teaching activity integrating Facebook discussion and simulation tools. The Internet and Higher Education, 22, 51- 56. https://doi.org/10.1016/j.iheduc.2014.05.001Lin, J. W., Su, Y. C., & Lai, C. N. (2016). Effects of group awareness and self- regulation level on online learning behaviors. International Review of Research in Open and Distance Learning, 17(4), 224- 241. https://doi.org/10.19173/irrodl.v17i4.2370Lin, J. W., Tsai, C. W., Hsu, C. C., & Chang, L. C. (2021). Peer assessment with group awareness tools and effects on project- based learning. Interactive Learning Environments, 29(4), 583- 599. https://doi.org/10.1080/10494820.2019.1591398Liu, M., Calvo, R. A., Pardo, A., & Martin, A. (2015). Measuring and visualizing students' behavioral engagement in writing activities. IEEE Transactions on learning technologies, 8(2), 215- 224. https://doi.org/10.1109/TLT.2014.2378786Liu, M., Liu, L., & Liu, L. (2018). Group awareness increases student engagement in online collaborative writing. The Internet and Higher Education, 38, 1- 8. https://doi.org/10.1016/j.iheduc.2018.04.001Liu, S., Liu, S., Liu, Z., Peng, X., & Yang, Z. (2022). Automated detection of emotional and cognitive engagement in MOOC discussions to predict learning achievement. Computers & Education, 181, Article 104461. https://doi.org/10.1016/j.compedu.2022.104461Liu, S., Zaiqham, G. H. K., Rashid, R. M., & Bilal, A. (2022). Social media- based collaborative learning effects on student performance/learner performance with moderating role of academic self- efficacy. Frontiers in Psychology, 13, Article 903919. https://doi.org/10.3389/fpsyg.2022.903919Lo, C. K., Hew, K. F., & Jong, M. S. Y. (2024). The influence of ChatGPT on student engagement: A systematic review and future research agenda. Computers & Education, Article 105100. https://doi.org/10.1016/j.compedu.2024.105100Lock, J., & Redmond, P. (2021). Embedded experts in online collaborative learning: A case study. The Internet and Higher Education, 48, Article 100773. https://doi.org/10.1016/j.iheduc.2020.100773Long, P., Siemens, G., Conole, G., & Gasevic, D. (2011). LAK11: Proceedings of the 1st international conference on learning analytics and knowledge. New York, NY, USA: Association for Computing Machinery. URL: https://dl.acm.org/doi/proceedings/10.1145/2090116. Loughry, M. L., Ohland, M. W., & DeWayne Moore, D. (2007). Development of a theory- based assessment of team member effectiveness. Educational and Psychological Measurement, 67(3), 505- 524. https://doi.org/10.1177/00131644062928- 56Lu, G., & Ba, S. (2025). Exploring the impact of eVAI- assisted feedback on pre- service teachers' situational engagement and performance in inquiry- based online discussion. Educational Psychology, 1- 26. https://doi.org/10.1080/01443410.2025.2489784Lucas, P., & Euan, L. (2024). Learning analytics dashboards are increasingly becoming about learning and not just analytics - a systematic review. Education and Information Technologies, 29(11), 14279- 14308. https://doi.org/10.1007/s10639- 023- 12401- 4Luo, H., Chen, Y., Chen, T., Koszalka, T. A., & Feng, Q. (2023). Impact of role assignment and group size on asynchronous online discussion: An experimental study. Computers & Education, 192, Article 10458. https://doi.org/10.1016/j.compedu.2022.104658Ma, X., Liu, J., Liang, J., & Fan, C. (2023). An empirical study on the effect of group awareness in CSCL environments. Interactive Learning Environments, 31(1), 38- 53. https://doi.org/10.1080/10494820.2020.1753730Maier, U., & Klotz, C. (2022). Personalized feedback in digital learning environments: Classification framework and literature review. Computers and Education: Artificial Intelligence, 3, Article 100080. https://doi.org/10.1016/j.caeal.2022.100080Marengo, A., Karagolan- Yilmaz, F. G., Yilmaz, R., & Ceylan, M. (2025). Development and validation of generative artificial intelligence attitude scale for students. Frontiers in Computer Science, 7, 1528455. https://doi.org/10.3389/fcomp.2025.1528455Matcha, W., Gasevic, D., Uzir, N. A., Jovanovic, J., & Pardo, A. (2019). Concepts of learning strategies: Associations with academic performance and feedback. In Proceedings of the 9th international conference on learning analytics & knowledge. https://doi.org/10.1145/3303772.3303787Matcha, W., Uzir, N. A. A., Gasevic, D., & Pardo, A. (2020). A systematic review of empirical studies on learning analytics dashboards: A self- regulated learning perspective. IEEE transactions on learning technologies, 13(2), 226- 245. https://doi.org/10.1109/TLT.2019.2166802Meek, S. E., Blakemore, L., & Marks, L. (2017). Is peer review an appropriate form of assessment in a MOOC? Student participation and performance in formative peer review. Assessment & Evaluation in Higher Education, 42(6), 1000- 1013. https://doi.org/10.1080/02602938.2016.1221052Mejeh, M., Sarbach, L., & Hascher, T. (2024). Effects of adaptive feedback through a digital tool - a mixed- methods study on the course of self- regulated learning. Education and Information Technologies, 29(14), 1- 43. https://doi.org/10.1007/s10639- 024- 1510- 8Melanie, E., & Bodemer, D. (2019). Improving collaborative learning: Guiding knowledge exchange through the provision of information about learning partners and learning contents. Computers & Education, 128, 452- 472. https://doi.org/10.1016/j.compedu.2018.10.009Milesi, M. E., Alfredo, R., Echeverria, V., Yan, L., Zhao, L., Tsai, Y. S., & Martinez- Maldonado, R. (2024). It's really enjoyable to See Me solve the problem like a Hero: GenAI- enhanced data comics as a learning analytics tool. In Extended abstracts of the CHI conference on human factors in computing systems. https://doi.org/10.1145/3613905.3651111Milesi, M. E., Mejia- Domenzain, P., Brandl, L., Echeverria, V., Jin, Y., Gasevic, D., Martin- Maldonado, R. (2025). Piecing data connections together like a Puzzle: Effects of increasing task complexity on the effectiveness of data storytelling enhanced Visualisations. In Proceedings of the 2025 CHI conference on human factors in computing systems (pp. 1- 18). https://doi.org/10.1145/3706598.3714270Miller, M., & Hadwin, A. (2015). Scripting and awareness tools for regulating collaborative learning: Changing the landscape of support in CSCL. Computers in Human Behavior, 52, 573- 588. https://doi.org/10.1016/j.vebo.2015.01.050Molenaar, I. (2022). Towards hybrid human- AI learning technologies. European Journal of Education, 57(4), 632- 645. https://doi.org/10.1111/ejed.12527Molinillo, S., Aguilar- Illescas, R., Anaya- Sanchez, R., & Vallespin- Aran, M. (2018). Exploring the impacts of interactions, social presence and emotional engagement on active collaborative learning in a social web- based environment. Computers & Education, 123, 41- 52. https://doi.org/10.1016/j.compedu.2018.04.012Morales Bueno Rodrigues, A., Diniz Junqueira Barbosa, G., Cortes Vieira Lopes, H., Diniz, J., & Barbosa, S. (2021). What questions reveal about novices' attempts to make sense of data visualizations: Patterns and misconceptions. Computers & Graphics, 94, 32- 42. https://doi.org/10.1016/j.cag.2020.09.015Narciss, S. (2013). Designing and evaluating tutoring feedback strategies for digital learning. Digital Education Review, (25), 7- 26. https://doi.org/10.1344/narciss, S., Prescher, C., Khalifah, L., & Kornidle, H. (2022). Providing external feedback and prompting the generation of internal feedback fosters achievement, strategies and motivation in concept learning. Learning and Instruction, 82, Article 101658. https://doi.org/10.1016/j.learninstruc.2022.101658Ng, P. M., Chan, J. K., & Lit, K. K. (2022). Student learning performance in online collaborative learning. Education and Information Technologies, 27(6), 8129- 8145. https://doi.org/10.1007/s10639- 022- 10923- 3Ng, D. T. K., Tan, C. W., & Leung, J. K. L. (2024). Empowering student self- regulated learning and science education through ChatGPT: A pioneering pilot study. British Journal of Educational Technology, 55(4), 1323- 1353. https://doi.org/10.1111/bjet.13454Niloy, A. C., Bari, M. A., Sultana, J., Chowdhury, R., Raisa, F. M., Islam, A., Hossen, M. A. (2024). Why do students use ChatGPT? Answering through a triangulation approach. Computers and Education: Artificial Intelligence, 6, Article 100208. https://doi.org/10.1016/j.caeal.2024.100208Noroozi, O., Alikhani, I., Järvela, S., Kirschner, P. A., Jusso, I., & Seppanen, T. (2019). Multimodal data to design visual learning analytics for understanding regulation of learning. Computers in Human Behavior, 100, 298- 304. https://doi.org/10.1016/j.chb.2018.12.019Northey, G., Govind, R., Bucic, T., Chylinski, M., Dolan, R., & van Esch, P. (2018). The effect of "here and now" learning on student engagement and academic achievement. British Journal of Educational Technology, 49(2), 321- 333. https://doi.org/10.1111/bjet.12589Ollesch, L., Venohr, O., & Bodemer, D. (2022). Implicit guidance in educational online collaboration: Supporting highly qualitative and friendly knowledge exchange processes. Computers and Education Open, 3, Article 100064. https://doi.org/10.1016/j.caeo.2021.100064

Orteg- Arranz, A, Amarasinghe, I, Martinez- Mones, A, Asensio- Perez, J. I., Dimitriadis, Y., Corrales- Astorgano, M., & Hernandez- Leo, D. (2024). Collaborative activities in hybrid learning environments: Exploring teacher orchestration load and students' perceptions. Computers & Education, 219, Article 105105. https://doi.org/10.1016/j.compedu.2024.105105Ouyang, F., Chen, Z., Cheng, M., Tang, Z., & Su, C. Y. (2021). Exploring the effect of three scaffolding on the collaborative problem- solving processes in China's higher education. International Journal of Educational Technology in Higher Education, 18, 1- 22. https://doi.org/10.1186/s41239- 021- 00273- yOuyang, F., & Zhang, L. (2024). AI- driven learning analytics applications and tools in computer- supported collaborative learning: A systematic review. Educational Research Review, 44, Article 100616. https://doi.org/10.1016/j.edurev.2024.100616Paneth, L., Jeitziner, L. T., Rack, O., Opwis, K., & Zahn, C. (2024). Zooming in: The role of nonverbal behavior in sensing the quality of collaborative group engagement. International Journal of Computer- Supported Collaborative Learning, 1- 43. https://doi.org/10.1007/s11412- 024- 09422- 7Park, E., Ifenthaler, D., & Clariana, R. B. (2023). Adapted or adapted to: Sequence and reflexive thematic analysis to understand learners' self- regulated learning in an adaptive learning analytics dashboard. British Journal of Educational Technology, 54(1), 98- 125. Parker, J. L., Becker, K., & Carroca, C. (2023). ChatGPT for automated writing evaluation in scholarly writing instruction. Journal of Nursing Education, 62(12), 721- 727. https://doi.org/10.3928/01484834- 20231006- 02Patchan Melissa, M., & Puranik Cynthia, S. (2016). Using tablet computers to teach preschool children to write letters: Exploring the impact of extrinsic and intrinsic feedback. Computers & Education, 102, 128- 37. https://doi.org/10.1016/j.compedu.2016.07.007Peng, Y., Li, Y., Su, Y., Chen, K., & Jiang, S. (2023). Effects of group awareness tools on students' engagement, performance, and perceptions in online collaborative writing. Interscroup information matters. The Internet and Higher Education, 53, Article 100945. https://doi.org/10.1016/j.chedu.2023.100845Perez- Lopez, R., Gurrea- Sarasa, R., Herrando, C., Martin- De Hoyos, M. J., Bordonaba- Juste, V., & Utrillas- Acerete, A. (2020). The generation of student engagement as a cognition- affect- behaviour process in a Twitter learning experience. Australasian Journal of Educational Technology, 36(3), 132- 146. https://doi.org/10.14742/AJET.5751Phung, T., Padurean, V. A., Singh, A., Brooks, C., Cambronero, J., Gulwani, S., ... Soares, G. (2024). Automating human tutor- style programming feedback: Leveraging gpt- 4 tutor model for hint generation and gpt- 3.5 student model for hint validation. In Proceedings of the 14th learning analytics and knowledge conference. https://doi.org/10.1145/3636555.3636846Pifarré, M., Cobot, R., & Argelagos, E. (2014). Incidence of group awareness information on students' collaborative learning processes. Journal of Computer Assisted Learning, 30, 300- 317. https://doi.org/10.1111/jcal.12043Pintrich, P. R., Smith, D. A. F., Garcia, T. A., & McKeachie, W. J. (1991). A manual for the use of the motivated strategies for learning questionnaire (MSLQ). Ann Arbor, Michigan: University of Michigan, National Center for Research to Improve Postsecondary Teaching and Learning. Retrieved from: https://eric.ed.gov/?id=ED338122. Polakova, P., & Ivenz, P. (2024). The impact of ChatGPT feedback on the development of EFL students' writing skills. Cogent Education, 11(1), Article 2410101. https://doi.org/10.1080/2331186X.2024.2410101Pozdniakov, S., Martinez- Maldonado, R., Tsai, Y. S., Echeverria, V., Srivastava, N., & Gasevic, D. (2023). How do teachers use dashboards enhanced with data storytelling elements according to their data visualisation literacy skills?. In LAK23: 13th international learning analytics and knowledge conference (pp. 89- 99). https://doi.org/10.1145/3576050.3576063Qureshi, M. A., Khaskeli, A., Qureshi, J. A., Raza, S. A., & Yousufi, S. Q. (2023). Factors affecting students' learning performance through collaborative learning and engagement. Interactive Learning Environments, 31(4), 2371- 2391. https://doi.org/10.1080/10494820.2021.1884584Ramaswami, G., Susnjak, T., Mathrani, A., & Umer, R. (2023). Use of predictive analytics within learning analytics dashboards: A review of case studies. Technology, Knowledge and Learning, 28(3), 959- 980. https://doi.org/10.1007/s10758- 022- 09613- xRegnier, J., Shafer, E., Sobiesk, E., Stave, N., & Haynes, M. (2024). From crisis to opportunity: Practices and technologies for a more effective post- COVID classroom. Education and Information Technologies, 29(5), 5981- 6003. https://doi.org/10.1007/s10639- 023- 11929- 9Roberts, L. D., Howell, J. A., & Seaman, K. (2017). Give me a customizable dashboard: Personalized learning analytics dashboards in higher education. Technology, Knowledge and Learning, 22, 317- 333. https://doi.org/10.1007/s10758- 017- 9316- 1Rojas, M., Nussbaum, M., Guerrero, O., Chiuminato, P., Greiff, S., Del Rio, R., & Alvares, D. (2022). Integrating a collaboration script and group awareness to support group regulation and emotions towards collaborative problem- solving. International Journal of Computer- Supported Collaborative Learning, 17(1), 135- 168. https://doi.org/10.1007/s11412- 022- 09362- 0Rousseuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20, 53- 65. https://doi.org/10.1016/0377- 0427(87)90125- 7Sahin, M., & Ifenthaler, D. (2021). Visualizations and dashboards for learning analytics: A systematic literature review. Visualizations and dashboards for learning analytics, 3- 22. https://doi.org/10.1007/978- 3- 030- 81222- 5.1Sansom, R. L., Bodily, R., Bates, C. O., & Leary, H. (2020). Increasing student use of a learner dashboard. Journal of Science Education and Technology, 29(3), 386- 398. https://doi.org/10.1007/s10956- 020- 09824- wSchnaubert, L., & Bodemer, D. (2022). Group awareness and regulation in computer- supported collaborative learning. International Journal of Computer- Supported Collaborative Learning, 17(1), 11- 38. https://doi.org/10.1007/s11412- 022- 09361- 1Schwarz, B., B. Swidan, O., Prusak, N., & Palatnik, A. (2011). Collaborative learning in mathematics classrooms: Can teachers understand progress of concurrent collaborating groups? Computers & Education, 165, Article 104151. https://doi.org/10.1016/j.compedu.2021.104151Sedrakyan, G., Malmberg, J., Verbert, K., Järvälä, S., & Kirschner, P. A. (2020). Linking learning behavior analytics and learning science concepts: Designing a learning analytics dashboard for feedback to support learning regulation. Computers in Human Behavior, 107, Article 105512. https://doi.org/10.1016/j.chb.2018.05.004Shao, H., Martinez- Maldonado, R., Echeverria, V., Yan, L., & Gasevic, D. (2024). Data storytelling in data visualisation: Does it enhance the efficiency and effectiveness of information retrieval and insights comprehension?. In Proceedings of the 2024 CHI conference on human factors in computing systems. https://doi.org/10.1145/3613904.3643022Siiman, L. A., Rannastu- Avalos, M., Poysa- Tarhonen, J., Häkkinen, P., & Pedaste, M. (2023). Opportunities and challenges for AI- assisted qualitative data analysis: An example from collaborative problem- solving discourse data. In International conference on innovative technologies and learning (pp. 87- 96). Cham: Springer Nature Switzerland. https://doi.org/10.1007/978- 3- 031- 40113- 8_9. Sinha, S., Rogat, T. R., Adams- Wiggins, K. R., & Henelo- Silver, C. E. (2015). Collaborative group engagement in a computer- supported inquiry learning environment. International Journal of Computer- Supported Collaborative Learning, 10, 273- 307. https://doi.org/10.1007/s11412- 015- 9218- ySu, Y., Lin, Y., & Lai, C. (2023). Collaborating with ChatGPT in argumentative writing classrooms. Assessing Writing, 57, Article 100752. https://doi.org/10.1016/j.asw.2023.100752Su, Y., Ren, J., & Song, X. (2024). The effects of group awareness tools on student engagement with peer feedback in online collaborative writing environments. Interactive Learning Environments, 32(5), 1907- 1923. https://doi.org/10.1080/10494820.2022.2131833Suraworachet, W., Seon, J., & Cukurova, M. (2024). Predicting challenge moments from students' discourse: A comparison of GPT- 4 to two traditional natural language processing approaches. In Proceedings of the 14th learning analytics and knowledge conference. https://doi.org/10.1145/3636555.3636905Susnjak, T. (2024). Beyond predictive learning analytics modelling and onto explainable artificial intelligence with a prescriptive analytics and ChatGPT. International Journal of Artificial Intelligence in Education, 34(2), 452- 482. https://doi.org/10.1007/s40593- 023- 00336- 3Sweller, J. (1988). Cognitive load during problem solving: Effects on learning. Cognitive Science, 12(2), 257- 285. https://doi.org/10.1016/0364- 0213(88)90023- 7Tan, Y., Ruis, A. R., Marquart, C., Cai, Z., Knowles, M. A., & Shaffer, D. W. (2023). Ordered network analysis. In International conference on quantitative Ethnography (pp. 101- 116). Cham: Springer Nature Switzerland. https://doi.org/10.1007/978- 3- 031- 31726- 2_8Tang, T., Sha, J., Zhao, Y., Wang, S., Wang, Z., & Shen, S. (2024). Unveiling the efficacy of ChatGPT in evaluating critical thinking skills through peer feedback analysis: Leveraging existing classification criteria. Thinking Skills and Creativity, 53, Article 101607. https://doi.org/10.1016/j.tsc.2024.101607

Teng, M. F. (2024). ChatGPT is the companion, not enemies": EFL learners' perceptions and experiences in using ChatGPT for feedback in writing. Computers and Education: Artificial Intelligence, 7, Article 100- 270. https://doi.org/10.1016/j.ceai.2024.100270Törnänen, T., Järvenoja, H., & Mänty, K. (2021). Exploring groups' affective states during collaborative learning: What triggers activating affect on a group level? Educational Technology Research & Development, 69(5), 2523- 2545. https://doi.org/10.1007/s11423- 021- 10087- 0Tretow- Fish, T. A. B., & Khalid, M. S. (2023). Methods for evaluating learning analytics and learning analytics dashboards in adaptive learning platforms: A systematic review. Electronic Journal of E- Learning, 21(5), 430- 449. https://doi.org/10.34190/ejel.21.5.3088Tsai, C. Y., Lin, Y. T., & Brown, I. K. (2024). Impacts of ChatGPT- assisted writing for EFL English majors: Feasibility and challenges. Education and Information Technologies, 29(17), 22427- 22445. https://doi.org/10.1007/s10639- 024- 12722- yVan den Bossche, P., Gijelaers, W. H., Segers, M., & Kirschner, P. A. (2006). Social and cognitive factors driving teamwork in collaborative learning environments team learning beliefs and behaviors. Small Group Research, 37, 490- 521. https://doi.org/10.1177/1046496406292938Van Horne, S., Curran, M., Smith, A., VanBuren, J., Zahrish, D., Larsen, R., & Miller, R. (2018). Facilitating student success in introductory chemistry with feedback in an online platform. Technology, Knowledge and Learning, 23, 21- 40. https://doi.org/10.1007/s10758- 017- 9341- 0Vieira, C., Parsons, P., & Byrd, V. (2018). Visual learning analytics of educational data: A systematic literature review and research agenda. Computers & Education, 122, 119- 135. https://doi.org/10.1016/j.compedu.2018.03.018Vygotsky, L. S. (1978). In M. Cole, V. John- Steiner, S. Scribner, & E. Souberman (Eds.), Mind in society: The development of higher psychological processes. Cambridge, MA: Harvard University press. https://doi.org/10.2307/j.ctvff9vz4. Weng, S. J., & Lin, S. S. (2007). The effects of error comparison of self- efficacy and collective efficacy on computer- supported collaborative learning. Computers in Human Behavior, 23(5), 2256- 2268. https://doi.org/10.1016/j.chb.2006.03.005Wiley, K., Dimitriadis, Y., & Linn, M. (2024). A human- centred learning analytics approach for developing contextually scalable K- 12 teacher dashboards. British Journal of Educational Technology, 55(3), 845- 885. https://doi.org/10.1111/bjet.13383Wilson, S. E., & Nishimoto, M. (2024). Assessing learning of computer programming skills in the age of generative artificial intelligence. Journal of Biomechanical Engineering, 146(5), Article 051003. https://doi.org/10.1115/1.4064364Wu, S. Y. (2022). Construction and evaluation of an online environment to reduce off- topic messaging. Interactive Learning Environments, 30(3), 455- 469. https://doi.org/10.1080/10494820.2019.1664594Wu, R., & Yu, Z. (2024). Do AI chatbots improve students learning outcomes? Evidence from a meta- analysis. British Journal of Educational Technology, 55(1), 10- 33. https://doi.org/10.1111/bjet.13334Yan, L., Martinez- Maldonado, R., & Gasevici, D. (2024). Generative artificial intelligence in learning analytics: Contextualising opportunities and challenges through the learning analytics cycle. In Proceedings of the 14th learning analytics and knowledge conference. https://doi.org/10.1145/363655.3636856Yan, L., Martinez- Maldonado, R., Jin, Y., Echeverria, V., Milesi, M., Fan, J., & Gasevici, D. (2025). The effects of generative AI agents and scaffolding on enhancing students' comprehension of visual learning analytics. Computers & Education, Article 105322. https://doi.org/10.1016/j.compedu.2025.105322Yan, L., Zhao, L., Echeverria, V., Jin, Y., Alfredo, R., Li, X., Martinnez- Maldonado, R. (2024). VizChat: Enhancing learning analytics dashboards with contextualised explanations using multimodal generative AI chatbots. In International conference on artificial intelligence in education (pp. 180- 193). Cham: Springer Nature Switzerland. https://doi.org/10.1007/978- 3- 331- 64299- 9.133. Ye, J. Hao, J. Hou, Y. Wang, Z., Xiao, S., Luo, Y., & Zeng, W. (2024). Generative ai for visualization: State of the art and future directions. Visual Informatics. https://doi.org/10.1016/j.eswa.2024.126147Ye, J. M., & Zhou, J. (2022). Exploring the relationship between learning sentiments and cognitive processing in online collaborative learning: A network analytic approach. The Internet and Higher Education, 25, Article 100875. https://doi.org/10.1016/j.iheduc.2022.100875Yilmaz, F. G. K., & Yilmaz, R. (2025). Exploring the role of self- regulated learning skills, cognitive flexibility, and metacognitive awareness on generative artificial intelligence attitude. Innovations in Education & Teaching International. https://doi.org/10.1080/14703297.2025.2484613Yilmaz, F. G. K., Yilmaz, R., & Ceylan, M. (2024). Generative artificial intelligence acceptance scale: A validity and reliability study. International Journal of Human- Computer Interaction, 40(4), 8703- 8715. https://doi.org/10.1080/10447318.2023.2288730Zamecnik, A., Kovanovic, V., Grossmann, G., Joktimovic, S., Jolliffe, G., Gibson, D., & Pardo, A. (2022). Team interactions with learning analytics dashboards. Computers & Education, 185, Article 104514. https://doi.org/10.1016/j.compedu.2022.104514Zhan, Y., Boud, D., Dawson, P., & Yan, Z. (2025). Generative artificial intelligence as an enabler of student feedback engagement: A framework. Higher Education Research and Development, 44(5), 1289- 1304. https://doi.org/10.1080/07294360.2025.2476513Zhan, Y., Wan, Z. H., & Sun, D. (2022). Online format: peer feedback in Chinese contexts at the tertiary level: A critical review on its design, impacts and influencing factors. Computers & Education, 176, Article 104341. https://doi.org/10.1016/j.compedu.2021.104341Zhao, L., Tan, Y., Gasevici, D., Shaffer, D. W., Yan, L., Alfredo, R., Martinnez- Maldonado, R. (2023). Analysing verbal communication in embodied team learning using multimodal data and ordered network analysis. In International conference on artificial intelligence in education (pp. 242- 254). Cham: Springer Nature Switzerland. https://doi.org/10.1007/978- 3- 331- 36272- 9.20Zhao, J. H., Yang, Q. F., Lian, L. W., & Wu, X. Y. (2024). Impact of pre- knowledge and engagement in robot- supported collaborative learning through using the ICAPB model. Computers & Education, 217, Article 105069. https://doi.org/10.1016/j.compedu.2024.105069Zheng, L., Gao, L., Huang, Z., Shi, Z., & Zhou, Y. (2025). A systematic meta- analysis of the impacts of group awareness tools on learning achievements, learning behaviors, and learning perceptions from 2010- 2023. Interactive Learning Environments, 1- 18. https://doi.org/10.1080/10494820.2025.245440Zheng, L., Long, M., Niu, J., & Zhong, L. (2023). An automated group learning engagement analysis and feedback approach to promoting collaborative knowledge building, group performance, and socially shared regulation in CSCL. International Journal of Computer- Supported Collaborative Learning, 18(1), 101- 133. https://doi.org/10.1007/s11412- 023- 09386- 0Zheng, Y., Yu, S., & Tong, Z. (2022). Understanding the dynamic of student engagement in project- based collaborative writing: Insights from a longitudinal case study. Language Teaching Research, Article 1362160213- 135808. https://doi.org/10.1177/13621688221115808Zheng, L., Zhong, L., & Fan, Y. (2023). An immediate analysis of the interaction topic approach to promoting group performance, knowledge convergence, cognitive engagement, and coregulation in online collaborative learning. Education and Information Technologies, 28(8), 9913- 9934. https://doi.org/10.1007/s10639- 023- 11588- wZhou, J., & Ye, J. min (2024). Investigating cognitive engagement patterns in online collaborative learning: A temporal learning analytic study. Interactive Learning Environments, 32(10), 6997- 7013. https://doi.org/10.1080/10494820.2023.2299976