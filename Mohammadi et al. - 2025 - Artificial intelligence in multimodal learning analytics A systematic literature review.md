# Artificial intelligence in multimodal learning analytics: A systematic literature review

Mehrnoush Mohammadi a, Elham Tajik b, Roberto Martinez- Maldonado c, Shazia Sadiq c, Wojtek Tomaszewski d,e, Hassan Khosravi f,  $\oplus$  \*

a School of Electrical Engineering and Computer Science, The University of Queensland, Brisbane, Australia  b Department of Educational Psychology and Learning Systems, Florida State University, Tallahassee, USA  c Centre for Learning Analytics at Monash, Faculty of Information Technology, Monash University, Melbourne, Australia  d Australian Research Council Centre of Excellence for Children and Families Over the Life Course, The University of Queensland, Brisbane, Australia  e Institute for Social Science Research, The University of Queensland, Brisbane, Australia  f Institute for Teaching and Learning Innovation, The University of Queensland, Brisbane, Australia

# ARTICLE INFO

Keywords:  AI  Multimodal data  Learning analytics  Multimodal learning analytics  Systematic review

# ABSTRACT

The proliferation of educational technologies has generated unprecedented volumes of diverse, multimodal learner data, offering rich insights into learning processes and outcomes. However, leveraging this complex, multimodal data requires advanced analytical methods. While Multimodal Learning Analytics (MMLA) offers promise for exploring this data, the potential of Artificial Intelligence (AI) to enhance MMLA remains largely unexplored. This paper bridges these two evolving domains by conducting the first systematic literature review at the intersection of AI and MMLA, analyzing 43 peer- reviewed papers from 11 reputable databases published between 2019 and 2024. The findings indicate a growing trend in AI- enhanced MMLA studies published predominantly in high- quality venues, led by education researchers with a predominant focus on tertiary education targeting diverse stakeholders. Guided by a novel conceptual framework, our analysis highlights the transformative role of AI across the MMLA process, particularly in model learning and feature engineering. However, it also uncovers significant gaps, including limited AI implementation in components requiring deep integration with learning theories, insufficient application of advanced AI techniques, and lack of large- scale studies in authentic learning environments. The review identifies key benefits, such as enhanced personalization and real- time feedback, while also addressing challenges related to ethical considerations, data integration, and scalability. Our study contributes by offering comprehensive recommendations for future research, emphasizing international collaboration, multi- level studies, and ethical AI implementation. These findings advance the theoretical understanding of AI's role in education, providing a foundation for developing sophisticated, interpretable, and scalable AI- enhanced MMLA approaches, potentially revolutionizing personalized learning across diverse educational settings.

# 1. Introduction

The increasing integration of educational tools, sensing technologies, interaction devices, and advanced artificial intelligence (AI) algorithms into learning environments has enabled a more extensive collection of behavioral data across the various modalities through which learners demonstrate their learning processesâ€”commonly referred to as multimodal data (Sharma and Giannakos (2020)). Analyzing learner activity across multiple modalities acknowledges that students often display and communicate knowledge, interests, and intent in diverse ways, extending beyond mere interactions with personal computers (Worsley et al. (2021)). The emerging research field of multimodal learning analytics (MMLA) leverages this rich tapestry of multimodal educational data, encompassing everything from students' digital footprints captured in log files and insights into attention spans derived from eye- tracking (e.g., Schneider et al. (2021)), to emotional analysis via camera recordings

(e.g., Emerson et al., 2020a; Chettaoui et al., 2023), automated speech content analysis of learners working in groups (e.g., D'Angelo and Rajarathinam (2024)), socio- spatial and motion behavior analysis of both students and teachers in physical classrooms (e.g., Yan et al., 2022, 2022), engagement measurement across various in- class activities (e.g., Sumer et al., 2023; Aslan et al., 2019), and even analysis of physiological indicators of learners that may be hard to detect with the naked eye but can reveal critical aspects such as stress (e.g., Alfredo et al., 2023). MMLA thus has the potential to expand our understanding of learners, contributing to a deeper comprehension of human learning, transcending the traditional one- size- fits- all educational paradigm, and fostering a more nuanced appreciation of each student's individual learning path (Ochoa et al., 2022).

MMLA has seen a significant surge in interest, particularly highlighted by the exponential growth in research output since the early discussions around its potential in 2012. Several systematic literature reviews in the area of MMLA have consistently reported on this growth as evidenced by an increasing number of publications across various academic venues, indicating a broad and interdisciplinary effort to harness multimodal data for advancing personalized and effective learning experiences (e.g. Mu et al., 2020; Ouhaichi et al., 2023). However, despite the significant growth in MMLA research and advancements in sensing technologies and computational methods, the field has yet to reach its full potential due to prevailing ethical, methodological, and technical challenges (Yan et al., 2022).

Giannakos and Cukurova (2023) suggested that integrating AI advancements with multimodal interaction data can unlock the full potential of MMLA systems, driving significant advancements in their research and development. This integration can broaden the range of multimodal data sources that can be analyzed using rapidly evolving multimodal analysis methodologies and tools. According to Davenport (2018) while simple analytics can enhance understanding and support decision- making through rule- based methods and descriptive visualizations, such as dashboards, AI and machine learning- powered predictive and prescriptive analytics enable more sophisticated interactions. These advancements can both facilitate the use of intelligent agents to create automated or semi- automated actions, elevating the potential of analytics beyond descriptive insights, and also raise critical implications for our current pedagogical practices (Buckingham Shum & Luckin, 2019).

Indeed, the application of AI in education, beyond MMLA, has seen substantial growth, introducing promising approaches to developing explainable models of student learning (Khosravi et al., 2022), identifying students at risk of course failure (Foster & Siddle, 2020), automating and scaffolding learning activities (Lim et al., 2023), and providing personalized nudges (Payne et al., 2023) as well as real- time, rich feedback (Jin et al., 2024). These AI- driven innovations also help educators devise more targeted and effective intervention strategies, thereby enhancing the overall quality of education. Furthermore, recent advancements in Generative AI (GenAI) are opening new opportunities to explore the creation of agents capable of continuously monitoring and analyzing learners' activities or intervening when necessary. A key question remains: to what extent have AI- powered developments influenced MMLA? Although some previous systematic literature reviews have indirectly touched on the methodologies frequently used in MMLA (e.g. Noroozi et al., 2020; Mu et al., 2020), none have specifically focused on exploring the current state at the intersection of AI and MMLA.

This paper serves as a conduit between these two evolving research fields by undertaking a systematic literature review at the nexus of AI and MMLA where MMLA has the potential to benefit from the latest AI technological advancements, and conversely, AI applications in education can be enriched by the theoretical underpinnings of MMLA. This paper specifically aims to explore the degree and ways in which AI has enhanced the MMLA process, particularly focusing on the critical pipeline of tasks including data acquisition and processing, data fusion, feature selection, data annotation, sense- making, and the formulation of interventions. This examination seeks to illuminate AI's role in refining these essential components, thereby advancing the efficacy and impact of MMLA in educational contexts. We specifically synthesize peer- reviewed journal articles and full conference papers for the last 5 years from 01/01/2019 to 01/03/2024, which report on primary research utilizing AI within the scope of MMLA sourced from several esteemed databases such as Web of Science (WoS), Scopus, ACM Digital Library, IEEE Xplore, Education Resources Information Center (ERIC), SpringerLink, ProQuest Education Databases, SAGE Journals, Wiley, EBSCO HOST, and Taylor & Francis Online.

# 2. Related works

Significant growth and unprecedented advancement in MMLA have led to the publication of numerous review papers that explore this evolving field. In this section, we review the existing perspectives, underscoring the need for our proposed systematic review, which focuses on the intersection of MMLA and AI.

Out of the various existing MMLA reviews, three systematic review papers have specifically addressed the critical role of multimodal data as a pivotal element for success in this domain. Sharma and Giannakos (2020) presented a comprehensive review of the empirical evidence surrounding the capabilities of multimodal data in understanding and supporting complex learning phenomena, particularly addressing underexplored areas such as supporting accessibility in learning environments. Similarly, Noroozi et al. (2020) explored the optimal identification and integration of various data modalities to gain comprehensive insights into the cognitive, motivational, and emotional aspects of learning. Furthermore, Crescenzi- Lanna (2020) examined the complexity of collecting multimodal data, focusing on identifying information sources and employing tools and strategies suitable for assessing the progress and behavior of children under six years old. These reviews collectively emphasize the essential role of multimodal data in advancing MMLA, highlighting its potential impact on understanding and improving educational outcomes.

The concept of data fusion in MMLA has been thoroughly examined, with a significant focus on how different data types and learning indicators are integrated to enhance educational insights. Mu et al. (2020) conducted a comprehensive literature review on multimodal data fusion in learning analytics, introducing a conceptual model that categorizes data types into five distinct spaces: digital, physical, physiological, psychometric, and environmental. Alongside these data types, the review identifies five key learning indicatorsâ€”behavior, cognition, emotion, collaboration, and engagementâ€”that are crucial for understanding learning processes. The study then explores the relationships between these data types and learning indicators, highlighting various data fusion methods used in MMLA. By systematically investigating these relationships, the review proposes a framework that positions multimodal data fusion as a central component of the MMLA process, facilitating a deeper and more holistic understanding of learning dynamics through the integration of diverse data sources. Furthermore, Wang and Gu (2024) focused on systematically exploring the complex relationship between learning indicators, multimodal data types, and data fusion strategies to improve the decision- making process in classroom environments. This study highlighted engagement as the leading learning indicator and addressed the fusion strategies employed at three levels of integration: data, features, and decisions.

The topic of ethics has also been extensively studied in the context of MMLA, with several literature reviews delving into the ethical implications and challenges that arise in this rapidly evolving field. Prinsloo et al. (2023) explores the delicate balance between utilizing MMLA for educational insights and safeguarding student privacy, particularly within higher education. This review identifies MMLA as existing in an 'inbetween' spaceâ€”straddling the line between laboratory research and classroom application, and between the encroachment on and respect for student data privacy. Alwahaby et al. (2022) further contributes by examining the real- world impacts and ethical considerations of MMLA,

highlighting the often- overlooked ethical challenges associated with implementing MMLA in actual educational settings. Finally, (Yan et al., 2022) addresses the ethical, scalability, and sustainability concerns tied to the use of sensor- based technologies in MMLA, underscoring the need for careful consideration of these factors to ensure the responsible adoption of MMLA innovations in education. These reviews collectively underscore the critical importance of addressing ethical issues in the deployment and development of MMLA.

On the other hand, Ouhaichi et al. (2023) conducted a systematic mapping study of the MMLA literature to review and classify the research methods, trending research themes, and technologies to provide guidelines for developing and evaluating the MMLA tools. Additionally, three other MMLA reviews have focused on concerns regarding the practical adaptation and feasibility of deploying MMLA in real- world and authentic settings. For instance, Shankar et al. (2018) explored the technical challenges and complexities associated with different stakeholders, multiple data sources, and data processing activities by analyzing proposed designs for MMLA software architectures. This study investigated the infrastructures of these software architectures in terms of their capabilities in MMLA, according to the analytics Data Value Chain. Bin Qushem (2020) systematically reviewed the studies on MMLA to investigate how it can enhance learning support without relying solely on technology- mediated tools, by exploring current MMLA methods and approaches and highlighting key challenges (i.e., data- related challenges, environmental challenges, learning and pedagogical challenges, and technical challenges) in adoption and implementation. Furthermore, Ouhaichi et al. (2024) addressed the challenges of consistency and effective development of MMLA systems, emphasizing the lack of standardized models and frameworks and the scattered nature of design recommendations in MMLA studies through a systematic review.

In addition to these systematic reviews, several non- systematic review papers have provided valuable patterns, perspectives, and theoretical insights, guiding and enhancing the direction of MMLA research. For instance, Giannakos and Cukurova (2023) conducted a semi- systematic literature review to investigate the role of learning theory in MMLA studies. Di Mitri et al. (2018) offered new insights into the modalities for learning, learning theories, and their practical application in various learning scenarios. The review paper (Chango et al., 2022) explored the various data types and modalities utilized, their potential for learning, and the fusion strategies employed to integrate them within learning environments. Cukurova et al. (2020) addressed the methodological, practical, and ethical challenges in MMLA research. Oviatt (2018) provided a summary and critical reflection on key opportunities and challenges in developing MMLA, focusing on technology trends, participatory design, and data privacy. Additionally, Worsley and Martinez- Maldonado (2018) conducted a preliminary review of the MMLA research, distinguishing between empirical and non- empirical works, analyzing publication trends, and identifying key opportunities for future development. Furthermore, In (Pei et al., 2023), the applications and academic development of MMLA were examined by analyzing publication and citation trends, identifying prolific contributors, exploring collaborative patterns, and tracing the evolution of research topics over time.

While there has been a wealth of research and numerous reviews conducted on various aspects of MMLA including the critical role of multimodal data, the integration of diverse data sources, ethical considerations, and the practical adaptation of MMLA in real- world settings, the specific intersection of MMLA with AI remains largely underexplored. This gap highlights the need for a systematic review that specifically addresses how AI can further enhance and transform the capabilities of MMLA in educational contexts.

# 3. Conceptualizing the review and research questions

To structure our research on the intersection of AI and MMLA, we adopted the framework proposed by Chatti et al. (2012) which covers four main dimensions: Who, What, How, and Why. Who questions are vital for identifying stakeholders, understanding different perspectives, and determining the relevance and impact on various individuals or groups. What questions are essential for defining subjects, providing detailed descriptions and characteristics, and breaking down topics into their essential components. How questions are crucial for explaining methods, outlining solutions, and detailing the steps and procedures required for the implementation of a process. Finally, why questions are fundamental for exploring reasons and motivations, understanding the significance of the topic, and identifying the objectives and purposes behind actions and decisions.

RQ 1. An important aspect of the study is to explore the current landscape and state of research in AI within MMLA studies. For that we leverage the what dimension to better understand the volume of studies, their publication quality, geographical distributions, collaboration patterns, and academic disciplines, which is crucial for identifying key trends, emerging areas of focus, and potential gaps in the field, which in turn can inform and guide future research efforts and collaborations. Our research here will be guided by the following questions.

# What is the current state of research in AI-enhanced MMLA?

- What is the evolution of publication volume and types of AI-enhanced MMLA studies over time?  
- What is the quality profile of publication venues in AI-enhanced MMLA studies?  
- What is the geographical distribution of AI-enhanced MMLA studies?  
- What are the patterns of authors' collaborations, and how have they shaped the geographical spread of AI-enhanced MMLA studies?  
- What academic disciplines are driving AI-enhanced MMLA studies?

RQ 2. Another important aspect is to explore the current context of AI- enhanced MMLA studies. For that we use the what and how dimensions to investigate the educational levels targeted, the stakeholders involved, and the modalities utilized in these studies. By identifying these aspects, we can gauge the reach and inclusiveness of current research, ensuring that AI- enhanced MMLA solutions cater to diverse educational needs and contexts. This knowledge helps identify underrepresented areas or groups, promoting more balanced and equitable research efforts, and fostering the development of tailored, effective educational interventions. Our research here will be guided by the following questions.

# In what contexts are AI-enhanced MMLA being applied?

- What educational levels are the primary focus of AI-enhanced MMLA papers?  
- Who are the main stakeholders targeted by AI-enhanced MMLA papers?  
- What modalities are used in AI-enhanced MMLA papers?

RQ 3. A critical aim of this study is to explore the role of AI in AI- enhanced MMLA studies. To achieve this, we leverage the what and how dimensions to investigate at which steps of the MMLA cycle AI has been utilized, and to explore how (i.e., using what techniques) AI has been incorporated in these studies. This dual approach will provide a comprehensive understanding of the integration of AI into MMLA studies. Our research here will be guided by the following questions.

- How has AI been implemented in MMLA studies?  
- At which stages of the MMLA process has AI been implemented?  
- What AI techniques are commonly used in MMLA studies?

RQ 4. Another important aim of this study is to examine the experimental designs and settings employed in AI- enhanced MMLA studies. Here, we leverage the what dimension to investigate the types of ex

perimental designs used, the settings in which these experiments are conducted, and the methodologies applied. This approach will provide a detailed understanding of the experimental frameworks that underpin AI- enhanced MMLA research, helping to identify best practices and areas for improvement in future studies. Our research here will be guided by the following questions.

# What experimental designs and settings are employed in AIenhanced MMLA studies?

- What are the typical sample sizes used in AI-enhanced MMLA studies? 
- What types of experimental settings are commonly employed in AI-enhanced MMLA studies? 
- What kind of ethical implications are considered in AI-enhanced MMLA studies?

RQ 5. Finally, the study aims to understand the motivations for using AI in MMLA studies, as manifested by reported benefits, to identify the driving factors behind its adoption and the positive impacts it brings to educational contexts. At the same time, we are interested in investigating the challenges encountered in the implementation of AI within these studies. We leverage the Why and What dimensions to explore benefits and challenges. This comprehensive understanding will contribute to advancing the field of MMLA by optimizing AI technologies and proactively addressing potential issues.

Why is AI adopted in MMLA studies, and what are some of the underlying challenges? What are the reported benefits of AI- enhanced MMLA studies? What are the reported challenges of AI- enhanced MMLA studies?

# 4. Methodology

This paper aims to provide a credible synthesis and structured review of the findings from studies on AI in MMLA. To achieve this objective, we conducted a systematic review and meta- analysis following the principles outlined in the Preferred Reporting Items for Systematic Reviews and Meta- Analyses (PRISMA) guidelines proposed by Page et al. (2021). This protocol involves two primary systematic phases: data collection and data extraction. The data collection procedure attempts to gather relevant studies to address our research questions by designing strong search strings tailored to research questions, extracting studies from reputable databases, screening the titles and abstracts of imported studies based on the predefined inclusion criteria, and subsequently refining the collected studies through full- text screening using a set of stringent constraints known as exclusion criteria. In the second phase, each included paper is systematically analyzed to extract the essential information for synthesis and analysis.

Fig. 1 illustrates the process of data collection procedure in our study. Notably, we utilized Covidence, an online platform designed for systematic review studies, to manage our data collection and extraction processes, as well as facilitate collaboration among our reviewers.

# 4.1. Data collection procedure

The data collection process is fundamental to ensuring the integrity and comprehensiveness of a systematic review. In this subsection, we outlined the methodological steps of this process in three main subsections to systematically identify, extract, and refine studies from reputable databases as follows:

# 4.1.1. Search terms

To ensure comprehensive coverage of relevant literature while remaining aligned with our research questions, rigorous designing of search strings is imperative. Hence, we initially reviewed the primary and systematic review studies addressing AI in Education and MMLA

![](images/2c6bc0f4833fa7bc0d4cd526f302a8fb7cd3c49e42d81d307936c765c25c66df.jpg)  
Fig. 1. PRISMA flowchart depicting the data collection process, including a search across eleven reputable databases, study selection based on inclusion and exclusion criteria, and data management using Covidence.

To ensure comprehensive coverage of relevant literature while remaining aligned with our research questions, rigorous designing of search strings is imperative. Hence, we initially reviewed the primary and systematic review studies addressing AI in Education and MMLA (e.g., Samuelsen et al., 2019; Ouyang & Jiao, 2021; Ouyang et al., 2022; Noroozi et al., 2020), published in prestigious academic journals and conferences, to find a combination of keywords, phrases, and controlled vocabulary terms related to the main concepts of our study. Through this synthesis, we identified AI, Multimodal, and Learning Analytics as the three main keywords in our research. We created a codebook for each keyword using the 'OR' (|) operator to ensure a comprehensive search. By concatenating these codebooks with the 'AND'(&) operator, we formulated our search strings to capture studies on AI in MMLA within the context of education, as presented in Table 1. It's important to note that truncation (*) allows for the inclusion of different word terminations.

# 4.1.2. Database search

In this step, we independently searched eleven reputable and relevant bibliographic databases, including Web of Science (WoS), Scopus, ACM Digital Library, IEEE Xplore, Education Resources Information Center (ERIC), SpringerLink, ProQuest Education Databases, SAGE Journals, Wiley, EBSCO HOST, and Taylor & Francis Online, to identify a comprehensive set of high- quality studies that included our key search strings in their titles or abstracts. During this process, we restricted the identified studies by searching among published conference and journal papers from January 1, 2019, to March 13, 2024 to capture the most relevant and recent advancements in AI and MMLA, consistent with the approach of other systematic reviews (e.g., DÃ­az & Nussbaum, 2024; Crescenzi- Lanna, 2020). This period represents a crucial phase of rapid technological development and methodological progress in both fields, ensuring that our analysis reflects the latest innovations and trends in AI applications within MMLA. As a result, an initial set of 686 papers, comprising 102 papers from WoS, 126 papers from Scopus, 46 papers from ACM Digital Library, 35 papers from IEEE Xplore, 48 papers from ERIC, 262 papers from SpringerLink, 31 papers from ProQuest Education Databases, three papers from SAGE Journals, 18 papers from Wiley, ten papers from EBSCO HOST, and five papers from Taylor & Francis Online, was produced to be refined in the next steps.

Table 1 Designed search strings.  

<table><tr><td>Topic</td><td>Search terms</td></tr><tr><td rowspan="2">AI</td><td>Applications: Intelligent System | Machine Intelligence | Intelligent Agent* | Intelligent Tutoring System | Expert System | Predictive Analytics | Detection | Feedback | Visualization | Pattern Recognition | Computer Vision.</td></tr><tr><td>Techniques: Artificial Intelligence | AI | Machine learning | ML | Supervised learning | Unsupervised learning | Semi-supervised learning | Self-supervised learning | Reinforcement learning | Predict | Analy*| Ensemble Learning | Transfer Learning | Feature Selection | Feature Extraction | Fusion | Integration | Clustering | Classif* | Natural Language Processing | NLP | Generative AI | GenAI | Deep Learning | Neural Networks | Decision Tree | K-means | Random Forest | Support Vector Machines | SVM | Logistic Regression | Naive Bayes | k-nearest Neighbors | k-NN | Fuzzy-Logic | Bayesian network | Latent Dirichlet allocation | genetic algorithm | genetic programming | Principal Component Analysis | PCA.</td></tr><tr><td>&amp;amp; 
Multimodal</td><td>Multi*modal | Multi mode | Multimodality | Multi*source | Multi*view | Multi*Channel | Multiple datasets | Multiple data | Triangular .</td></tr><tr><td>&amp;amp; 
Learning Analytics</td><td>Learning analytics | Educational Data Analytics.</td></tr></table>

Table 2 The inclusion and exclusion criteria used to screen papers.  

<table><tr><td>Inclusion criteria</td><td>Exclusion criteria</td></tr><tr><td>Published between 2019 and 2024</td><td>Non-English papers</td></tr><tr><td>Written in English</td><td>Duplicated papers</td></tr><tr><td>Full text available</td><td>Secondary research papers</td></tr><tr><td>Journal or Conference papers</td><td>Less than two modalities have been used</td></tr><tr><td>Primary research papers</td><td>No contribution to use of AI in MMLA</td></tr><tr><td>Conducted in the education field</td><td>No empirical results are reported</td></tr><tr><td>Must have more than one modality</td><td></td></tr><tr><td>Employs at least one AI application or technique</td><td></td></tr></table>

# 4.1.3. Study screening and selection

In this subsection, we describe the process of study screening and selection steps for our systematic review. Firstly, we imported our identified 686 papers from databases (exported as .RIS files) to the Covidence platform. It automatically identified and removed 220 duplicated papers, resulting in 466 studies for the title and abstract screening (see Fig.1).

Next, an initial screening of the titles and abstracts was performed. In this incremental process, we kept a paper if it met specific inclusion criteria designed to address our research questions (see Table 2). Regarding this inclusion criteria, firstly, we limited the publication time between 2019 and 2024 to focus on recent advancements in AI in MMLA. Then, to facilitate comprehension and accessibility of the reviewers to the project's scope papers, the constraints, including written in English, Full text available, and Published in journals or conferences, were applied. Furthermore, restricting the candidate papers to the primary type of research ensures the depth and originality of their findings. Finally, to keep the relevance to our research questions, only studies that involve more than one modality and at least one AI application/technique within the context of education were considered for the selection step.

To enhance the reliability of the abstract screening step, this process was performed by three reviewers. Before beginning abstract paper screening, our three reviewers screened ten papers together to gain an in- depth understanding of inclusion criteria. Then, to increase the reliability of the screening, our two reviewers, MM and ET, independently screened a random set of 30 papers. Their inter- rater reliability (Cohen's kappa) was 0.861, which confirms the high consistency and reliability of screening by reviewers. Any conflicts or disagreements were resolved through discussion with a third reviewer, HKH, to ensure the consensus of the final candidate papers in our study. After resolving disagreements on two papers, the reviewers screened the remaining 431 papers, resulting in 168 included papers for full- text screening. To ensure inter- rater reliability during the full- text screening stage, our two reviewers screened a random set of 30 papers, achieving an inter- rater reliability (Cohen's kappa) of 0.7887. Three conflicted papers were resolved by HKH through a comprehensive analysis with the reviewers. After reaching this agreement, MM and ET screened the remaining 138 papers. Exclusions were made as follows: seven secondary papers, 41 papers with fewer than two modalities, 35 papers not contributing to the use of AI in MMLA, and 27 papers lacking empirical results. As a result, we identified 43 of the most relevant papers for further analysis.

# 4.2. Analysis

This subsection aims to identify and extract key information from selected papers to address our research questions. To achieve this, we coded the included papers as follows:

RQ1. To address this research question, we conducted our analysis from three perspectives: 1) Publication trends and quality profiles of AI- enhanced MMLA studies, 2) The geographic spread and co- authorship networks of authors of these papers, and 3) The disciplinary composition of lead researchers contributing to the field.

To achieve the first objective, we collected data on the publication years and paper type (conference or journal) from included studies, while also assessing the academic standing of these publication venues using established metrics, including SCImage Journal Rankings, which categorizes journals into quartiles Q1- Q4, for journal papers, and CORE Conference Rankings, which classifies conferences as A*, A, B, or C, for conferences papers. This analysis allowed us to track not only the temporal progression of research in the field but also the evolution of publication quality over time. From the second perspective, the university the authors belong to and their countries were recorded from each included paper to gain insights into the distribution of research efforts and potential patterns of collaboration. Finally, for the third perspective, we analyzed the interdisciplinary landscape of AI- enhanced MMLA research by examining the academic backgrounds of first authors across our included papers. Following methodological approaches established in previous AI in education reviews by Zawacki- Richter et al. (2019), Crompton and Burke (2023), and Bond et al. (2024), we systematically

![](images/a3673d06adc7ebd5402e1d910a3e950d79a064a535268b4e1375dccd7d942a33.jpg)  
Fig.2. t t studies.

extracted and coded the departmental affiliations of first authors from each publication.

RQ2. In response to this research question, we developed a coding scheme to identify and categorize the multifaceted objectives driving research efforts in the AI- enhanced MMLA literature. In this coding scheme, the pivotal variables, including the educational level under investigation, main stakeholders involved, and sources and modalities of input data were extracted from included papers. These codewords allow us to uncover the context, trends, and priorities of the AI- enhanced MMLA.

RQ3. Focusing on RQ3, we conducted a detailed data extraction process to capture key aspects of AI utilization in MMLA. It involved extracting critical details regarding the specific phases of the MMLA where AI was deployed, the particular AI techniques employed, and the role of AI in each phase. By coding and analyzing these facets from our selected papers, researchers can gain invaluable insights into the current landscape of AI in MMLA research, which is essential for advancing the field.

To make such systematic coding within our included papers, we developed a framework outlining the main phases, steps, and subcomponents of the MMLA process. Our framework builds upon previous MMLA frameworks (e.g., Di Mitri et al., 2018; Sharma et al., 2019; Shankar et al., 2023), learning analytics frameworks (e.g., Zhao et al., 2023; Sailer et al., 2024), and broader data fusion and modeling lifecycle frameworks (e.g., Torre- Bastida et al., 2021; Huang et al., 2023b). Importantly, we align with Gibson et al. (2023), Giannakos and Cukurova (2023), and Yan et al. (2025) studies that establishes learning theories as the essential foundation of effective learning analytics processes. As they demonstrate, learning theories provide the conceptual infrastructure that gives meaning and purpose to technical implementations, ensuring that analytics serve pedagogical aims rather than exist as only technological procedures. Through careful integration of these foundational works and their underlying theoretical perspectives into our framework, we synthesized a comprehensive framework consisting of eight key phases: Data Collection, Data Storage, Pre- processing, Annotation, Fusion, Modelling, Analysis, and Intervention (see Fig. 2). In the following, we provide the details of each step in depth.

The Data Collection phase includes two essential steps: Source Identification and Data Acquisition. During the source identification step, researchers determine which theoretical learning constructs to investigate (such as engagement, collaboration, or cognitive load) and aim to identify the most appropriate information sources (i.e., indicators) across different information spaces (physical, digital, physiological, environmental, and psychometric) that can effectively capture these constructs. For example, to assess engagement, researchers might select facial expressions and eye movements (physical space), system logs (digital space), and electrodermal activity (physiological space) as relevant information sources. The selection of these multiple information sources is guided by learning theories that identify which observable behaviors and signals best represent specific learning constructs (Blikstein & Worsley, 2016). This theoretically- driven process ensures that collected data meaningfully represents learning phenomena, creating a foundation for an enriched multimodal dataset that can capture the complex dimensions of the learning process (Worsley et al., 2016). Importantly, this relationship between theory and data is bidirectional, while existing learning theories guide information source selection, the multimodal analysis of novel information sources can also lead to refinements or extensions of learning theories themselves. After the source identification, various techniques and tools, such as face tracking and Kinect cameras, are employed in the Data Acquisition step to collect massive amounts of real- time data from these sources (Peng et al., 2021; Di Mitri et al., 2022). The high quality of the collected data in this phase is necessary for the success of MMLA, as it directly impacts the reliability and interpretability of the insights obtained from the next phases.

Then, the collected data from different modalities is sent to a learning record database (Erdas et al., 2017). The Data Storage phase aims to efficiently optimize the storage process in handling volume, velocity, and heterogeneity of the data through two key sub- components: Data Warehousing and Data Management. During the Data Warehousing process, collected multimodal data is stored and organized in a structured manner within the database or data warehouses. Then, the Data Management sub- component focuses on ensuring the privacy and accessibility of stored data by finding proper data indexing strategies, which is essential for efficient retrieval and analysis in the next phases.

The data pre- processing phase aims to convert raw captured data, including text, image, video, audio, and sensor data, into a consistent and well- structured format suitable for modeling and analysis. Anonymizing, Synchronization, Cleaning, Transformation, and Augmentation are the key sub- components in this phase. The Anonymizing sub- component focuses on the privacy concerns regarding the collected

data, such as their sensitivity or existing unexpected details or patterns unrelated to the learning task within them (Alwahaby & Cukurova, 2024). Removing personal information (for instance, addresses) and applying blurring or masking techniques in the collected data are two popular privacy- preserving techniques in this step. The data cleaning aims to enhance the dataset's quality and completeness while reducing redundancy by removing noise, duplicates, and irrelevant data, handling missing values, and correcting errors in the collected data (Rahul Katarya, 2023). Furthermore, the diversity of employed tools in the Data Acquisition necessitates the Symptomization step in the MMLA process to establish the alignment and coordination among the collected modalities in various timestamps (Shankar et al., 2023; Ochoa & Worsley, 2016). In the Transformation sub- component, the prepared raw data structures are converted into the required specific format (Shankar et al., 2023). One of the most well- known techniques in this step is extracting feature vectors from each data type. Lastly, in the data augmentation, the dataset is fostered by various augmentation techniques, like resampling, to increase its robustness, diversity, and class balance, making it more accurate and generalizable for analytical and modeling phases.

The Annotation phase consists of two sub- components: Labeling and Metadata Generation. In the Labeling sub- component, meaningful learning labels are assigned to the time intervals of the observed learning process (i.e., collected data), typically by experts or through self- reports. This translation process is specifically led by fundamental learning theories that inform which learning states, behaviors, and outcomes are meaningful to label and how they should be interpreted within the broader learning context (Di Mitri et al., 2018). These carefully assigned labels are then used to train and validate supervised learning models, creating a direct link between learning theory and algorithmic development. On the other hand, Metadata Generation provides supplementary context and information about the collected data, facilitating its effective interpretation and utilization (Mangaroska & Giannakos, 2018).

The Fusion phase focuses on unifying and increasing the effectiveness of the available data from different modalities through Feature Engineering, Standardization, and Feature Integration. Feature Engineering process seeks to condense multimodal data into a lower- dimensional space while preserving critical features as much as possible, extracting hidden information, and dropping noises and redundancy (Ayesha et al., 2020). Dimension reduction techniques, including feature extraction and feature selection, are the two main techniques used in this step. In the Standardization sub- component, discovered features are scaled (for instance, by the min- max scaling strategy) to ensure the equality of the modality's contribution in the unified feature space. Finally, Feature Integration provides a unified and enriched representation of collected data by combining complementary and consensus information from different modalities.

The Modelling phase in MMLA encompasses two key sub- components: Exploratory Data Analysis (EDA) and Model Learning. During the EDA process, initial examinations, often statistical analyses, are performed to understand the data distribution, patterns, and relationships, usually through visualizations like histograms, box plots, and scatter plots (Chan et al., 2023). The insights gained from this step are then used in the Model Learning step to select, develop, or build accurate models. In the Model Learning step, various AI models, including supervised, unsupervised, semi- supervised, and reinforcement learning models, are utilized to learn the complex and meaningful information from multimodal data aligned with the specific constructs.

The Analysis phase includes Statistical Analysis, Visualization, and Insight Generation sub- components. The Statistical Analysis component aims to evaluate the initial assumptions or theories about the multimodal data, typically through statistical tests. In the visualization process, a visual representation of the data and discovered insights are generated to facilitate the decision- making process for stakeholders. Finally, the Insight Generation component serves as a critical bridge between analytics and educational practice, where technical findings are translated into actionable recommendations. This process is grounded in educa tional principles, ensuring that patterns discovered by the models have pedagogical meaning and practical utility. The value of this component lies in making complex learning patterns understandable for educators, students, and administrators, transforming model outputs into meaningful guidance. This typically involves interpreting results to identify each modality's contribution and developing specific recommendations for stakeholders to improve the learning process.

The Intervention phase represents the practical implementation of the insights and recommendations derived from the Analysis phase. This phase involves designing and implementing targeted interventions, such as personalized learning pathways, new feedback mechanisms, modified instructional materials, or restructured learning activities, based on the comprehensive understanding gained from the multimodal data. The effectiveness of these interventions can then be evaluated through subsequent iterations of the MMLA process, creating a continuous improvement cycle that refines educational practices based on multimodal evidence. This phase connects the analytical power of MMLA directly to educational practice, ensuring that data driven insights lead to improvements in teaching and learning outcomes. Through this connection, MMLA becomes not just an analytical approach but a driver of evidence- based educational innovation.

RQ4. To address this research question, we systematically coded key details of experimental design, namely sample size, experimental environment, and any noted ethical considerations in the included papers. By analyzing these aspects, we aimed to assess the quality of AI- enhanced MMLA studies in terms of their reliability and generalizability and investigate their ethical accountability.

RQ5. The considered codewords regarding RQ5 involved extracting relevant information on the benefits and challenges of AI- enhanced MMLA approaches reported in the included papers. Systematically synthesizing such information can provide valuable insight for researchers, educators, policymakers, and other stakeholders in making informed decisions about the adoption, implementation, and optimization of AI- enhancing MMLA research.

# 5. Research findings

# 5.1. RQ1: what is the current state of research in AI-enhanced MMLA?

This research question analyzes the growth rate, publication quality, geographic spread, patterns of national and international collaboration, and disciplinary composition of lead researchers in AI- enhanced MMLA.

# 5.1.1. What is the evolution of publication volume and types of AI-enhanced MMLA studies over time?

Fig. 3 outlines the recorded trend in the publication of our included studies, comparing journal and conference outputs from 2019 to 2024. This figure shows irregular trends in publishing included papers over this period. In 2019, each of the journals and conferences had two publications. In the next year, 2020, a significant shift occurred, publishing five conference papers and three journal publications. However, the bin of 2021 suffers from a reduction in journal publications dropping back to two, and conference publications decreasing to four. This reduction continues into 2022, with conference publications dropping to two papers. A significant growth happened in 2023 by increasing the journal and conference publications to 10 and eight papers, respectively. This change confirms a recovery interest and activity in the field. Additionally, each of the journals and conferences published only one paper during the three months of 2024 (our final search on March 13), indicating a growth potential of AI- enhanced MMLA studies. Overall, the figure highlights the publication patterns in AI- enhanced MMLA research from 2019 to 2024, marked by initial growth, a mid- period decrease, and a recovery. This trend reflects the various factors influencing research and publication activities, particularly the peaks of COVID- 19 in 2021 and 2022.

![](images/696eb2b6dc5cf708451c52f9142fb7cfee0bea402c81e243825680612c703257.jpg)  
Fig. 3. Chronological trend in the publication of the AI-enhanced MMLA studies: Journals vs. Conferences.

![](images/729f952b295daa905b779bfbef3c1f092102b54f56f5bf3b0f7e82a9ad2f83a0.jpg)  
Fig. 4. Distribution of AI-enhanced MMLA studies across journal quartiles (Q1-Q2) and conference rankings (A-C). Journal classifications are based on SCImago Journal Rank, while conference rankings follow the CORE classification system. "NR" denotes venues not listed in these ranking systems.

# 5.1.2. What is the quality profile of publication venues in AI-enhanced MMLA research?

Fig. 4 illustrates the distribution of our included studies across established quality metrics. Notably, the majority of journal papers, 20 studies, were published in Q1- ranked journals, specifically, six studies in the Education and Information Technologies journal, followed by four papers in the British Journal of Educational Technology, accounting for half of the Q1 publications. Only one paper (Sabuncuoglu & Sezgin, 2023) appeared in a Q2- ranked journal (Proceedings of the ACM on Human- Computer Interaction), and there were no included papers that were published in Q3 or Q4 journals and met our inclusion criteria. This distribution in top- ranked journals reveals that AI- enhanced MMLA research is predominantly published through prestigious journal venues with a strong focus on educational technology applications.

For conference papers, we observed a more distributed pattern across different quality levels. Ten papers were presented at A- ranked conferences, predominantly at the Learning Analytics and Knowledge (LAK) conference, which accounted for seven papers. Eight papers were published at B- ranked conferences, e.g., the International Conference on Human- Computer Interaction and the International Conference on Multimodal Interaction. Additionally, one paper (Nandi et al., 2021) was presented at a C- ranked conference, the International Conference on Engineering Applications of Neural Networks. It is worth noting that three papers were from the International Conference on Pattern Recognition and Machine Intelligence, the International Conference on Interactive

Collaborative Learning, and the International Conference on Collaboration Technologies and Social Computing conferences that are not currently ranked in the CORE ranking systems (NR). This broader distribution across conference levels reflects the diversity of venues where emerging AI- enhanced MMLA research is being published, with high- quality conferences like LAK serving as primary platforms for publishing cutting- edge work in this field.

# 5.1.3. What is the geographical distribution of AI-enhanced MMLA studies?

For this question, we have depicted the geographical distribution of the contributing authors in the included AI- enhanced MMLA studies on a map as illustrated in Fig. 5. The numbers on this figure represent country contributions rather than individual papers, as many studies involved international collaborations. Consequently, a single paper with authors from multiple countries contributes to the count of each affiliated country. This approach highlights the global reach of AI- enhanced MMLA research and the extent of cross- border collaboration across 24 countries on six continents. Furthermore, Fig. 5 reveals that Europe emerged as the most prolific region, contributing to 27 papers. In this continent, Germany, Spain, the United Kingdom (UK), and Estonia led with contributions to four papers each. Norway and the Netherlands followed with three papers each. Finland contributed to two papers, while Italy, Sweden, and Switzerland each contributed to one paper. The figure also shows America as the second- ranked region, appeared in 19 papers, with the USA leading at 16 papers, followed by Canada, Mexico, and Ecuador with one paper each. Then, Asian authors, by contributing to 12 papers, highlighted this continent as the third representative. China was the primary contributor appearing in four papers, followed by Japan in three, India in two, and Taiwan, Malaysia, and Turkey in one paper each. Africa's contribution cannot be ignored, with Nigeria, Egypt, and Tunisia each making valuable contributions in one paper, reflecting the continent's emerging presence in this subject. Furthermore, Australian authors contributed to the conduct of two papers reflecting their participation in this research endeavor. From a global perspective of our results presented in this figure, the USA leads in the AI- enhanced MMLA publications, followed by China, Germany, Spain, the UK, and Estonia.

# 5.1.4. What are the patterns of authors' collaborations and their impact on the geographical spread of AI-MMLA studies?

For this question, we focused on the two levels of geographical spread in the collaboration patterns within the selected studies, highlighting international and national cooperations. Hence, we first classified the included studies by tagging them with two types: International for papers conducted by authors from multiple countries and National for those written by authors from different universities within a single country. This analysis resulted in 17 National, 14 International, and 16 unlabeled papers. Notably, each unlabeled paper was conducted by authors from a single university and not included in our analysis in this subsection. Additionally, for papers that contributed to spreading the field at both international and national levels, we considered them as papers involving two types of collaborations, consequently labeling them with two tags.

Next, we constructed a weighted hypergraph to visualize these patterns and identify the contributions of different countries in the geographical spreading of AI- enhanced MMLA studies. A hypergraph is a type of network that allows an edge to connect more than two vertices. This property made it a popular tool for modeling complex relationships and interactions, such as collaborations among multiple countries or universities (Gao et al., 2020). Our hypergraph, visualized in Fig. 6, consists of two key components: countries as nodes and International papers as hyperedges. Each hyperedge, marked by an ellipse, denotes an International paper, and its members (the nodes inside it) represent the countries that contributed to the publication of that paper. Also, in this graph, we incorporated the internal collaborative spread of countries by weighting the nodes based on the number of their national papers con

![](images/b24183459d60d8a1645d8b97364f5c0bc7a69302b4cbe450a1c95e8180a7bda4.jpg)  
Fig. 5. Geographical distribution of country contributions in AI-enhanced MMLA studies. Numbers represent papers with at least one author affiliated with each country.

![](images/84266eb010c711622a158482bf8384413b754afe5e2290ea38e42247e53448c9.jpg)  
Fig. 6. Collabation Network among Countries and Universities in the AI-enhanced MMLA Studies. Each country, reented by a node in this network, is weighted by the number of collaborative studies between its universities (national papers). And, each ellipse, a hyperedge, encompasses a group of countries involved in an international paper.

ducted. Notably, we indicated the weight of each node by the size and the number displayed on it.

From Fig. 6, it is observable that the USA plays a critical role in spreading the field by collaborating on conducting four International and five national papers. Spain, with three International and two national papers, is marked as the second spreader, followed by China and Germany, which each contributed to three International and one national paper. Estonia and the UK each shared the AI- enhanced MMLA knowledge across three International papers, and Japan published two International papers and one national paper. Hence, these countries are strong candidates for the third position. By continuing our analysis, Canada, Egypt, and the Netherlands were recognized as the next top geographical spreading, each conducting one International and one National paper. Additionally, Norway also ranked alongside them by collaborating on two International papers. Among the remaining 13 countries involved in the included papers, 12 countries collaborated on only one paper that can have a role in the geographical spreading of the field, and Australian papers didn't contribute to spreading knowledge between countries or diverse universities.

# 5.1.5. What academic disciplines are driving AI-enhanced MMLA studies?

Fig. 7 presents the distribution of academic disciplines contributing to the included AI- enhanced MMLA studies. This result reveals that Education dominates the field, contributing 20 papers, suggesting that

![](images/2614d61b63a1ba60457aa17716e67d7a0a00a931ac402c12e6de37cf7f09440f.jpg)  
Fig. 7. Distribution of the first authors of the AI-enhanced MMLA studies across academic disciplines.

![](images/3a0c2dc9044842122cf7e25d1c5e989ba4279721de63d4e233bda044b6018293.jpg)  
Fig. 8. Distribution of the AI-enhanced MMLA studies across academic levels.

educational researchers are at the forefront of integrating AI into MMLA. Within Education, we identified several subdisciplines, including Educational Technology & Learning Sciences (8 papers), Teacher Education & Educational Sciences (5 papers), Educational Leadership & Policy (3 papers), Science & Subject Education (2 papers), and Educational Assessment & Testing (2 papers). This diversity reflects the multifaceted educational perspectives shaping AI- enhanced MMLA research. Computer Science & IT is the second most represented discipline with 10 papers, underscoring the critical technical foundation required for developing and implementing AI techniques in MMLA. Engineering follows closely with eight papers, with contributions primarily from the Electrical Engineering and Computer Engineering departments, further highlighting the role of technical expertise in driving methodological advancements in the field. The remaining contributions come from Humanities and Social Sciences (2 papers), Interdisciplinary Research Institutes (2 papers), and Informatics (1 paper). This distribution confirms that while education researchers lead in advancing AI- enhanced MMLA, the field draws strength from diverse academic perspectives.

Additionally, our result indicates that six papers originate from nonacademic entities, such as independent research organizations like the Educational Testing Service and industry research units like Fujitsu Research. These non- academic contributors provide valuable perspectives that complement traditional academic research, often focusing on applied solutions and real- world implementations.

# 5.2. RQ2: in what contexts are AI-enhanced MMLA being applied

In this section, we explore the context of AI- enhanced MMLA studies based on three key aspects: level of education, stakeholders, and used modalities.

# 5.2.1. What educational levels are the primary focus of AI-enhanced MMLA studies?

The included papers spanned a broad wide range of educational levels, as outlined in Fig. 8. The majority of them, 22 studies, focused on enhancing the learning process at the tertiary level. Among them, the authors of seven papers conducted their studies at the undergraduate level in universities. For instance, one paper sought to improve nurse students' learning and patient outcomes (Vatral et al., 2023). The other critical tertiary level for researchers was the graduate group of students, which was the focus of three studies. In one of these (Peng et al., 2021), the authors aimed at improving instructors' understanding and students' learning outcomes. Two included papers (Emerson et al., 2023, 2020a) specifically focused on enhancing the performance and interest of col

![](images/810e4e5305d19d812f5ae2e749b24713060a8da9e328c71689e710990f9fc4be.jpg)  
Fig. 9. Distribution of the AI-enhanced MMLA studies across identified target audiences.

lege students in game- based learning environments. Finally, one study targeted postgraduate education to improve their collaborative learning outcomes (Zhou et al., 2024). It is worth noting that the most conducted papers in tertiary education, none, did not specify the education levels from which data were collected. For example, the authors of (Sabuncuoglu & Sezgin, 2023) assisted higher education students in evaluating their engagement over time. In another study, (Di Mitri et al., 2022), the authors presented a real- time feedback CPR Tutor for training students.

Secondary education, with ten papers, was the second most addressed level of education in our included studies. Three of these papers focused their studies on middle school students to enhance their learning process. For instance, improving the performance of middle school students during educational gameplay was the aim of (Moon et al., 2022). Additionally, two papers supported high school students in achieving better collaborative learning outcomes, exemplified by Olsen et al. (2020). Notably, the remaining five papers, such as that by Chejara et al. (2023b), conducted their experiments on the less explored level of secondary education to augment the collaborative learning quality of the students.

Three papers conducted their experiments at the primary level of education to improve students' engagement or learning performance, which can be exemplified by Emerson et al. (2020b) and Chettaoui et al. (2023), respectively. Two papers addressed the learning process enhancement at the level of early childhood education. For instance, one of them concentrated on Students With Special Education Needs to enhance the existing Applied Behavior Analysis (ABA) therapy (Chan et al., 2023). Moreover, among our analyses, one paper conducted its studies on the in- service teachers to develop technological pedagogical content knowledge (Huang et al., 2023a).

Furthermore, a key observation is that the educational level of participants in five of the included studies was unknown. These papers have mainly focused on improving learning outcomes (e.g., Nandi et al., 2021), facilitating management and implementation of the lesson plan in offline classrooms (Akila et al., 2023), developing tools that capture 21st- century skills (e.g., Huang et al., 2019), and improving learners' self- regulated learning (Yun et al., 2020).

# 5.2.2. Who are the main stakeholders targeted by AI-enhanced MMLA papers?

In this subsection, we categorized the papers based on their target audiences to understand how they addressed the needs of various stakeholder groups. We identified four types of stakeholders: Learners, Instructors, Researchers, and Technology Developers. Fig. 9 depicts the distribution of papers across these groups.

Our analysis revealed a significant subset of papers with learners as their stakeholders (19 papers). They sought to enhance the learning experience, improve academic performance, or enrich students' engagement. For instance, Sabuncuoglu and Sezgin (2023) presented the engagement level of students on dashboards to facilitate their self- evaluation skills. Another set of papers, tailored for researchers (16 papers), endeavored to enrich and expand the knowledge of MMLA by suggesting new insights, methods, and frameworks and discovering hidden aspects of this field. As an example, (e.g., Chejara et al., 2023b) addressed the generalizability of the proposed methods for collaboration quality estimation by focusing on the difference in time scales of multimodal data. The papers with Instructors as their stakeholders (12 papers) aimed to foster the instructors' insights into the learning process, highlight the diverse needs of learners, and provide real- time decision support for instructors. As a sample of this category, Peng et al. (2021) emphasized the students' mental states for teachers as an essential constructor for monitoring to guide them about adapting learning materials for improving students' learning outcomes. Furthermore, our analysis showed that technology developers were the target audience of 11 papers. These papers aimed to develop the assistant tools and technological infrastructures for facilitating and augmenting the learning process. For example, in (Cebral- Loureda & Torres- Huitzil, 2021), the smart infrastructure of a digital humanities laboratory was enhanced by a novel, deep learning- based model to capture the emotional states of students.

It is worth noting that our analysis showed some overlaps among these categories. Six papers targeted both Learners and researchers. For example, Moon et al. (2022) proposed new insights into the support of cognitive- affective states of students during educational gameplay. Three papers addressed the needs of both Instructors and Technology. For instance, Chango et al. (2021) developed Intelligent Tutoring Systems (ITS) to predict student performance, providing teachers with clear explanations of this prediction. Two papers supported both researchers and technology developers. For example, Huang et al. (2019) targeted these stakeholders by developing tools to capture multimodal data for discovering different collaborative learning insights. Learners and Instructors both were the target audiences of the two papers. For example, Kawamura et al. (2021) sought to advise learner and teacher feedback to MMLA. At last, we found Learners and Technology developers as the stakeholders in two papers. For instance, Di Mitri et al. (2022) designed and developed a cardiopulmonary resuscitation (CPR) tutor, improving the learners' CPR skills by providing audio feedback.

# 5.2.3. What modalities are used in AI-enhanced MMLA papers?

We used the categories outlined by Di Mitri et al. (2018) and Mu et al. (2020) for grouping selected papers into different information spaces and modalities. Fig. 10 provides an overview of the results.

Physical space. In this space, authors seek to specify meaningful learners' physical activities that reflect or influence different dimensions of their learning process. Authors of 18 out of 43 included papers, exemplified by (Israel et al., 2021), involved the facial expression features in their learning analysis processes, which confirms their significant role in exploring meaningful insights into the learning process. Speech modality is another prominent physical activity that emerged in 17 studies such as (Lin et al., 2023), followed by Eye- tracking modalities in 12 papers (e.g., Emerson et al., 2020a). Moon and Gesture modalities were each used in seven papers. They can be exemplified by (Chng et al., 2020) and (Closser et al., 2022), respectively. Additionally, Visual features were collected in six papers, such as (Cebral- Loureda & Torres- Huitzil, 2021). Head position and posture each fed into the learning analytics in two papers, for instance, (Akila et al., 2023) and (Yusuf et al., 2023). Hand and eyebrow movements and the mouth region were other physical activities considered by the authors in one paper each. (Lee et al., 2023) and (Chejara et al., 2023a) serve as examples for them.

Digital space. This space encompasses diverse digital traces on digital platforms engaged in the learning process, revealing the state of the learning process. Systems logs are informative digital traces collected by authors of 12 included studies, such as (Chango et al., 2021). Additionally, textual information like learners' chats, posts, and comments is the second most frequently focused digital data among the included papers, appearing in six studies, which can be exemplified by Ouyang et al. (2023). Performance data appearing in five papers (e.g., Chango et al., 2021), stood in third place. Furthermore, screen recordings were

![](images/f01e92e718d2a3a9758794e3f2b7aa8bd00609489a4c875d5678d21e21650b19.jpg)  
Fig. 10. Frequency of modalities utilized in the AI-enhanced MMLA studies, categorized by physical, digital, physiological, psychometric, and environmental spaces (Mu et al., 2020).

another digital trace considered in the two included studies, such as Ouyang et al. (2023).

Physiological space. The factors in this space reflect the learners' mental and health state during their learning. EDA, the most frequently used physiological data among the included papers, exemplified by (Reilly & Schneider, 2019), emerged in 12 studies, followed by Electrocardiogram (ECG) data in four papers, such as (Yun et al., 2020). Skin temperature, Respiratory Belt (RB), and Electroencephalogram (EEG) were each utilized in two papers. They can be exemplified by (Chan et al., 2023), (Nandi et al., 2021), and (Sharma et al., 2019), respectively. Furthermore, Blood Volume Pulse (BVP) used in (Sharma et al., 2019), and Electromyographic (EMG) employed in (Di Mitri et al., 2022) were other involved physiological modalities.

Psychometric space. This information source, which appeared in nine papers (for instance, in (Liu et al., 2023)), primarily captured the learners' mental states through their self- report questionnaires.

Environmental space. In this case, authors aimed to identify key factors in the physical environment that significantly influenced students' learning processes. Among our 43 analyses, four papers, such as (Li et al., 2023), involved the location or spatial information of learners during their learning analyses. Also, the data on humidity, temperature, light intensity, and indoor carbon dioxide collected together in paper (Chan et al., 2023), along with seat pressure data used in paper (Kawamura et al., 2021), emerged as other critical environmental modalities during our analyses.

It is crucial to note that integrating information from these spaces allows authors to explore unknown aspects of learning and gain holistic and accurate insights into the learning process. Fig. 11 depicts the distribution of included papers across information spaces. This figure confirms the high usage of information modalities from physical space in the AI- enhanced MMLA studies, which appeared in 41 papers. Among these, nine papers collected their multimodal data exclusively from this space. For instance, the authors of (Ivleva et al., 2023) combined audio (speech) and facial expression modalities to recognize the emotional state of students and teachers. Digital space, with 19 appearances in the papers, ranked as the second most used informative space. In 15 of these papers, modalities were from both Physical and Digital spaces. For example, one study utilized gaze (eye- tracking), speech, and log data (from an intelligent tutoring system) to predict collaborative learning outcomes (Olsen et al., 2020).

![](images/296559d444d93a25ef61a6107855c81ff97c45d44969ed3ed980d291e67ddb04.jpg)  
Fig. 11. Distribution of the AI-enhanced MMLA studies across information spaces.

Modalities from the physiological space, appearing in 10 papers, were the next most frequently used. In the four cases of these papers, the authors integrated this information with physical modalities to ex

![](images/132b07b871bedd904947f49548736231ba3fe2ba4b4654b73c5f83b368746d3a.jpg)  
Fig. 12. Frequency of AI utilization across various phases and subcomponents of the MMLA process in the AI-enhanced MMLA studies.

plore diverse learning dimensions. As an example, in one paper (Huang et al., 2019), the collaborative learning states were estimated by integrating EDA data with eye- tracking and motion data. It is worth noting that the authors of the two papers focused particularly on fusing the different modalities from this space. Specifically, the authors of (Nandi et al., 2021) collected EEG, EDA, and RB to estimate learners' emotions in the e- learning context. Next, questionnaires from the psychometric space were the most utilized modalities among the included papers, appearing in nine studies. Among them, four papers focused on different aspects of learning by analyzing modalities from both physical and psychometric spaces. Take the case of one study where the authors collected gaze, speech, and questionnaire data to interpret effective collaborative learning interactions (Zhou et al., 2024). Moreover, three papers enriched their data collection by integrating modalities from psychometric space with both physical and digital spaces. For instance, in one study, facial expressions, eye tracking, system logs, performance data, and questionnaires were investigated to predict student performance and interest after interacting with a game- based learning environment (Emerson et al., 2020a).

Lastly, environmental modalities appeared in six papers, combined with modalities from other spaces. Two papers utilized these modalities alongside physical and physiological information. For example, the authors of (Kawamura et al., 2021) utilized students' heart rates, seat pressure, and facial expressions to model their level of wakefulness. One paper integrated them with physical modalities (i.e., Zhao et al., 2024), and another (i.e., Chan et al., 2023) combined them with modalities from physical, digital, and physiological spaces. Additionally, one paper contained multimodal data collected from environmental, physiological, physical, and psychometric spaces. In particular, its authors enhanced their analyses by investigating the 3D positions of body joints alongside the collected questionnaires from teachers and the gathered EMG and visual data from students to develop a CPR tutor system with real- time feedback generation (Di Mitri et al., 2022). Finally, in another study, spatial data was combined with visual and questionnaire information to estimate the position of participants within the learning environment (Li et al., 2023).

# 5.3. RQ3: how has AI been implemented in MMLA studies?

Our analysis categorized the implementation of AI into several main phases based on the framework presented in Fig. 2, above.

# 5.3.1. At which stages of the MMLA process has AI been implemented?

Fig. 12 shows a visual representation of the roles distribution of AI throughout various phases of MMLA in the analyzed papers. Notably, the majority of these papers, totaling 36, leveraged AI techniques predominantly in the Model Learning step to extract valuable information from multi- modal datasets, aiding decision- making processes. This prominence suggests that institutions are increasingly capable of processing complex student interaction data to support evidence- based educational decisions and personalized learning interventions.

Moreover, the pre- processing phase stands in the second rank of times used of AI techniques, occurring 34 times across 43 papers. In this phase, the authors of a paper utilized AI techniques for the privacy preservation of learners, and four papers employed AI methods to address noise, missing values, and erroneous data. Furthermore, in the Data Transformation sub- component, AI techniques played pivotal roles in converting data into formats suitable for analysis, as observed in 26 studies. Additionally, three papers enhanced the class balance of multi- modal data through popular data augmentation techniques in AI. These applications are crucial for ensuring ethical, reliable, and robust learning analytics systems that educators can trust.

Furthermore, during our analysis, various AI techniques emerged in 15 papers for feature engineering from feature selection and feature extraction perspectives, positioning the Fusion phase as the third most AI- enhanced phase in MMLA. This indicates growing sophistication in combining multiple learning modalities to create more comprehensive student models, potentially enabling teachers to understand complex learning behaviors better. Then, in the Data Collection phase, eight papers employed AI techniques to enhance the accuracy and richness of the collected data during the Data Acquisition step. Also, the authors of two papers applied these techniques to derive actionable insights from the modeling results in the Analysis phase, which ranked after Data collection. The Analysis phase showed emerging applications in deriving

![](images/6c6fd94b50d0875dab6966a384f0112e4bc96fe85fcf69cc661812442b40fea8.jpg)  
Fig.13. Recognition, Gaze Behavior Identification, and Spatial Orientation Identification tasks.

actionable insights from models, though only in two papers, indicating an area for potential growth in translating technical findings into practical teaching strategies.

Lastly, an AI technique was used to annotate data with relevant labels in the Labeling step, placing the Data annotation phase as the least AI- enhanced phase in MMLA. The detailed descriptions of the AI- enhanced steps for each included paper are reported in Appendix A (Table 5). This pattern shows us where AI is already making its mark in MMLA, especially in model learning and data pre- processing, while also revealing areas that remain largely untapped - pointing to exciting new paths for future research.

# 5.3.2. What AI techniques are commonly used in MMLA studies?

The following details outline the AI techniques employed across the various AI- enhanced phases of MMLA in the included papers.

AI- enhanced approaches in the Data Collection phase Concerning this phase, in the data acquisition step of the eight included studies, advanced AI techniques were applied to ensure the quality and comprehensiveness of the collected data. These methods effectively minimized redundancy, facilitating the subsequent pre- processing and storage steps. They were tailored to five diverse tasks: Face Recognition, Facial Expression, Speech Recognition, Spatial Orientation Identification, and Gaze Behavior Identification. These tasks aligned with the specific goals of the papers, and each contributed to a more robust data collection framework. Fig. 13 illustrates the distribution of the included studies across these tasks, as well as the employed AI- enhanced tools in each category are as follows:

- Face Recognition: In paper (Chng et al., 2020), the OpenFace toolkit was used to identify students and instructors by labeling individuals in each data collection instance. This application was crucial for detecting episodes of collaboration. In another study, Ivleva et al. (2023) generated their data with the Google Image Search API, and FER2013 automatically detected, centered, resized, and cropped the facial regions, ensuring that each face was roughly centered and occupied a similar amount of space in each image. Furthermore, BlazeFace, a neural network model for real-time face detection, was used by (Alre et al., 2023) to confirm the presence of faces in the frames during the data acquisition process.

- Facial Expression: Authors in paper (Peng et al., 2021) employed ARKit packages for face tracking on the iPhone, involving AI techniques to process depth sensor data and generate a facial mesh. Additionally, they utilized ARKit packages to detect various facial attributes and calculate blend shape coefficients that rely on AI algorithms. In another study (Moon et al., 2022), student facial data was collected using two well-known facial-expression detection toolkits: OpenFace and Facial Expression Recognition (FER- 2013). OpenFace generates AU data by tracking facial- muscle movements, while the FER- 2013 data- driven open- source toolkit computes the probabilities of the "big five" emotions based on image- based emotion classification data.

- Speech Recognition: CoTrack, an audio-capturing prototype system, was utilized to perform voice detection, directional analysis, and feature extraction from the audio during the data acquisition process of the paper (Chejara et al., 2020).

- Spatial Orientation Identification: Authors in (Di Mitri et al., 2022) used the Microsoft Kinect v2 depth camera tool for collecting data in the CPR Tutor, a C# application running on a Windows 10 computer. This tool utilizes depth- sensing technology, which often involves machine learning algorithms for depth estimation and body tracking. These algorithms enable the camera to capture 3D kinematic data of body joints.

- Gaze Behavior Identification: In the data acquisition phase of (Chettaoui et al., 2023), the gaze module, an AI-driven gaze tracking system, was employed to track students' visual attention on specified Areas of Interest (AOIs) in real-time. This algorithm leverages the Dlib library's facial key points predictor, which combines facial detection with gaze-tracking capabilities. The Dlib library was chosen due to its pre-trained detector based on the IBUG 300-W face landmark dataset, employing an ensemble of regression trees to identify 68 facial landmarks.

- AI-enhanced approaches in the Pre-processing phase The AI techniques discovered in the different steps of the Pre-processing phase, based on the analysis of 43 papers, are as follows:

Data anonymization. In study (Li et al., 2023), authors utilized OpenCV and the MMTracking algorithm to anonymize students' facial identities. They achieved this by hiding the facial identities with the black boxes placed on the top 20% region of the tracking box generated through these techniques.

Data cleaning. In this step, paper (Ivleva et al., 2023) utilized a face recognition library to validate and filter out non- face images from the dataset. Additionally, the Deepface framework was employed to identify images with high emotion recognition rates and to eliminate incorrectly labeled images from the dataset. In paper (Sabuncuoglu & Sezgin, 2023), authors employed Dlib's feature extractor to promote data uniformity by centering and cropping faces to a standardized resolution. Moreover, in another paper (Moon et al., 2022), the data bias and missing data problems in multimodal data were addressed by a KNN- based imputation technique. Also, the authors of paper (Chng et al., 2020) handled the is

![](images/80f7799059155d8997672fc836a8c3b25ffc802a67d95ed558d32d46297ccd68.jpg)  
Fig.14. t   t   i t orientation, Gaze Behavior Identification, Natural Language Processing, Face Recognition, and Attention Behavior Identification tasks.

sue of duplicate arising from the simultaneous use of two Kinect sensors by aiding the Decision Tree method.

Data transformation. In most cases of our included papers, this step involved extracting task- specific features from raw data. We categorized the utilized AI techniques in this step of the included paper based on the used strategies, as illustrated in Fig. 14, as follows:

- Facial Expression: During our analysis, five studies (e.g., Ma et al., 2022; Sabuncuoglu & Sezgin, 2023) utilized the OpenFace toolkit to extract facial Action Units (AUs) or 3DFace Landmarks of the frames captured from videos directly related to facial expression. Additionally, this toolkit was employed to generate the facial expression recognition modules in the (Lin et al., 2023). On the other hand, FACET is another well-known toolkit that uses computer vision and machine learning techniques to analyze facial expressions and emotions from video streams or images. This toolkit was used to extract and analyze AUs from video frames during the Data Transformation step of two papers (Emerson et al., 2023, 2020a).

Concerning the Data Transformation step of the remaining included papers, in the study by Peng et al. (2019), a well- trained universal model (haarcascade frontalface- default) in OpenCV was employed to detect and extract facial expressions from recorded videos (screenshots per second), while Cebral- Loureda and Torres- Huitzil (2021) combined OpenCV with a face recognition library to identify the critical points of the user's face. Additionally, they utilized the Py- Feat library to identify facial expression features, including action units, emotions, and landmarks, from images and videos. In the next paper (Chango et al., 2021), the authors analyzed the videos using the Microsoft Emotion API (2019 Automatic Facial Recognition Software), which involved processing the video frames to transform them into structured and categorized emotional data suitable for further analysis. Also, to analyze the captured frames from students' face videos, an open- source Python package that contains a CNN was performed by Israel et al. (2021). Finally, in the paper by Akila et al. (2023), a deep- learning model was trained and fine- tuned offline with facial images captured from video frames to identify the target person's facial features.

- Speech Recognition: The OpenAI Whisper speech recognition package was used by authors of two studies, Lin et al. (2023) and Zhao et al. (2024), for automatically generating transcriptions from audio data. Among them, the paper by Zhao et al. (2024) focused on the Whisper-large model. Additionally, both Chejara et al. (2023b) and Zhao et al. (2024) employed Voice Activity Detection (VAD) methods to extract utterance timing (i.e., speaking time and turn-taking) from continuous audio data. In another study (Zhou et al., 2024), the audio information, including the content, speaker

ID, and time stamps of the start and end time of each speech, were automatically detected by Amazon Transcribe, an automatic speech recognition service, and saved in as.json files.

As further examples in this category of methods, in the paper by Ivleva et al. (2023), the Libros library was operated to extract key audio features, including Root Mean Square Energy, Zero- Crossing Rate, and Mel Frequency Cepstral Coefficients (MFCCs), which were concatenated and returned as NumPy array tailored to emotion detection task. In the paper by Ma et al. (2022), authors used online transcription services to generate the textual transcript for each dyad, openSMILE for extracting acoustic- prosodic features, and VG- Gish for generating embeddings from audio spectrograms in its Data Transformation step. Additionally, the authors of (Vatral et al., 2023) employed a trained deep- learning model to extract specific features from audio data. Finally, the Automatic Speech Recognition (ASR) models have been applied for student categorization by authors of paper (Akila et al., 2022) in this step.

- Spatial Orientation Identification: In three papers placed in this category, e.g., Kawamura et al. (2021) and Ma et al. (2022), the OpenFace toolkit was employed to estimate the head poses, resulting in a vector containing the location of the head concerning the camera. Additionally, the authors of (Sabuncuoglu & Sezgin, 2023) and (Cukurova et al., 2020) utilized the OpenPose, a powerful deep learning-based library, to extract pose features from videos. In paper (Akila et al., 2023), the Transforming Eyesight with Retina face model was employed to extract the head-pose parameters from video data. In another paper (Cebral- Loureda & Torres- Huitzil, 2021), MediaPipe was used for gesture recognition. MediaPipe is an open-source framework that provides artificial vision technologies, a combination of deep-learning-based models, to developers for quickly creating visual perception applications, such as gesture recognition, object tracking, face detection, and multiband tracking, with real-time performance. At last, the authors of (Li et al., 2023) adopted the multi-object tracking function implemented in MMTracking to extract the positions and motions from video data.

- Gaze Behavior Identification: Among the 43 included papers, two papers, (Ma et al., 2022) and (Sabuncuoglu & Sezgin, 2023), utilized OpenFace as a gaze behavior identification toolkit for analyzing video data. In the former, the authors used this toolkit for identifying specific gaze behaviors, including eye landmarks, eye direction vectors, and eye directions in radius. In the other case, it was employed to extract Gaze Directions from video frames. Additionally, the (Lin et al., 2023) authors employed OpenFace 2.0 to build the eye gaze tracking modules. Also, they utilized the Yolov7- tiny to specify the objects that students were looking at. In the paper by Zhou et al. (2024), the authors utilized computer vision tech

![](images/8e6895b7785ba680a8dd52374fa15e423f8d26af8f6826d8bbd2cf6dca5c8844.jpg)  
Fig. 15. Frequency of AI methods utilized in the Feature Engineering step of AI-enhanced MMLA studies, categorized by Feature Extraction and Feature Selection strategies.

niques, known for their high accuracy (Zhou et al., 2023), and the Computer Vision Annotation Tool (CVAT) (cvat.org) for identifying and labeling gaze behaviors (i.e., gazing at peers, gazing at laptops, gazing at tutors, and gazing at other objects) in video data, respectively. In another study (Vatral et al., 2023), the authors utilized the YOLOvSL model to generate person- class bounding boxes, enabling the computation of a specific eye gaze feature called PersonGaze by measuring the overlap between them and Tobii gaze coordinates. At last, in paper (Peng et al., 2021), the eye- blink frequency, an eye- related feature, was estimated using the peak detection methods employed to the coefficient time series data.

- Natural Language Processing: The studies in this category employed AI-enhanced models to transform the text into numerical representations (vectors) that capture semantic meaning. In the paper by Peng et al. (2019), the authors involved the Baidu NLP to analyze and classify the emotional tendency of each 'Danmaku' comment. Then, Jieba, an NLP tool, specifically designed for Chinese text segmentation, was adopted to segment sentences into individual words. Finally, they used Gensim, a library for topic modeling and document similarity analysis using unsupervised machine learning, to convert words into vector representations. In the paper by Ma et al. (2022) employed three language models, including Word2Vec, Fine-tuned BERT, and Speaker-aware Fine-tuned BERT, to extract semantic features from the generated textual transcript from audio data. In the paper by Emerson et al. (2023), two well-known word embedding techniques, 300-dimensional GloVe embeddings and 1024-dimensional ELMo embeddings, were applied to encode the students' written reflection response.

- Face Recognition: In the paper by Akila et al. (2023), the Transforming Eyesight with Retina face (TER) model was utilized to identify faces from video data collected using Internet of Things (IoT) technologies in the classroom. In another study (Chejara et al., 2023a), faces were detected using the face detection framework proposed by Viola and Jones (2004), which contains image processing and AdaBoost learning methods, to identify mouth regions as a vision-based speaking activity feature. Additionally, in the paper by Cukurova et al. (2020), FaceNet, an open-source library, was involved in the feature extraction process from videos by recognizing students and matching their actual IDs with assigned random identifiers.

- Attention Regulator Behavior Identification: Lee et al. (2022) utilized ResNet architectures (ResNet-18, ResNet-50, ResNet-101), pre-trained on ImageNet, to extract frame-level attention regulator behaviors from video data. Additionally, they employed a CNN-RNN model to identify the video level of these behaviors.

Data Augmentation. Imbalanced classification is a common problem in machine learning where the training dataset has an imbalanced distribution of class instances. This imbalance leads to poor predictive performance, particularly for the minority class, addressed in the Data Augmentation steps of three included studies (e.g., Moon et al., 2022; Lee et al., 2023). Moon et al. (2022) and Lee et al. (2023) generated synthetic samples for the minority class using the SMOTE, which is based on the k- Nearest Neighbor (kNN) algorithm. While in the paper by Chan et al. (2023), the APIs from the Python imbalanced- learn toolbox were chosen to perform several data resampling techniques, including RUS, AllKNN, Tomek, ROS, SMOTE, SMOTENC, SVMSMOTE, SMOTETomek.

AI- enhanced approaches in the Fusion phase In this phase of MMLA, the feature engineering process plays a pivotal role in preparing multimodal data for analysis. In the following, we outline the AI methods utilized in this sub- component of included papers from the perspective of their strategies, including feature extraction and feature selection. Also, Fig. 15 illustrates the distribution of the used methods across these strategies.

- Feature Extraction. Feature extraction methods focus on discovering informative task-specific features by extracting hidden features or patterns from data that enhance model performance or interpretability (Guyon & Elisseeff, 2006). Principal Component Analysis, PCA, is a well-known unsupervised feature extractor used in five included papers (e.g., Sharma et al., 2019; Yun et al., 2020). Notably, PCA in these papers significantly reduced the chance of models overfitting. Also, Singular Value Decomposition, SVD, is

![](images/7361101eb714317e08801f815a3fa8aa1bbcac8bb274266c73228c04efccaea9.jpg)  
Fig.16.  t  t tti Traditional Clustering, Ensemble Learning, and Deep Learning techniques.

another unsupervised method employed by Moon et al. (2022) to extract features from data. Closser et al. (2022) involved k- means clustering analysis to identify behavior profiles that emerged across the participants and to examine verbal strategies. Furthermore, in paper by Ouyang et al. (2023), Optimal Matching (OM) and Ward's Clustering (WC) methods both cooperated to find collaborative pattern types among collaborative problem- solving (CPS) activities. Then, the transitional characteristics between hidden states (sequence features) of the collaborative patterns were revealed by fitting Hidden Markov Models (HMM) for each type of collaboration pattern. Moreover, Yusuf et al. (2023) employed HMM to estimate the transition matrix from the observed behavioral features. The OM was then used to identify representative sequences of the hidden states.

On the other hand, we found two included papers that adapted supervised machine learning methods in this step, consequently making the extracted features more specific for their applications. Cukurova et al. (2020) utilized Decision Tree methods (DT) to initialize indicators for making, watching, speaking, and listening from the data. Then, these indicators were used as new features and fed into another model to predict competence in the modeling phase. Lee et al. (2022) adopted a kNN classifier to identify Attention regulation behaviors, which then were used as input for some classification models for Attentional state recognition.

Feature Selection. Feature selection methods directly identify and select the most relevant features from a data for modeling. By eliminating irrelevant, redundant, or noisy features, these techniques help to improve model performance and reduce overfitting. Additionally, they enhance interpretability by preserving the physical meanings of the original features (Li et al., 2017). Random Forest (RF), a popular machine learning method, was used for the feature selection process in two papers, by Sharma et al. (2019) and Yun et al. (2020), due to its ability to provide feature importance scores. As another method, Ma et al. (2022) employed a Support Vec tor Machine, SVM, to specify the predictive unimodal features for impasse detection. SVM is known for its robust classification performance, particularly with small- sized datasets. In the continuation of analysis, we found that the CfsSubsetEval method, a correlation- based feature selection technique implemented in the WEKA tool, was used in by Chango et al. (2021). Peng et al. (2021) adopted the RELIEF- F method (Urbanowicki et al., 2018) to select a set of relevant features from the data. Emerson et al. (2020b) utilized univariate linear regression tests for identifying features with p- values less than or equal to 0.15 from the training set. In the final case (Chettaoui et al., 2023), the chi- square distribution function was involved in selecting K's best features. It is worth noting that the founded methods in this step of included papers have involved the ground truth labels during their feature selection process, indicating a supervised approach.

AI- enhanced approaches in the Annotation phase During our 43 analyses, we identified only one paper that employed AI- enhanced methods for annotation. Akre et al. (2023), involved the MobileNet architecture during their labeling process. This neural network model was fine- tuned on the DAISEE dataset and used to generate pseudo- labels for the multimodal data.

AI- enhanced approaches in the Modelling phase Among the included papers, the Model Learning step of this phase has received the most attention for utilizing AI techniques. We categorized employed methods in this step in terms of their highlighted characteristics in their papers, illustrated in Fig. 16, as follows:

- Traditional Classification Models. These methods were employed to classify data into predefined classes or categories by using traditional techniques. SVM emerged as the most frequently used traditional classification model, contributing to eight studies (e.g., Reilly & Schneider, 2019; Sharma et al., 2019). DT and kNN models each

appeared in five studies, which can be exemplified by (Chettaoui et al., 2023) and (Chan et al., 2023), respectively, while the Naive Bayes Classifier (NBC) was used in four papers (i.g., Yusuf et al., 2023; Reilly & Schneider, 2019). In two other studies, e.g., (Peng et al., 2021), Multi- Layer Perceptron (MLP) appeared as a classification model. Rule Induction and Bayesian Network models were each employed in one study (Chango et al., 2021; Lee et al., 2022).

- Traditional Regression Models. The models in this category were designed to address tasks with continuous target variables by predicting their values. Support Vector Regression (SVR), Logistic Regression, Linear Regression, and MLP were utilized in two papers, e.g., (Emerson et al., 2020b) and (Closser et al., 2022), and consequently stood as the first-ranked performers among other employed regression methods. As well as, Lasso Regression, Gaussian Process Regression, and Fuzzy Logic Approach each appeared in one study (e.g., Emerson et al., 2020b; Sharma et al., 2019). These results indicate a diverse use of regression techniques to fit various multimodal data and modeling requirements.

- Traditional Clustering Models. Models in this category employ unsupervised learning techniques to organize similar samples into groups based on specific metrics. Huang et al. (2019) and Chejara et al. (2023a) utilized the K-Means model to identify collaborative states from different perspectives. K-Means is a metric-based clustering algorithm well-known for its effectiveness in handling data with convex-shaped clusters (linear data).

- Ensemble Models. These models train multiple machine-learning models and then combine their results to make a decision, typically in supervised machine-learning tasks. This combination improves the performance and robustness of models, as well as their overfitting significantly decreases (Sagi & Rokach, 2018). RF, a well-known model in this category, was the most frequently used in the included papers, appearing in 13 studies, which can be exemplified by (Kawamura et al., 2021) and (Som et al., 2020). Gradient Boosted Regression (GBR) was utilized in three studies, such as (Vatral et al., 2023), while XGBoost and Adaptive Boosting models were each used in two studies, which can be exemplified by (Chejara et al., 2020). Additionally, CatBoost Regression was employed in one study (Kawamura et al., 2021).

These results illustrate the wide use of RF in the Model Learning step of AI- enhanced MMLA studies. That is because of its precision, resilience to non- normally distributed data, and ability to handle both continuous and categorical variables, which makes it particularly effective for managing small sample sizes and high- dimensional feature spaces (Huang et al., 2023a).

- Deep Learning-based Models. These models are particularly well-known for their ability to automatically extract features and patterns from raw data and their proficiency in capturing temporal and nonlinear dependencies within the high-dimensional multivariate data (Li et al., 2022). The Convolutional Neural Networks (CNN) were the most frequently used deep learning-based model in the model learning step, appearing in 43 analyses across four studies. Nguyen et al. (2023) classified the sequences of regulatory and physiological activities using CNN to predict collaborative learning success. Another study (Cebral-Loureda & Torres-Huitzil, 2021) employed CNN to classify the emotion and identify the attention-degree levels from images and gaze behavior data. Furthermore, Ivleva et al. (2023) utilized CNN to predict emotion state from speech and facial features. Finally, in the last paper within this class of models (Peng et al., 2019), CNN was involved in classifying the learning status. Additionally, The Long Short-Term Memory Network (LSTM) is another deep learning-based model employed in four included studies. In the paper by JarvelÃ¤ et al. (2023), it was utilized for sequence prediction from regulatory activities in the collaborative learning environment. Moreover, Peng et al. (2019) involved LSTM to classify the comments data, and it was also used to predict the performance and Normalized Learning Gain (NLG) by Olsen et al. (2020). Moreover, Di Mitri et al. (2022) classified the chest compression using this model. In addition to the models mentioned, two papers explored the application of a Deep MLP architecture in this step. The first paper, authored by Som et al. (2020), utilized a 5- layer MLP model to evaluate group collaboration quality based on individual roles and behaviors exhibited by group members. Meanwhile, another study (Ma et al., 2022) tackled impasse detection during the collaborative problem- solving process by employing a Deep MLP classifier. This classifier consists of two feed- forward layers and two dropout layers to mitigate overfitting. Furthermore, in papers by Chan et al. (2023) and Lee et al. (2023), the authors employed sequential neural network architectures, DNN, to achieve remarkable performance in their classification tasks. As well as DL- SARF, a deep learning- student attention recognition framework, was developed to assess the offline classroom, particularly focusing on students' engagement, in paper (Akila et al., 2023).

AI- enhanced approaches in the Analysis phase Focusing on this phase, two studies involved machine learning techniques in their Insight Generation step to interpret the results of their respective models by identifying and leveraging key features. Sabuncuoglu and Sezgin (2023) utilized InterpretML with LIME to explain the classification model behavior and generate interpretable rules. Conversely, Chejara et al. (2023a) employed an RF model to identify essential features that clarify the clustering results concerning collaborative quality scores.

# 5.4. RQ4: what experimental designs and settings are employed in AI-enhanced MMLA studies?

# 5.4.1. What were the types of settings employed in the studies?

We grouped different types of settings in the studies into two discrete categories: Lab. and classroom. A significant proportion of the studies, totaling 34 papers, were conducted in a lab situation (e.g., Li et al., 2023; Reilly & Schneider, 2019). These studies involve controlled environments, simulations, and structured tasks, and focus on particular groups of participants that do not correspond to real contexts, thereby indicating a potential limitation in the ecological validity of these studies. For example, Zhou et al. (2024) described working with 34 postgraduate students at a UK tertiary institution. The students were assigned to groups of four or five, considering diverse first languages, interdisciplinary backgrounds, and mixed- gender composition. Over a 10- week course, each group collaborated for approximately 60 minutes each week to design technological solutions for educational challenges. Another study demonstrates that the participants were randomly assigned to three groups: a control group with no learning analytics system, experimental group 1 (EG1) using unimodal data, and experimental group 2 (EG2) utilizing multimodal data (Lin et al., 2023). These studies exemplify a controlled situation and quasi- experimental design rather than a true experimental design, as they did not involve a random assignment.

The second portion of the studies, comprising 7 papers, is assigned to real settings that resemble the classroom (e.g., JÃ¤rvelÃ¤ et al., 2023; Yusuf et al., 2023) because these studies were embedded into regular learning practices and spanned multiple sessions or weeks, indicating that the essence of their settings aligns with the real classroom environment. As an example, Nguyen et al. (2023) highlighted that data collection was carried out in real science classrooms with weekly lessons involving 94 secondary school students, aged 13 (36 males and 58 females). Similarly, Akre et al. (2023) describe their dataset, which reflects "in the wild" conditions (i.e., real context), consisting of video recordings of people in a virtual learning environment tagged with labels indicating attentiveness, frustration, confusion, and boredom. It also includes log data of their actions on the learning management system and the results of quizzes they attempted.

Yet, The remaining percentage of the studies, including 2 papers, is allocated to those studies that did not specify the types of settings,

![](images/ed8aece19a617f099711623d6455fb023f77b1dadf436772ce5e9157e7b9c129.jpg)  
Fig. 17. Sample size distribution across the AI-enhanced MMLA studies. The blue dots represent individual included papers.

whether controlled situations or real contexts. As a result, Considering our analysis, it can be ascertained that the majority of MMLA research tends to focus on controlled lab settings rather than authentic, real- world contexts.

# 5.4.2. What was the range of sample size used in the studies?

We examined the range of sample sizes employed across 43 studies to determine the extent to which the results of a study are applicable to other samples. As indicated by Fig. 17, the sample sizes of different studies varied significantly, ranging from 4 participants (Peng et al., 2021) to 304 participants (Li et al., 2023). The median sample size was 40 participants (Chango et al., 2021) and the standard deviation of sample sizes across studies was 54.61. The most frequently reported sample sizes ranged from 4 to 30 (e.g., Chng et al., 2020; Chejara et al., 2020). This suggests a common tendency among researchers to recruit this number of participants for their studies. For example, Peng et al. (2019) indicated that fifteen university students, including 7 men and 8 women aged 21 to 27, were recruited for their research focused on aggregating multimodal data, including facial expressions and timeline- anchored comments, to design a tool for instructors that shows how students' status changes along the lecture video timeline. Additionally, Vatral et al. (2023) involved 14 student nurses who used Tobii 3 eye- tracking glasses during their training in a simulated hospital environment. In this simulation, the students conducted evaluations and administered treatments to a manikin patient. In contrast, a few studies used very large sample sizes (e.g., Chettaoui et al., 2023; Emerson et al., 2023), reflecting potentially higher generalizability to different contexts. For instance, Lin et al. (2023) worked with a total of 60 voluntary participants (32 females and 28 males), all of whom were senior high school students, to examine the impact of different learning analytics diagnostic systems on students' learning behaviors, learning performance, and motivation in STEM collaborative learning activities. Another study involved 70 students from two higher education institutions who were exposed to emotional stimulants to investigate how wearable technology, through monitoring physiological data, could potentially support learners' self- regulation (Yun et al., 2020).

# 5.4.3. What kind of ethical implications were considered in the studies?

As shown in Fig. 18, various ethical implications, including informed consent, ethics approval, privacy protection, and algorithmic bias, were examined in different studies. Informed consent was the most frequently reported ethical considerations specified in 20 studies. For example, Sharma et al. (2019) reported that, prior to starting the research, students were asked to sign consent forms informing them about the study's procedures and granting permission to the researchers to use the collected data for research purposes. In this case, Yun et al. (2020) noted that they explained the study's purpose and procedures to the participants, and received both verbal and written consent from them 15 studies (e.g., Chango et al., 2021; Kawamura et al., 2021) pointed out ethical approval, indicating that these studies secured approval from an Institutional Review Board (IRB) or ethics committee to adhere to ethical regulations and standards.

A total of 11 studies mentioned privacy protection, indicating attempts to protect participants' personal data and enhance confidentiality. For instance, Sabuncuoglu and Sezgin (2023) addressed privacy concerns by avoiding the collection of demographic data and ensuring anonymous use. Only teachers have access to student- specific data, and engagement data for policymakers is aggregated to protect personal identifiers. To minimize surveillance concerns, they point out that the platform does not store data and restricts data access to third parties and students. Similarly, Chettaoui et al. (2023) highlighted that they safeguarded participant identification by saving their IDs, test scores, and eye- tracking data.

![](images/7af88c18e60005e0ab213408339f36c8e57fa60b172c0b654f62fea198aede5e.jpg)  
Fig. 18. Reported Ethical Considerations in AI-enhanced MMLA studies, including key issues and frequencies.

Five studies addressed algorithmic bias involving mitigating biases in data analysis algorithms. For example, Lee et al. (2023) balanced their data using SMOTE to prevent an imbalance between distracted and attentive states, ensuring neither state dominated proportion. This provided sufficient data points for training and reduced biases from different data ranges. Likewise, Cukurova et al. (2020) mentioned that to reduce biases, they selected participants with similar levels of knowledge to ensure that differences in knowledge and skills did not impact their collaborative problem- solving (CPS) competence. In contrast, a significant proportion of studies (14) did not draw attention to any ethical considerations, raising concerns about the transparency, fairness, accountability, and ethics of the studies.

# 5.5. RQ5:why is AI adopted in MMLA studies, and what are some of the underlying challenges?

In RQ5, we discuss the primary motivations for using AI in MMLA studies, as evidenced by the reported benefits, and explore the underlying challenges.

# 5.5.1. What are the reported benefits of AI-enhanced MMLA studies?

A total of 11 benefits were identified across 43 analyses in AI- enhanced studies as shown in Table 3, of which the top 5 are reported. Increasing insight into the student learning process is commonly reported in 36 studies. These studies emphasized how MMLA enables educators to better understand how students engage with learning (e.g., Akila et al., 2023; Emerson et al., 2020a). For instance, A study conducted by Israel et al. (2021) explores how combining data from facial expressions and gameplay logs can help understand students' emotional and problem- solving states during computational thinking (CT) game- based learning. And also, Akre et al. (2023) introduced "EngageDat- vL," a novel learning analytics dataset that combines emotional, cognitive, and behavioral data, providing deep insight into student engagement in e- learning settings.

Furthermore, after gaining deeper insight into the learning process, personalized learning was frequently reported in 18 studies. For instance, Lin et al. (2023) demonstrated that using multimodal data an

Table 3 Reported benefits across AI-enhanced MMLA studies.  

<table><tr><td>#</td><td>Benefits</td><td>Number of Studies</td></tr><tr><td>1</td><td>Increasing insight into student learning process</td><td>36</td></tr><tr><td>2</td><td>Personalized learning</td><td>18</td></tr><tr><td>3</td><td>Providing real-time feedback</td><td>16</td></tr><tr><td>4</td><td>Supporting learning experience</td><td>14</td></tr><tr><td>5</td><td>Enhancing the accuracy student behavior prediction</td><td>10</td></tr><tr><td>6</td><td>Positive effect on learning outcome</td><td>6</td></tr><tr><td>7</td><td>Detecting students&#x27; performance</td><td>6</td></tr><tr><td>8</td><td>Feasibility and Practicality</td><td>5</td></tr><tr><td>9</td><td>Automated Estimation of Collaboration</td><td>4</td></tr><tr><td>10</td><td>Development of teaching strategies</td><td>3</td></tr><tr><td>11</td><td>Reduction in the burden for teachers</td><td>2</td></tr></table>

analytics supports adaptive learning by providing personalized guidance through the identification and addressing of student learning challenges in STEM collaborative learning activities. Additionally, Yun et al. (2020) studied data from physiological sensors, specifically Electrodermal Activity (EDA) and Electrocardiogram (ECG), to create a context- aware personal learning support system aimed at supporting self- regulation in higher education.

Additionally, providing real- time feedback, as mentioned in 16 studies, plays a key role in fostering 21st- century skills (e.g., collaboration - Huang et al., 2019) and helps learners maintain longer attention spans and reduce distractions (Lee et al., 2023). This is evidenced by a realtime feedback system for cardiopulmonary resuscitation (CPR) training (Di Mitri et al., 2022), which demonstrated that real- time feedback has a short- term positive impact on CPR performance. Furthermore, Peng et al. (2019) indicate that collecting multimodal data, such as facial expressions and comment text from students watching teaching videos online, allows instructors to offer timely feedback on student engagement and comprehension, which enhances online education.

Fourteen studies focused on supporting the learning experience, highlighting that MMLA enhances learning by promptly identifying students' difficulties and offering customized interventions (e.g., Ma et al., 2022; Emerson et al., 2023). For example, Ma et al. (2022) demonstrated that using multimodal learning analytics supports learning experiences among middle school learners by detecting impasses during collaborative problem- solving. And also, another example by Vatral et al. (2023) shows that predictive models for self- confidence, based on multimodal data from eye gaze and speech patterns, could be part of a larger assessment framework. This framework would provide instructors with additional tools to support and improve student learning and patient outcomes, thereby enhancing the learning experience in simulation- based nurse training programs.

Ten studies underlined the importance of increasing the accuracy of student behavior prediction by leveraging multiple data sources, such as eye- gaze features and academic data, aiding instructors in identifying students' indicators of learning gains more accurately (e.g., Emerson et al., 2020a; Olsen et al., 2020), for example, Chettaoui et al. (2023) indicated that combining eye- gaze tracking with other learning and behavioral data can provide an accurate prediction of learning outcomes. Another example, as indicated by Huang et al. (2019), is that combining two different modalities of data, such as eye- tracking and physiological activity data, could lead to better predictions of the quality of collaboration.

# 5.5.2. What are the reported challenges of AI-enhanced MMLA studies?

Table 4 outlines 15 challenges that were raised in the included studies. One of the prominent challenges identified is the issue of small sample sizes, which has been extensively discussed across 13 studies (e.g., Ouyang et al., 2023; Chng et al., 2020). Additionally, limited generalizability, although ranked as the third challenge in the table appeared in 11 studies, is closely related to sample size. Thus, We are addressing both challenges together due to their interconnected nature. For instance, Ma et al. (2022) reported that because the data was obtained from a limited group of 46 middle school students, the predictive features identified would not generalize to different groups of students in various educational contexts (i.e., online learning environments). Furthermore, Moon et al. (2022) pointed out that the dataset they worked with was too limited to create a widely applicable predictive model. Therefore, they emphasized the importance of expanding the dataset to validate and improve the prediction model.

Model selection and overfitting challenges, ranked second, were reported in 11 studies. Choosing the right method for analysis is particularly challenging. In this case, Closser et al. (2022) highlighted that to determine if actions, verbal communication, or gestures are related to student performance, simple regression is suitable for analyzing such data. However, understanding the complexity of student speech and language requires advanced techniques. Additionally, Reilly and Schneider (2019) reported overfitting in their study's models, which they attributed to the lack of regularization of model complexity and the small sample size. They also suggested that it might be more appropriate to use a regression model instead of classifying the scores, considering the range of - 2 to 2 used for evaluating the quality of collaboration.

Eight studies reported the issue of limited practical implementation as one of the challenges of MMLA. Lin et al. (2023) mentioned that the limitation of practical implementation stems from the short duration of instructional learning activities impeding learning and assimilation of knowledge. Additionally, Zhou et al. (2024) acknowledged that while modern computer vision techniques are capable of automatically identifying students' behaviors, there are challenges in applying these AI- enabled technologies in real- world contexts, resulting in the need for manual work in some parts of the process.

Data quality and emotion/behavior detection challenges were identified in six studies. For example, Data quality decreases due to occlusion and illumination and the failure of facial detection when learners are not directly facing the camera (Ma et al., 2022). Also, identifying emotions was another challenge, as Geral- Loureda and Torres- Huitzil (2021) noted the difficulty in detecting negative emotions during task performance. As such, It is very challenging to capture emotions due to variations occurring in video datasets (Akre et al., 2023).

# 6. Summary and future research directions

In this section, we analyzed the key findings from the results, highlighting the patterns and insights observed across the included papers. We then explored the challenges and opportunities these findings present for the field. This analyses provides a deep reflection on the overall impact of AI in MMLA studies and the directions for future research.

# 6.1. Evolution, publication quality, distribution, collaboration, and disciplinary composition of lead researchers in AI-Enhanced MMLA Studies

Findings from RQ1 provide novel insights into the state of AI- based MMLA research up to 2024, revealing patterns previously unexplored in the field. Initially, the publication of AI- enhanced MMLA studies increased, then showed a temporary decline during the COVID- 19 pandemic, during a period when online learning environments and their associated data streams significantly expanded, followed by a significant recovery in 2023. This recovery, coming after a period of increased online learning adoption, underscores both the growing recognition of AI's importance in MMLA and the research community's response to the expanding need for analyzing diverse educational data streams. This trend suggests increasing efforts to explore how AI can enhance MMLA capabilities to support traditional and online educational experiences.

Our analysis of publication quality reveals that AI- enhanced MMLA studies predominantly appear in high- quality academic venues. Specifically, the majority of journal papers (20 studies) were published in Q1- ranked journals, most notably in leading educational technology venues, such as Education and Information Technologies and the British

Table 4 Reported challenges across AI-enhanced MMLA studies.  

<table><tr><td>#</td><td>Challenges</td><td>Number of Studies</td></tr><tr><td>1</td><td>Sample size</td><td>13</td></tr><tr><td>2</td><td>Model selection and over-fitting challenges</td><td>11</td></tr><tr><td>3</td><td>Limited generalizability</td><td>10</td></tr><tr><td>4</td><td>Limited practical implementation</td><td>8</td></tr><tr><td>5</td><td>Data quality and Emotion/Behavior Detection challenges</td><td>6</td></tr><tr><td>6</td><td>Technical issues</td><td>4</td></tr><tr><td>7</td><td>Unreliable self-reported metrics</td><td>4</td></tr><tr><td>8</td><td>Feature selection /extraction and redundancy issues</td><td>4</td></tr><tr><td>9</td><td>Complexity in data fusion</td><td>3</td></tr><tr><td>10</td><td>Data Accessibility, Privacy, and Context Challenges</td><td>3</td></tr><tr><td>11</td><td>Annotation, Representation &amp;amp; Data depth challenges</td><td>3</td></tr><tr><td>12</td><td>Challenge in Modelling temporal process</td><td>1</td></tr><tr><td>13</td><td>AI dataset balancing issues</td><td>1</td></tr><tr><td>14</td><td>Costly Measurement Hardware</td><td>1</td></tr></table>

Journal of Educational Technology. This concentration validates the methodological rigor and innovative contributions that characterize the field, reflecting its increasing recognition among established academic communities. Conference papers showed broader distribution, with 10 studies in A- ranked conferences (predominantly at the LAK conference), eight in B- ranked venues, and smaller numbers in C- ranked or non- ranked conferences. This distribution indicates a more dynamic and heterogeneous landscape in the early stages of research spreading. Top- ranked conferences like LAK serve as venues for cutting- edge innovations, while presented studies in specialized venues underscore the multifaceted nature of AI- enhanced MMLA research, where diverse aspects of this emerging field are explored through various methodological approaches and disciplinary lenses. The presence of papers in both top- ranked venues and more specialized conferences indicates an effective structure for knowledge spreading in this field, balancing strict academic standards with opportunities for presenting innovative work at various stages of development. Moving forward, establishing dedicated special issues on AI in MMLA within top- ranked venues will be crucial for unifying methodological approaches and elevating quality standards across the field. As the field continues to mature, maintaining this balance will be essential for transforming how AI techniques are integrated into the MMLA process, finally advancing educational analytics to new boundaries.

Geographically, the research is spread across 24 countries on six continents, with Europe leading in contributions, particularly from Germany, Spain, the UK, and Estonia, revealing the high potential of Europe in advancing the field. This finding significantly extends our understanding compared to previous reviews like Noroozi et al. (2020), which showed concentrated MMLA efforts in a few countries, such as the US and Australia. Moreover, our result demonstrates a substantial expansion into diverse nations, including Egypt, Tunisia, and Nigeria. This result marks a significant shift from (Tahiru, 2021), which reported no AI in education publications from developing countries between 2010 and 2019, highlighting the rapid advancements now emerging in these countries. The presence of developing countries in this field marks a shift towards a more inclusive and globally representative model of AI integration in educational technologies.

The analysis of collaboration patterns among countries reveals significant insights into the global landscape of AI- enhanced MMLA research. Our findings show that the USA continues to lead in international and national publications, consistent with earlier studies by Deng and Zhao (2022) and Pei et al. (2023). Spain has emerged as another key player, indicating a shift in the global research dynamics. This leadership pattern suggests an evolution in how advanced technological infrastructure influences the development and adoption of AI in educational settings (Deng & Zhao, 2022). However, our study also uncovers that some countries, such as India, Turkey, Mexico, Taiwan, and Australia, have only contributed to spreading the field at the national and local geographic. This finding suggests a focused but limited engagement with AI in MMLA within these countries, highlighting their potential to ac celerate the progress of the AI- enhanced MMLA field across boundaries. From a pedagogical perspective, these collaboration patterns have important implications for the global development of AI- enhanced learning tools. The leadership of technologically advanced countries may accelerate the creation of sophisticated MMLA systems, while the localized focus of other nations could lead to more culturally adapted applications. This diversity in approach highlights the need for a more inclusive model of AI integration in educational technology that can bridge global innovations with local educational needs and contexts.

Analysis of the disciplinary composition of lead researchers reveals that researchers from Education are leaders in AI- enhanced MMLA studies, accounting for  $46.5\%$  of the included papers. This aligns with the trend identified by Crompton and Burke (2013), who reported Education departments as the most represented discipline  $(28\%)$  in AI in higher education research. However, our notably higher percentage suggests an especially strong resonance of MMLA with educational researchers. This represents a notable shift from earlier findings in paper (Zawacki- Richter et al., 2019), where only  $8.9\%$  of AI in higher education papers were led by authors with Education backgrounds. Interestingly, authors of (Bond et al., 2024) found that STEM- affiliated researchers still dominate in AI in higher education evidence synthesis  $(56.1\%)$  with Education- affiliated authors representing  $25.8\%$  making our findings particularly distinctive for the MMLA subfield. The strong presence of Education researchers in our analysis indicates that MMLA may be emerging as a domain where pedagogical perspectives are increasingly driving technological innovation, rather than technology leading pedagogical applications. Computer Science & IT  $(23.3\%)$  and Engineering  $(18.6\%)$  departments contribute substantial technical expertise, forming essential interdisciplinary bridges that enable the sophisticated data analysis required for effective multimodal learning analytics. Notably, 6 papers in our analysis originate from non- academic entities such as independent research organizations and industry research units, reflecting the practical applications of AI- enhanced MMLA. The smaller representation from Humanities and Social Sciences  $(4.7\%)$ , Interdisciplinary Research Institutes  $(4.7\%)$ , and Informatics  $(2.3\%)$  indicates opportunities for broader disciplinary engagement. This distribution illustrates that while AI- enhanced MMLA is more balanced in its disciplinary composition than general AI in education research, with stronger educational leadership, it could benefit from additional perspectives that might enrich the understanding of human learning behaviors, social contexts, and information processing. The interdisciplinary nature of the existing contributions highlights how AI- enhanced MMLA sits at the intersection of pedagogical insight and technical innovation, requiring collaboration across disciplinary boundaries to achieve its full potential in understanding and supporting complex learning processes.

A call for increased collaboration. In terms of future research, the findings from RQ1 underscore a need for increased collaboration across geographical and institutional boundaries to address the imbalances in geographic representation. The analysis revealed that while research efforts

are concentrated in regions like Europe and North America, significant contributions are emerging from diverse locations such as China, Japan, and various countries in Africa and South America. However, the distribution of studies shows that many regions remain underrepresented, indicating a potential for broader collaborative efforts. Expanding collaboration between countries with advanced infrastructure and those with expertise in both AI and LA fields is essential to accelerate and foster progress in AI- enhanced MMLA studies. Advanced countries (like the USA and Spain), with their key roles and robust resources, must strengthen collaboration with other countries to fully leverage global expertise and capabilities. Each country has a set of unique strengths, and integrating these with the potential of countries yet to contribute can lead to significant breakthroughs in the field. This collaboration could result in more sophisticated, culturally sensitive MMLA systems that combine global technological advancements with local educational expertise, potentially revolutionizing personalized learning on a global scale. In the context of Vygotsky's Sociocultural Theory, this call for increased collaboration can be seen as an effort to create a global Zone of Proximal Development (ZPD) in AI- based MMLA research (Gauvain, 2020). By fostering partnerships between more experienced researchers and emerging contributors, we can scaffold the development of expertise in this field across diverse cultural contexts, leading to more effective and culturally appropriate AI- based learning tools.

# 6.2. Application contexts, educational levels, stakeholders and modalities in AI-Enhanced MMLA Studies

Findings from RQ2 emphasize a predominant focus on tertiary education, specifically undergraduate students similar to the results reported by (Noroozi et al., 2020; Prinsloo et al., 2023), with a substantial number of studies targeting learners, researchers, instructors, and technology developers. This result not only underscores the critical importance of enhancing learning processes and outcomes in universities and colleges but also highlights the advanced infrastructure now available for collecting multimodal data at the higher education level (Samuelsen et al., 2019). Furthermore, this finding illustrates the research community's growing confidence in the AI- enhanced MMLA's ability to transform educational practices, specifically in the post- secondary institutions.

Though higher education leads the field, secondary education shows meaningful representation, with studies addressing the needs of learners, researchers, and instructors, indicating growing attention to understanding and improving learning outcomes at this level. The lack of studies with technology developers as the target audience at the secondary level suggests a gap in research and innovation, reflecting a missed opportunity to engage technology developers in creating or adapting tools specifically designed for the unique needs of secondary education. This gap highlights the need for a more comprehensive application of Design- Based Research approaches (Anderson & Shattuck, 2012) in AI- enhanced MMLA, ensuring that technological developments are rooted in educational theory and practice.

Primary and early childhood education was less frequently addressed, with a few studies focusing on improving engagement and learning outcomes for younger students, underscoring the challenges and difficulty in identifying and capturing various dimensions of learning at these levels (e.g., Parental consent, strict privacy and data protection requirements for minors, and the logistical difficulties of collecting multimodal data in dynamic early learning environments), which is consistent with the findings in (Crescenzi- Lanna, 2020; Noroozi et al., 2020). This gap presents an opportunity to enhance our understanding of early childhood learning processes through data- driven insights. AI- enhanced MMLA could offer unique capabilities for observing and analyzing children's learning behaviors, potentially validating or refining aspects of established developmental theories such as Piaget's Cognitive Development Theory (McLelland, 2024; Cerovac & Keane, 2024). Furthermore, five studies did not specify the educational levels in which their experiments were conducted, raising concerns about the generalizability of their findings across different educational contexts. By not accounting for the distinct pedagogical challenges, sensitivities, and requirements of various educational levels, these studies may risk sacrificing reliability and failing to capture the full potential of AI in enhancing learning outcomes. This limitation underscores the importance of contextualizing AI- enhanced MMLA within educational theories, such as Situated Learning Theory (Lave, 1991), to ensure that findings are theory- based and practically applicable.

In terms of the modalities utilized, recent technological advancements, the growth of online learning environments, and the adoption of gameplay- based learning processes have significantly enhanced the opportunity for collecting physical and digital data through tools used by learners (Samuelsen et al., 2019; Mu et al., 2020). Our results confirm this trend, indicating that the most frequently employed modalities are physical activities (such as facial expressions, speech, eye movements, and motion) recorded by embedded tools like cameras, eye trackers, and microphones, consistent with the finding by Mu et al. (2020), while digital traces (including system logs and textual data from comments, posts, and online chats) captured by digital learning platforms are the second most frequently utilized modalities. This focus aligns with the findings of (Giannakos & Cukurova, 2023), who identified three dominant theories in MMLA research, including Embodied Cognition (EC), Cognitive Load Theory (CLT), and Control- Value Theory of Achievement Emotions (CV- TAE). Additionally, our analysis reveals a growing use of physiological signals and psychometric data, such as EDA, Heart rate, and questionaries, which provides insights into learners' emotional, stress responses, and cognitive load. In contrast, modalities from environmental space, such as lighting conditions, temperature, and physical space configuration, were less frequently utilized (Chan et al., 2023; Kawamura et al., 2021). Despite its potential to offer valuable insights into how the learning environment impacts learner engagement and performance, environmental data remains an underexplored area in the current studies. The results also indicate that most of the included papers integrated modalities from only two information sources, revealing a prevalent focus on combining a limited range of data types. However, only eight out of 43 studies employed modalities from more than two information spaces, underscoring the challenges of handling large and complex data. This trend suggests that while dual- modality approaches are common, there may be opportunities to enhance the richness of analyses by incorporating additional sources of information.

Opportunity for better utilization of data across modalities in information spaces. The modalities collectively provide a rich, multimodal understanding of learner interactions and behaviors, enabling deep insights into the learning process (Ochoa et al., 2017). Our analysis reveals that significant studies have integrated modalities from physical and digital spaces, leveraging recent advancements in online learning environments and gameplay- based learning. This focus, while valuable, overlooks the rich possibilities offered by other information spaces, such as environmental, physiological, and psychometric data. Despite the clear potential of these underutilized data sources to provide deeper insights into learners' emotional states, stress responses, and cognitive load, they remain underexplored in the current research landscape. For instance, environmental data, such as lighting conditions, temperature, and physical space configuration, could offer valuable insights into how the learning environment impacts learner engagement and performance. The collection of such complex modalities could become more feasible through AI- enhanced data collection approaches. Hence, there is a significant opportunity to explore and integrate these underutilized data sources by leveraging tools and techniques capable of capturing and analyzing such diverse information. This opportunity extends the work of Giannakos and Cukurova (2023), who noted MMLA's potential as a new information source for understanding and supporting learning process. While they focused on embodied cognition, cognitive load, and emotions, our review suggests that including environmental factors could

further enhance MMLA's ability to capture the full spectrum of the learning experience.

further enhance MMLA's ability to capture the full spectrum of the learning experience.Nevertheless, increasing the volume and heterogeneity of the modalities requires more complex and time- intensive storage and analysis, which, without the appropriate tools and insights, makes traditional analytical methods unusable (Sharma & Giannakos, 2020). Recently, the advancements in AI, particularly in areas such as machine vision, machine learning, and explainable AI, have made it feasible to manage and analyze such complex data effectively (Slupczynski & Klamma, 2021; Blikstein & Worsley, 2016). This need for advanced analytical methods reflects the observations of Giannakos and Cukurova (2023), who highlighted MMLA's ability to index cognitive load in an unobtrusive and temporal manner. Our analysis extends this observation, suggesting that AI- driven techniques can seamlessly fetch, condense, and integrate diverse modalities, uncovering intricate patterns and relationships that might otherwise remain hidden. By leveraging such technologies, researchers can push the boundaries of MMLA, moving beyond dual- modality approaches to explorer richer, more diverse information spaces. This exploration can uncover deeper insights into the complex dynamics of learning, potentially leading to knowledge breakthroughs in our understanding of cognitive processes, emotional changes in learning, and the impact of environmental factors on educational outcomes. For example, AI- driven analysis of combined physiological, environmental, and digital interaction data could reveal new patterns in how mental effort changes under various conditions, leading to more effective strategies for managing information presentation in digital learning environments. This approach not only advances our theoretical understanding of the AI application in MMLA but also has significant practical implications for the design and implementation of adaptive learning technologies. It showcases the potential of AI to revolutionize MMLA by enabling the integration and analysis of diverse data sources, thereby enhancing our ability to gain deep insights into learning processes.

A call for broader inclusion and multi- level studies. Our results reveal a significant imbalance in AI- enhanced MMLA research across educational levels, with a strong focus on higher education and insufficient attention to primary, secondary, and early childhood settings. This concentration creates a critical gap in understanding how AI techniques can effectively support MMLA across different educational contexts. The unique characteristics of each educational level from data collection challenges and privacy requirements to pedagogical intervention methods suggest that AI approaches in MMLA need careful adaptation rather than simple replication across levels. This finding highlights a critical need for multilevel studies that employ adaptive strategies to integrate analyses across multiple educational contexts. From the theoretical perspective, such studies should leverage adaptive AI methodologies to ensure that research is reliable and context- specific. By tailoring AI approaches to the unique characteristics of each educational level, researchers can achieve more accurate findings, potentially extending current learning theories to account for technological interventions across different ages. At the same time, they should aim for generalizability by identifying cross- level patterns and trends that can inform broader educational practices and policies and might lead to new theoretical frameworks that span the entire levels of education. Pedagogically, implementing adaptive AI within a multilevel framework allows us to generate deeper and more wide- ranging insights than previously possible. Our call for such comprehensive research aligns with the need for methodological diversity highlighted in Martin et al. (2020), emphasizing the importance of gaining deeper insights into the technology use in education. This approach offers valuable contributions to understanding learning processes from early childhood to higher education, potentially transforming how we conceptualize and support learning across different ages.

Compared to the current literature (e.g. Moon et al., 2022; Emerson et al., 2023), this integrated approach will not only fill the existing gaps but also enhance the effectiveness of AI in supporting diverse educational needs. It represents a significant advancement in the use of technology for educational purposes, offering new ways to personalize learning experiences and inform educational policies based on comprehensive, data- driven insights.

# 6.3.Use of AI in MMLA studies

To address RQ3, we developed a conceptualized framework for systematically analyzing AI integration across the MMLA process. This comprehensive framework, spanning from data collection to intervention phases, not only represents a theoretical advancement in understanding the role of AI in the MMLA field but also serves as a pedagogical bridge connecting technical implementations to educational outcomes. It enables a structured mapping of AI techniques into specific MMLA phases, providing insights that directly inform teaching and learning practices. Our framework- based analysis reveal that AI techniques were utilized in all phases, except for the Data Storage and Intervention, but were limited to specific sub- components of the MMLA process. This uneven integration of AI across the analytical pipeline highlights both the potential and current limitations of AI in MMLA.

Notably, AI techniques were predominantly applied in the modeling phase of MMLA studies, playing a critical role in extracting valuable information from multimodal data and significantly aiding decision- making processes. In this phase, the most frequently employed techniques are traditional and ensemble classification methods, particularly RF, SVM, and Decision Trees, within the model learning sub- component. However, despite the potential of deep learning models to uncover complex patterns in large- scale data, they have been employed in only a few studies. Similarly, unsupervised techniques, such as K- means clustering, have seen limited application, with only two papers utilizing them. These results reveal that most studies employed traditional supervised techniques within their model learning sub- components, which inherently requires advanced annotation phases across all these studies. Despite the critical role of accurate labeling, our analysis reveals that only one paper attempted to incorporate automated labeling methods, while the majority relied on manual processes for data annotation. This labeling process not only limits scalability but also highlights a significant area for innovation in future research. Furthermore, only two papers attempted to generate interpretations of the model learning results, underscoring a lack of focus on explainability and transparency in current MMLA studies. For educators, this gap means fewer opportunities to make data- driven decisions, while learners miss personalized insights that could enhance their educational experience. These limitations underscore a serious gap between the theoretical potential of MMLA, described by Giannakos and Cukurova (2023), and its practical application. While MMLA is highlighted as a tool for uncovering hidden learning processes and providing deeper insights through advanced AI techniques, the actual implementation falls short of this promise. The ever- reliance on manual data labeling and the lack of model interpretation represent missed opportunities to leverage the full capabilities of AI.

On the other hand, the complexity of managing heterogeneous modalities (i.e., images, text, audio, and video types) collected by different tools and technologies placed the pre- processing step as the second most critical phase for AI application in MMLA. This phase of included studies involves sub- components such as data anonymization, cleaning, transformation, and augmentation, with AI techniques like image processing, speech recognition, and resampling (e.g., SMOTE) to transfer data into a protected, accurate, and consistent format, as well as address data imbalances and improve data quality. The transformation sub- component is crucial for extracting valuable and consistent features from diverse modalities. Given its complex nature and the challenge of uncovering latent features, it requires a substantial shift from human to AI capabilities. This necessity is supported by the fact that more than half of the included studies have leveraged AI techniques in tasks like facial expression, speech recognition, and spatial orientation, in this subcomponent, highlighting the pivotal role of AI in achieving accurate and

comprehensive data transformation. AI techniques were also extensively utilized across other phases of the MMLA process, including Fusion and Acquisition. The fusion phase, which involves integrating features from multiple modalities to create a unified representation through feature engineering, standardization, and feature integration sub- components, is essential for enhancing the richness and comprehensiveness of the dataset. Notably, among these sub- components, AI techniques were only applied in the feature engineering sub- component of 15 studies. PCA, RF, and HMM were the most frequently used methods, demonstrating their effectiveness in dimension reduction and feature extraction. Similarly, the data collection phase, encompassing source identification and acquisition, is critical for ensuring the quality and relevance of the data used in MMLA. In this phase, AI applications were leveraged to automate data collection through advanced tools and sensors capable of handling diverse modalities, from text to multimedia. Despite its importance, only a few studies have focused on utilizing AI technologies, with their application limited to specific tasks such as face recognition, facial expression analysis, speech recognition, gaze behavior tracking, and spatial orientation identification. These targeted AI- driven approaches aim to enhance the data acquisition process and reduce redundancy by focusing on the collection of specific relevant features.

Beyond these technical observations, our systematic mapping reveals a fundamental challenge, AI implementation in MMLA is weakest precisely in the components that require deep integration with learning theories. The components such as source identification, labeling, insight generation, and intervention that are most deeply rooted in the learning theories are the exact areas where AI implementation remains most limited. Furthermore, the limited application of unsupervised learning approaches represents a significant missed opportunity within MMLA research. While AI techniques should be guided by learning theories across most MMLA phases, including the selection and interpretation of analytical methods, unsupervised learning algorithms offer a unique complementary approach. Unlike supervised methods that depend heavily on theory- derived labels for training, unsupervised techniques can explore patterns in multimodal data without predefined categories, potentially uncovering novel relationships that current theoretical frameworks might not expect. These AI- driven discoveries can reveal emergent patterns in educational data that extend or refine our understanding of learning processes while suggesting innovative pedagogical interventions. This capacity for AI to actively contribute to educational theory development, rather than just implementing existing theoretical frameworks, represents an important yet largely unexplored dimension of AI- enhanced MMLA research.

This result suggests that advancing AI in MMLA requires not just technical innovation in algorithms and models, but a more intentional dialogue between educational theorists and AI developers. While learning theories should provide the foundation and guiding framework for AI development in educational contexts, AI discoveries can also inform and enrich these theories. This mutual relationship ensures that AI supports educational aims rather than diverting attention to technological aspects, while also allowing AI insights to contribute to theoretical advancement. Future MMLA research should prioritize developing AI approaches that can effectively operationalize existing learning constructs and identify patterns that might extend theoretical understanding. This integration represents not just a technical challenge but an opportunity to strengthen the pedagogical impact of AI- enhanced MMLA systems while advancing our understanding of learning processes.

Opportunity for expanding AI applications across MMLA phases. Our review highlights a significant opportunity to enhance the MMLA process by expanding AI applications across underutilized sub- components, including source identification, data warehousing, data management, synchronization, metadata generation, feature integration, EDA, statistical analysis, and visualization. Leveraging AI technologies in these areas could lead to more efficient and robust data handling, improved accuracy in data integration, and deeper insights through advanced an alytical techniques (Noroozi et al., 2019). For instance, by leveraging AI- driven source identification, researchers can achieve more accurate and efficient aggregation of relevant data, minimizing manual effort and reducing the risk of missing critical information. Advanced AI models for data management can analyze patterns, relevance, and quality of sources in real time, ensuring that only the most pertinent and high- quality data are incorporated. AI- enhanced EDA approach, could significantly improve the understanding of complex multimodal data by automatically uncovering latent patterns, correlations, and anomalies that traditional methods may overlook. Also, integrating AI into visualization techniques could produce more dynamic and interactive data representation. These advanced visualizations could reduce extraneous cognitive load, facilitating better understanding and decision- making for learners and educators. By addressing these gaps, researchers can significantly improve the effectiveness and impact of MMLA studies, ultimately leading to more informed and actionable insights for educational stakeholders.

From a pedagogical perspective, these AI- enhanced advancements offer transformative potential for teaching and learning practices. For instance, AI- enhanced source identification and data management could enable educators to more efficiently tailor learning experiences to individual learner needs, supporting personalized learning approaches. AI- enhanced EDA could help teachers identify learning patterns and efforts, facilitating real- time interventions. Moreover, AI- enhanced visualizations could make complex learning multimodal data more accessible to both educators and students, fostering decision- making and self- regulated learning. These advancements not only push the boundaries of current MMLA research but also offer promising roads for revolutionizing classroom practices and assessment methods. By bridging the gap between advanced analytics and practical pedagogy, AI- enhanced MMLA has the potential to significantly improve learning outcomes and educational experiences across diverse learning environments.

On the other hand, by employing AI technologies across most phases of the MMLA process, researchers can significantly enhance their ability to develop more automated and adaptive learning environments. The integration of cutting- edge AI technologies offers a transformative opportunity to streamline and automate each phase of the MMLA process, from data acquisition to analysis. For instance, in the data acquisition phase, generative AI can create synthetic data to augment existing datasets, enhancing their diversity and robustness (Eigenschink et al., 2023). During the pre- processing and fusion phases, Generative AI can assist in data cleaning and transformation, producing high- quality, standardized data ready for analysis. Generative AI is particularly well- suited for metadata generation, improving the richness and context of data (Asthana et al., 2023). Furthermore, in the modeling phase, generative AI techniques can aid in developing more sophisticated models by simulating complex learning scenarios and generating predictive insights. For analysis, generative AI can also assist in visualizing data patterns and generating comprehensive reports that translate complex analytics into actionable insights (Narayanan, 2024).

A call for advanced AI techniques in MMLA studies. Recent advances in AI present a transformative opportunity for researchers in the MMLA field to enhance their methodologies and outcomes. While AI has been integrated into some phases of the MMLA process, the lack of cutting- edge AI techniques, such as deep learning, federated learning, generative models, transformers, Self- Supervised Learning, Few- Shot Learning, Active Learning, Meta- Learning, and reinforcement learning, highlights a notable gap and a substantial opportunity for innovation. These advanced techniques promise to address current limitations, refine analytical capabilities, and drive significant progress in the field, ultimately leading to more insightful and impactful educational technologies.

From the theoretical perspective, they offer new ways to conceptualize and model learning processes in complex, multimodal learning environments. For example, integrating Explainable AI (XAI) bridges the gap between advanced technology and human understanding by en

hancing the interpretability of complex models, consequently fostering transparency and trust among educators and researchers (Khosravi et al., 2022; Tiukhova et al., 2024). This approach not only represents an advancement over 'black box' models but also aligns with learning theories such as constructivism and cognitive load theory, which emphasize the importance of clear, understandable processes for effective knowledge construction. For instance, when an XAI system explains to an educator why it identified a learner as struggling with specific concepts, it provides actionable insights that both educator and learner can incorporate into their mental models (Hou et al., 2024), directly supporting constructivist learning approaches while reducing irrelevant cognitive load. The Few- Shot Learning technique offers a valuable opportunity to enhance data labeling in MMLA by enabling models to learn from only a few annotated examples (Song et al., 2023; Carpenter et al., 2024). This addresses a critical challenge in educational data mining and potentially changes how we accomplish data collection and annotation in diverse learning contexts. Also, By enabling decentralized model training, federated learning allows data integration from multiple sources while maintaining privacy and security (Tan et al., 2022; van Haastrecht et al., 2024). This technique not only enhances data diversity but also facilitates collaborative learning among institutions, enabling a community- oriented approach to education. Reinforcement learning offers the potential to develop adaptive and personalized learning environments by continuously optimizing strategies based on real- time feedback (Deeva et al., 2021; Diaz & Nussbaum, 2024), aligning closely with behaviorist learning theories. It enables dynamic, real- time adaptations to individual learner needs, potentially transforming how we implement adaptive educational systems. Generative models can augment data sets and generate synthetic examples, enhancing the robustness of training data and enabling more sophisticated simulations (van Breugel et al., 2024; Mozafari et al., 2024). Additionally, Generative Pre- trained Transformers (GPT) offer a novel approach to personalized learning in MMLA by generating context- aware feedback based on multimodal data analysis (Hou et al., 2024; Yan et al., 2023). While AI- driven feedback traditionally supports adaptive learning, GPT can further facilitate constructivist learning by encouraging self- explanation, reflection, and iterative knowledge construction. By dynamically tailoring responses to individual learners' needs, GPT has the potential to promote deeper engagement with learning materials, aligning with constructivist principles that emphasize active meaning- making and personalized learning pathways. Moreover, by dynamically tailoring feedback to individual needs, GPT can reduce extraneous cognitive load while fostering deeper engagement with multimodal learning materials, complementing cognitive load theory. In pedagogical practice, this suggests that GPT- based feedback mechanisms could be integrated into formative assessment, self- regulated learning environments, and intelligent tutoring systems to provide real- time, personalized support that aligns with modern learning theories (Pozdniakov et al., 2025). Therefore, the MMLA research community is called upon to utilize these advancements to push the boundaries of what is possible in MMLA and improve learning outcomes on a broader scale.

# 6.4. Experimental settings and ethical considerations in AI-enhanced MMLA studies

The findings from RQ4 reveal a diverse experimental designs and settings employed in AI- enhanced MMLA studies. Sample sizes vary significantly, reflecting the heterogeneity in study designs, from small- scale experiments with tens of participants to large- scale studies involving hundreds of learners. The experimental settings also differ widely, including controlled laboratory environments, real- world classrooms, and online learning platforms. While this diversity demonstrates the adaptability of AI across educational contexts, our analysis identifies a critical gap of the predominance of small- scale and controlled studies that may not fully capture the complexities of authentic learning environments. This limitation advances the discussion on ecological validity in MMLA research, emphasizing the need for real- world applications to ensure the generalizability and practical relevance of findings.

Ethical considerations are also a critical component of these studies, with many reporting informed consent and ethics approval. However, there is a need for greater emphasis on privacy protection and addressing algorithmic bias to ensure ethical integrity. This is consistent with other studies and reviews (e.g., Prinsloo et al., 2023; Alwahaby & Cukurova, 2024), highlight that sensor data, such as eye- tracking and facial recognition, can uncover personal feelings and health information, which are susceptible to misuse outside of learning environments, thereby reducing the likelihood of privacy protection. Hence, it is essential to carefully consider the context, the level of intrusiveness of the tools, and the type of data being collected in MMLA studies (Mangaroska et al., 2021). This careful consideration that helps improve the understanding, interpretation, and validity of the collected data was often inadequately addressed in current research studies.

Furthermore, Alwahaby and Cukurova (2024) argue that traditional informed consent forms fail to adequately explain how sensor data is collected and how algorithms work. This shortage raises significant ethical concerns in MMLA, such as anxiety, discomfort, and even simulator sickness among participants (OECD, 2023). To address this, Alwahaby and Cukurova (2024) suggest enhancing participant awareness through visuals, videos, and pre- sessions to demonstrate how MMLA optimizes learning experiences. On the other hand, due to the lack of large- scale datasets in MMLA, the machine learning models currently in use may develop algorithmic biases (Yan et al., 2022). Furthermore, while AI tools are widely used in MMLA, they often fail to identify essential differences in disabled individuals, such as atypical facial expressions, motion, speech patterns, or cognitive variations, which increases these biases (Guo et al., 2020). Another area susceptible to bias is the labeling process, where subjective human judgment can skew results. Although replicating subjective labels with objective ones where feasible can help, certain concepts, like 21st- century skills and emotions, inherently require subjective assessment (Baker & Hawn, 2022). Additionally, the manual annotation process is both time- consuming and labor- intensive, often resulting in small, biased datasets (D'mello & Kory, 2015). Thus, to advance AI- enhanced MMLA, the MMLA community must address a range of complex challenges, some of which are emphasized in the study.

A call for large- scale in- the- field studies. The results indicate that most of the included studies were conducted in inauthentic situations (i.e., labs) on a small scale, failing to capture the complexities of real classrooms (e.g., dynamic interactions between participants). The prevalence of lab- based studies potentially undermines our understanding of how AI techniques can enhance MMLA in authentic learning environments. This gap is consistent with a study review by Prinsloo et al. (2023), underlining that it remains unclear how MMLA can be applied, scaled, and replicated in actual classroom environments. Also, (Yan et al., 2022) acknowledge that approximately 71 percent of predictive analytics studies in MMLA employed small sample sizes (i.e., fewer than 50 participants), jeopardizing their ecological validity. This trend not only limits the generalizability of findings but also constrains our ability to develop AI- enhanced MMLA systems. While Martinez- Maldonado et al. (2023) emphasized that MMLA can be employed in real- world situations, they also highlighted a set of practical and logistical challenges, such as technology deployment and sustainability, that need to be addressed.

Thus, due to the lack of MMLA deployment in authentic contexts, it is required for future researchers to bridge the gap between experimental studies and real learning environments. This could involve collaborative partnerships between researchers, educators, and technology developers to facilitate large- scale implementations, and the development of scalable, user- friendly MMLA tools that can be easily integrated into existing educational technologies.

A call for increased ethical considerations. As AI- enhanced MMLA continues to evolve, even though the scholars (Alwahaby & Cukurova, 2024; Mangaroska et al., 2021) have examined the ethical implications of MMLA, there is imperative to address the ethical considerations that accompany its implementation. The integration of AI into educational contexts introduces complex issues related to privacy, data security, and the potential for biased outcomes. Current studies often overlook these ethical dimensions, focusing primarily on technological advancements and educational outcomes. To ensure the responsible use of AI in education, there must be a concerted effort to incorporate ethical guidelines into research and practice. This includes developing robust data protection measures, ensuring transparency in AI models, and actively working to mitigate biases that could disadvantage certain groups of students.

By prioritizing ethical considerations, researchers and educators can foster trust in AI technologies, ensuring their application supports fair and equitable educational opportunities for all learners. As the field of MMLA advances in both technological capability and ethical integrity, it is crucial to safeguard students' rights and well- being. Our work provides a foundation for developing more informed policies on the use of AI and data analytics in education, ultimately leading to more responsible and effective implementation of computer- based learning tools in educational institutions.

# 6.5. Benefits and challenges of AI-enhanced MMLA studies

The findings from RQ5 highlight both the benefits and challenges of AI- enhanced MMLA studies. On the positive side, AI has significantly improved the ability to monitor and analyze complex learning behaviors, leading to more personalized and adaptive learning experiences (Sharma & Giannakos, 2020). The integration of AI allows for real- time feedback, automated assessment, and enhanced decision- making for educators, ultimately contributing to better student engagement and performance, which are consistent with the findings of other studies (Ez- Zaouia & LavouÃ©, 2017; Thomas, 2018). However, several challenges persist, as highlighted in previous studies (e.g., Alwahaby & Cukurova, 2024; Cukurova et al., 2020), including ethical concerns related to privacy and the potential for algorithmic bias. Additionally, the implementation of AI in real- world educational settings often requires large sample sizes and sophisticated technical infrastructure, which can be resource- intensive and demand expert knowledge. Thus, while educators can benefit remarkably from the deployment of AI- enhanced MMLA in a contextualized learning environment, it is essential to increase their awareness of its existing hurdles.

Interestingly, it appears that the top five reported benefits of AI- enhanced MMLA are interrelated with each other resulting in optimizing the learning environment and improving learning gains among individual learners, as indicated by several studies (e.g., Emerson et al., 2020a; Reilly & Schneider, 2019). Collectively, the results manifest that the key adaptability of MMLA lies in offering deeper insights, personalized learning paths, real- time feedback, enhanced learning experiences, and making accurate behavior predictions.

# 7. Limitation

As the first SLR at the intersection of the rapidly evolving fields of AI and MMLA, this paper serves as a foundational starting point, but it is not without limitations. Many of these limitations arise from the methodological decisions that followed the PRISMA protocol to collect and analyze literature. Designing search terms and a set of criteria, along with the reliance on a limited number of databases, might introduce certain biases that could influence the comprehensiveness of the results. To reduce such potential biases, we conducted our literature search across 11 reputable databases, ensuring a broader and more representative sample of studies. However, by searching in the title and abstract of studies across these databases for initial selection, we may have overlooked a subset of MMLA studies that employed AI techniques but did not highlight them as primary contributions, as well as studies that utilized multimodal data without explicitly using terms like "multi*modal" or "multi*sourced" in their title or abstract. To mitigate these issues, we considered a broad set of search terms for each main code word. Furthermore, selecting only papers published in English should be acknowledged as another limitation, which may have introduced bias in our findings about the geographic distribution of AI- enhanced MMLA studies. Concerning this bias, we mostly focused on international collaborations.

Another limitation that should be acknowledged is that in our analysis, we made a methodological decision to consider all included studies with equal weight, regardless of the depth or centrality of AI integration in their MMLA approaches. This approach allowed us to capture a broad overview of AI applications in MMLA, encompassing both studies that focus on developing AI techniques for MMLA and those that utilize existing AI methods in supporting roles. While this approach provided a comprehensive view, it may not fully capture the nuances in AI's role and significance across different studies. Future research could benefit from a more granular analysis that distinguishes between these different types of AI integration in MMLA. By highlighting this methodological consideration, we hope to encourage more detailed classifications in future reviews, ultimately contributing to a richer understanding of AI's evolving role in MMLA.

# 8. RediT authorship contribution statement

Mehrnoush Mohammadi: Writing - review & editing, Writing - original draft, Visualization, Software, Resources, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Elham Tajik: Writing - original draft, Methodology, Formal analysis, Data curation, Conceptualization. Roberto Martinez Maldonado: Writing - review & editing, Writing - original draft, Methodology, Conceptualization. Shazia Sadiq: Writing - review & editing, Supervision, Conceptualization. Wojtek Tomaszewski: Writing - review & editing, Supervision, Conceptualization. Hassan Khosravi: Writing - review & editing, Writing - original draft, Visualization, Supervision, Software, Resources, Methodology, Investigation, Formal analysis, Data curation, Conceptualization.

# Statements on open data and ethics

This study is a systematic literature review that analyzed published research and did not involve direct human participants. Therefore, no ethical approval was required. The complete list of analyzed papers is provided in the appendix of this manuscript. Additional materials can be obtained by sending a request email to the corresponding author.

# Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

# Acknowledgements

This research was supported by the Australian Government through the Australian Research Council's Industrial Transformation Training Centre for Information Resilience (CIRES) project number IC200100022.

# Appendix A

This appdix proide a compnive smary of the phs, st, and sub- componens within the MMLA framework that were enanced through AI techniques in the included studies. Table 5 serves as a detailed reference for researchers seeking to understand how different stages of the MMLA pipeline, ranging from data collection to intervention, have been augmented by AI methods across the reviewed literature.

Table 5 Al- th  t  t  t  t  

<table><tr><td rowspan="2"># Papers</td><td rowspan="2">Collection Acquisition</td><td colspan="4">Pre-processing</td><td>Annotation Labeling</td><td rowspan="2">Fusion F-Engineering</td><td rowspan="2">Modelling M-Learning</td><td rowspan="2">Analysis I-Generation</td></tr><tr><td>Acquisition</td><td>Anonymizing</td><td>Cleaning</td><td>Transformation</td><td>Augmentation</td></tr><tr><td>1 (Reilly &amp;amp; Schneider, 2019)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>2 (Sabuncuoglu &amp;amp; Sezgin, 2023)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>3 (Huang et al., 2019)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>4 (Sharma et al., 2019)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>5 (Chejara et al., 2020)</td><td>âœ“</td><td></td><td></td><td></td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>6 (Yun et al., 2020)</td><td></td><td></td><td></td><td></td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>7 (Olsen et al., 2020)</td><td></td><td></td><td></td><td></td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>8 (Emerson et al., 2020b)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>9 (Emerson et al., 2020a)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>10 (Cukurova et al., 2020)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>11 (Som et al., 2020)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>12 (Chng et al., 2020)</td><td>âœ“</td><td></td><td>âœ“</td><td></td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>13 (Chang et al., 2021)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>14 (Kawamura et al., 2021)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>15 (Israel et al., 2021)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>16 (Cebral-Loureda &amp;amp; Torres-Huitzi, 2021)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>17 (Peng et al., 2021)</td><td>âœ“</td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>18 (Nandi et al., 2021)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>19 (Closer et al., 2022)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>20 (Lee et al., 2022)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>21 (Di Mitri et al., 2022)</td><td>âœ“</td><td></td><td></td><td></td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>22 (Chettaoui et al., 2023)</td><td>âœ“</td><td></td><td></td><td></td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>23 (Moon et al., 2022)</td><td>âœ“</td><td></td><td>âœ“</td><td></td><td>âœ“</td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>24 (Ma et al., 2022)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>25 (Chejara et al., 2023b)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>26 (Chan et al., 2023)</td><td></td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>27 (Akila et al., 2023)</td><td>âœ“</td><td></td><td></td><td></td><td></td><td>âœ“</td><td></td><td>âœ“</td><td></td></tr><tr><td>28 (Akila et al., 2023)</td><td></td><td></td><td>âœ“</td><td></td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>29 (Yusuf et al., 2023)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>30 (Vatral et al., 2023)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>31 (Sabuncuoglu &amp;amp; Sezgin, 2023)</td><td></td><td>âœ“</td><td>âœ“</td><td></td><td></td><td></td><td></td><td>âœ“</td><td>âœ“</td></tr><tr><td>32 (Ouyang et al., 2023)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td></td><td></td></tr><tr><td>33 (Nguyen et al., 2023)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>34 (Lin et al., 2023)</td><td></td><td></td><td>âœ“</td><td></td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>35 (Lee et al., 2023)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td>âœ“</td><td></td><td>âœ“</td><td></td></tr><tr><td>36 (JÃ¤rvelÃ¤ et al., 2023)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>37 (Ivleva et al., 2023)</td><td>âœ“</td><td></td><td>âœ“</td><td></td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>38 (Huang et al., 2023a)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>39 (Emerson et al., 2023)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td>âœ“</td><td>âœ“</td><td></td></tr><tr><td>40 (Li et al., 2023)</td><td></td><td>âœ“</td><td></td><td>âœ“</td><td></td><td></td><td></td><td>âœ“</td><td></td></tr><tr><td>41 (Chejara et al., 2023a)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td></td><td>âœ“</td><td>âœ“</td></tr><tr><td>42 (Zhou et al., 2024)</td><td></td><td></td><td></td><td>âœ“</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>43 (Zhao et al., 2024)</td><td></td><td></td><td>âœ“</td><td></td><td></td><td></td><td></td><td>âœ“</td><td></td></tr></table>

# References

Akila, D., Garg, H., Pal, S., & Jeyalaksskimi, S. (2023). Research on recognition of students attention in offline classroom- based on deep learning. Education and Information Technologies, 1- 29. Akre, S., Palandurkar, N., Iyengar, A., Chayande, G., & Kumar, P. (2023). Engagedat- vl: A multimodal engagement dataset comprising of emotional, cognitive, and behavioral cues in virtual learning environment. In International conference on pattern recognition and machine intelligence (pp. 270- 278). Springer. Alfredo, R., Riordan, D., Nie, L., Kennedy, P., Power, T., Hayes, C., Chen, H., McGregor, C., Swiecki, Z., Gasevic, D., & Martinez- Maldonado, R. (2023). "That Student Should be a Lion Tamer!" StressViz: Designing a Stress Analytics Dashboard for Teachers. In LAK23: 13th international learning analytics and knowledge conference (pp. 57- 67). Alwahaby, H., & Cukurova, M. (2024). Navigating the ethical landscape of multimodal learning analytics: A guiding framework. In Ethics in online AI- based systems (pp. 25- 53). Elsevier. Alwahaby, H., Cukurova, M., Papamitsiou, Z., & Giannakos, M. (2022). The evidence of impact and ethical considerations of multimodal learning analytics: A systematic literature review. In The multimodal learning analytics handbook (pp. 289- 325). Anderson, T., & Shattuck, J. (2012). Design- based research: A decade of progress in education research? Educational Researcher, 41, 16- 25.

Aslan, S., Alyuz, N., Tanriover, C., Mete, S. E., Okur, E., D'Mello, S. K., & Arslan Esme, A. (2019). Investigating the impact of a real- time, multimodal student engagement analytics technology in authentic classrooms. In Proceedings of the 2019 chi conference on human factors in computing systems (pp. 1- 12). Asthana, S., Arif, T., & Thompson, K. C. (2023). Field experiences and reflections on using llms to generate comprehensive lecture metadata. Ayesha, S., Hanif, M. K., & Talib, R. (2020). Overview and comparative study of dimensionality reduction techniques for high dimensional data. Information Fusion, 59, 44- 58. Baker, R. S., & Hawn, A. (2022). Algorithmic bias in education. International Journal of Artificial Intelligence in Education, 1- 41. Bin Qushem, U. (2020). Trends of multimodal learning analytics: A systematic literature review. https://erepo.uef.fi. Blikstein, P., & Worsley, M. (2016). Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journal of Learning Analytics, 3, 220- 238. Bond, M., Khosravi, H., De Laat, M., Bergdahl, N., Negrea, V., Oxley, E., Pham, P., Chong, S. W., & Siemens, G. (2024). A meta systematic review of artificial intelligence in higher education: A call for increased ethics, collaboration, and rigour. International Journal of Educational Technology in Higher Education, 21.

van Breugel, B., Seedat, N., Imrie, F., & van der Schaar, M. (2024). Can you rely on your model evaluation? Improving models, evaluation with synthetic test data. Advances in Neural Information Processing Systems 36. Buckingham Shum, S. J., & Luckin, R. (2019). Learning analytics and ai: Politics, pedagogy and practices. British Journal of Educational Technology, 50, 2785- 2793. Carpenter, D., Min, W., Lee, S., Ozogul, G., Zheng, X., & Lester, J. (2024). Assessing student explanations with large language models using fine- tuning and few- shot learning. In Proceedings of the 19th workshop on innovative use of NLP for building educational applications (BBA 2024) (pp. 403- 413). Cebral- Loureda, M., & Torres- Huitzil, O. (2021). Neural deep learning models for learning analytics in a digital humanities laboratory. In 2021 machine learning- driven digital technologies for educational innovation workshop (pp. 1- 8). IEEE. Cerovac, M., & Keane, T. (2024). Early insights into piaget's cognitive development model through the lens of the technologies curriculum. International Journal of Technology and Design Education, 1- 21. Chan, R. Y. Y., Wong, C. M. V., & Yum, Y. N. (2023). Predicting behaviour change in students with special education needs using multimodal learning analytics. IEEE Access. Chango, W., Cerezo, R., Sanchez- Santillan, M., Azevedo, R., & Romero, C. (2021). Improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources. Journal of Computing in Higher Education, 33, 614- 634. Chango, W., Lara, J. A., Cerezo, R., & Romero, C. (2022). A review on data fusion in multimodal learning analytics and educational data mining. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 12, Article e1458. Chatti, M. A., Dyckhoff, A. L., Schroeder, U., & Thus, H. (2012). A reference model for learning analytics. International Journal of Technology Enhanced Learning, 4, 318- 331. Chejara, P., Prieto, L. P., Rodriguez- Triana, M. J., Ruiz- Calleja, A., Kasepalu, R., Chounta, I. A., & Schneider, B. (2023a). Exploring indicators for collaboration quality and its dimensions in classroom settings using multimodal learning analytics. In European conference on technology enhanced learning (pp. 60- 74). Springer. Chejara, P., Prieto, L. P., Rodriguez- Triana, M. J., Ruiz- Calleja, A., & Khalil, M. (2023b). Impact of window size on the generalizability of collaboration quality estimation models developed using multimodal learning analytics. In LAK23: 13th international learning analytics and knowledge conference (pp. 559- 565). Chejara, P., Prieto, L. P., Ruiz- Calleja, A., Rodriguez- Triana, M. J., Shankar, S. K., & Kasepalu, R. (2020). Quantifying collaboration quality in face- to- face classroom settings using mmla. In Proceedings 26. Collaboration technologies and social computing: 26th international conference, Collab Tech 2020 (pp. 159- 166). Springer. Chettaooui, N., Atia, A., & Bouhlel, M. S. (2023). Student performance prediction with eye- gaze data in embodied educational context. Education and Information Technologies, 28, 833- 855. Chng, E., Seyam, M. R., Yao, W., & Schneider, B. (2020). Using motion sensors to understand collaborative interactions in digital fabrication labs. In Proceedings, Part I: Vol. 21. Artificial intelligence in education: 21st international conference, AIED 2020 (pp. 118- 128). Springer. Closser, A. H., Erickson, J. A., Smith, H., Varatharaj, A., & Botelho, A. F. (2022). Blending learning analytics and embodied design to model students' comprehension of measurement using their actions, speech, and gestures. International Journal of Child- Computer Interaction, 32, Article 100391. Crescenzi- Lanna, L. (2020). Multimodal learning analytics research with young children: A systematic review. British Journal of Educational Technology, 51, 1485- 1504. Crompton, H., & Burke, D. (2023). Artificial intelligence in higher education: The state of the field. International Journal of Educational Technology in Higher Education, 20, 22. Cukurova, M., Giannakos, M., & Martinez- Maldonado, R. (2020). The promise and challenges of multimodal learning analytics. British Journal of Educational Technology, 51, 1441- 1449. Cukurova, M., Zhou, Q., Spikol, D., & Landolfi, L. (2020). Modelling collaborative problem- solving competence with transparent learning analytics: Is video data enough? In Proceedings of the tenth international conference on learning analytics & knowledge (pp. 270- 275). D'Angelo, C. M., & Rajarathinam, R. J. (2024). Speech analysis of teaching assistant interventions in small group collaborative problem solving with undergraduate engineering students. British Journal of Educational Technology, 55, 1583- 1601. Davenport, T. H. (2016). From analytics to artificial intelligence. Journal of Business Analytics, 1, 73- 80. Deeva, G., Bogdanova, D., Serral, E., Snoeck, M., & De Weerdt, J. (2021). A review of automated feedback systems for learners: Classification framework, challenges and opportunities. Computers and Education, 162, Article 104094. Deng, J. H., & Zhao, Y. (2022). A literature review of data- driven multimodal learning analytics in education based on citiespace. In Proceedings of the 2022 5th international conference on education technology management (pp. 390- 397). Di Mitri, D., Schneider, J., & Drachsler, H. (2022). Keep me in the loop: Real- time feedback with multimodal data. International Journal of Artificial Intelligence in Education, 32, 1093- 1118. Di Mitri, D., Schneider, J., Specht, M., & Drachsler, H. (2018). From signals to knowledge: A conceptual model for multimodal learning analytics. Journal of Computer Assisted Learning, 34, 338- 349. Diaz, B., & Nussbaum, M. (2024). Artificial intelligence for teaching and learning in schools: The need for pedagogical intelligence. Computers and Education, 105071. D'mello, S. K., & Kory, J. (2015). A review and meta- analysis of multimodal affect detection systems. ACM Computing Surveys, 47, 1- 36.

Eigenschink, P., Reutterer, T., Vamosi, S., Vamosi, R., Sun, C., & Kalcher, K. (2023). Deep generative models for synthetic data: A survey. IEEE Access, 11, 47304- 47320. Emerson, A., Cloudes, E. B., Azevedo, R., & Lester, J. (2020a). Multimodal learning analytics for game- based learning. British Journal of Educational Technology, 51, 1505- 1526. Emerson, A., Henderson, N., Rowe, J., Min, W., Lee, S., Minogue, J., & Lester, J. (2020b). Early prediction of visitor engagement in science museums with multimodal learning analytics. In Proceedings of the 2020 international conference on multimodal interaction (pp. 107- 116). Emerson, A., Min, W., Rowe, J., Azevedo, R., & Lester, J. (2023). Multimodal predictive student modeling with multi- task transfer learning. In LAK23: 13th international learning analytics and knowledge conference (pp. 333- 344). Eze, M., Rodriguez- Triana, M. J., & Llanperre, J. (2017). How to aggregate les- sadon observational data into learning analytics datasets? In Joint proceedings of the 6th multimodal learning analytics (MMLA) workshop and the 2nd cross- LAK workshop colocated with 7th international learning analytics and knowledge conference (LAK 2017) (pp. 74- 81). CEUR. Eze, M., & Llanperre, J. (2017). Anè¯¾å¤– A tutor- oriented multimodal and contextual emotional dashboard. In Proceedings of the seventh international learning analytics & knowledge conference (pp. 429- 438). Foster, E., & Siddle, R. (2020). The effectiveness of learning analytics for identifying at- risk students in higher education. Assessment & Evaluation in Higher Education, 45, 842- 854. Gao, Y., Zhang, Z., Lin, H., Zhao, X., Du, S., & Zou, C. (2020). Hypergraph learning: Methods and practices. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44, 2548- 2566. Gauvain, M. (2020). Vygotsky's sociocultural theory. In J. B. Benson (Ed.), Encyclopedia of infant and early childhood development (second edition) (pp. 446- 454). Oxford: Elsevier. Giannakos, M., & Cukurova, M. (2023). The role of learning theory in multimodal learning analytics. British Journal of Educational Technology, 54, 1246- 1267. Gibson, D., Kovanovic, V., Ifenthaler, D., Dexter, S., & Feng, S. (2023). Learning theories for artificial intelligence promoting learning processes. British Journal of Educational Technology, 54, 1125- 1146. Guo, A., Kamar, E., Vaughan, J. W., Wallach, H., & Morris, M. R. (2020). Toward fairness in ai for people with disabilities sbg@ a research roadmap. ACM SIGACCESS Accessibility and Computing, 1, 2. Guyon, I., & Elisseff, A. (2006). An introduction to feature extraction. In Feature extraction: Foundations and applications (pp. 1- 25). Springer. van Haastrecht, M., Brinkhuis, M., & Spruit, M. (2024). Federated learning analytics: Integrating the poverty- persistence benefits of machine learning for educational analytics. In International conference on artificial intelligence in education (pp. 62- 74). Springer. Hou, C., Zhu, G., Zheng, J., Zhang, L., Huang, X., Zhong, T., Li, S., Du, H., & Ker, C. L. (2024). Prompt- based and fine- tuned gpt models for context- dependent and independent deductive coding in social annotation. In Proceedings of the 14th learning analytics and knowledge conference (pp. 518- 528). Huang, K., Bryant, T., & Schneider, B. (2019). Identifying collaborative learning states using unsupervised machine learning on eye- tracking, physiological and motion sensor data. In International educational data mining society. Huang, L., Dodeck, T., Chen, B., Huang, X., Tan, C., Lajoie, S. P., & Wang, M. (2023a). Multimodal learning analytics for assessing teachers' self- regulated learning in planning technology- integrated lessons in a computer- based environment. Education and Information Technologies, 28, 15823- 15843. Huang, X., Liu, Y., Huang, L., Onstein, E., & Meschbrock, C. (2023b). Bim and iot data fusion: The data process model perspective. Automation in Construction, 149, Article 104792. Israel, M., Liu, T., Moon, J., Ke, F., & Dahlstrom- Hukki, I. (2021). Methodological considerations for understanding students' problem solving processes and affective trajectories during game- based learning: A data fusion approach. In International conference on human- computer interaction (pp. 201- 215). Springer. Ivleva, N., Pentel, A., Dunajeva, O., & JuÅ¡tÅ¡enko, V. (2023). Deep learning based audiovisual emotion recognition in a smart learning environment. In International conference on interactive collaborative learning (pp. 430- 431). Springer. JÃ¤rvelÃ¤, S., Nguyen, A., Vuorenmaa, E., Malmberg, J., & JÃ¤rvenoja, H. (2023). Predicting regulatory activities for socially shared regulation to optimize collaborative learning. Computers in Human Behavior, 144, Article 107737. Jin, F., Maheshi, B., Martinez- Maldonado, R., Gasevic, D., & Tsai, Y. S. (2024). Scaffolding feedback literacy: Designing a feedback analytics tool with students. Journal of Learning Analytics, 11, 123- 137. Kawamura, R., Shirai, S., Takemura, N., Alizadeh, M., Cukurova, M., Takemura, H., & Nagahara, H. (2021). Detecting drowsy learners at the wheel of e- learning platforms with multimodal learning analytics. IEEE Access, 9, 115165- 115174. Khosravi, H., Shum, S. B., Chen, G., Conati, C., Tsai, Y. S., Judy, J., Knight, S., Martinez- Maldonado, R., Sadiq, Sh., & Gasevic, D. (2022). Explainable Artificial Intelligence in education. Computers and Education: Artificial Intelligence, 3, Article 100074. Lave, J. (1991). Situated learning: Legitimate peripheral participation. Cambridge University Press. Lee, Y., Chen, H., Zhao, G., & Specht, M. (2022). Wedar: Webcam- based attention analysis via attention regulator behavior recognition with a novel e- reading dataset. In Proceedings of the 2022 international conference on multimodal interaction (pp. 319- 328).

Lee, Y., Migut, G., & Specht, M. (2023). Behavior- based feedback loop for attentive e- reading (bflae): A real- time computer opinion approach. In Micro- gesture analysis for hidden emotion understanding 2023 (pp. 12). Li, J., Cheng, K., Wang, S., Morstatter, R., Trevino, R. P., Tang, J., & Liu, H. (2017). Feature selection: A data perspective. ACM Computing Surveys, 50, 1- 45. Li, X., Yan, L., Zhao, L., Martinez- Maldonado, R., & Gasevic, D. (2023). CVPE: A computer vision approach for scalable and privacy- preserving socio- spatial, multimodal learning analytics. In LAK23: 13th international learning analytics and knowledge conference (pp. 175- 185). Li, Q., Peng, H., Li, J., Xia, C., Yang, R., Sun, L., Yu, P. S., & He, L. (2022). A survey on text classification: From traditional to deep learning. ACM Transactions on Intelligent Systems and Technology, 13, 1- 41. Lim, L., Bannert, M., van der Graaf, J., Singh, S., Fan, Y., Surendrannair, S., Rakovic, M., Molenaar, I., Moore, J., & Gasevic, D. (2023). Effects of real- time analytics- based personalized scaffolds on students' self- regulated learning. Computers in Human Behavior, 139, Article 107547. Lin, C. J., Wang, W. S., Lee, H. Y., Huang, Y. M., & Wu, T. T. (2023). Recognitions of image and speech to improve learning diagnosis on stem collaborative activity for precision education. Education and Information Technologies, 1- 26. Liu, Q., Pinto, J. D., & Paquette, L. (2024). Applications of explainable ai (xai) in education. In Trust and inclusion in AI- mediated education: Where human learning meets learning machines (pp. 93- 109). Springer. Ma, Y., Celepkolu, M., & Boyer, K. E. (2022). Detecting impasse during collaborative problem solving with multimodal learning analytics. In LAK22: 12th international learning analytics and knowledge conference (pp. 45- 55). Mangaroska, K., & Giannakos, M. (2018). Learning analytics for learning design: A systematic literature review of analytics- driven design to enhance learning. IEEE Transactions on Learning Technologies, 12, 516- 534. Mangaroska, K., Martinez- Maldonado, R., Vesin, B., & Gasevic, D. (2021). Challenges and opportunities of multimodal data in human learning: The computer science students' perspective. Journal of Computer Assisted Learning, 37, 1030- 1047. Martinez- Maldonado, R., Echeverria, V., Fernandez- Nieto, G., Yan, L., Zhao, L., Alfredo, R., Li, X., Dix, S., Jaggard, H., Wotherspoon, R., & Osborne, A. (2023). Lessons learnt from a multimodal learning analytics deployment in the wild. ACM Transactions on Computer- Human Interaction, 31, 1- 41. Martin, F., Chen, Y., Moore, R. L., & Westin, C. D. (2020). Systematic review of adaptive learning research designs, context strategies, and technologies from 2009 to 2018. Educational Technology Research and Development, 68, 1903- 1929. McLelland, J. (2024). Connecting piaget's cognitive development theory to technology in the early years, vol. 4. He, K. Pupu. Moon, J., Ke, F., Sokolikj, Z., & Dahlstrom- Hakki, I. (2022). Multimodal data fusion to track students' distress during educational gameplay. Journal of Learning Analytics, 9, 75- 87. Mozafari, J., Jangra, A., & Jatowt, A. (2024). Triviahg: A dataset for automatic hint generation from factoid questions. In Proceedings of the 47th international ACM SIGIR conference on research and development in information retrieval (pp. 2060- 2070). Mu, S., Cui, M., & Huang, X. (2020). Multimodal data fusion in learning analytics: A systematic review. Sensors, 20, 685- 6. Nandi, A., Xhafa, F., Subirats, L., & Fort, S. (2021). Real- time multimodal emotion classification system in e- learning context. In International conference on engineering applications of neural networks (pp. 423- 435). Springer. Narayanan, N. (2024). The era of generative ai: Transforming academic libraries, education, and research. Nguyen, A., Jarvela, S., Rosc, C., Jarvenoja, H., & Malmberg, J. (2023). Examining socially shared regulation and shared physiological arousal events with multimodal learning analytics. British Journal of Educational Technology, 54, 293- 312. Noroozi, O., Alikhani, I., Jarvela, S., Kirschner, P. A., Juuso, I., & Seppanen, T. (2019). Multimodal data to design visual learning analytics for understanding regulation of learning. Computers in Human Behavior, 100, 298- 304. Noroozi, O., Pijera- Diaz, H. J., Sobocinski, M., Dindar, M., Jarvela, S., & Kirschner, P. A. (2020). Multimodal data indicators for capturing cognitive, motivational, and emotional learning processes: A systematic literature review. Education and Information Technologies, 25, 5499- 5547. Ochoa, X., Lang, A. G., & Siemens, G. (2017). Multimodal learning analytics. In The handbook of learning analytics, vol. 1 (pp. 129- 141). Ochoa, X., Lang, C., Siemens, G., Wise, A., Gasevic, D., & Merceron, A. (2022). Multimodal learning analytics- rationale, process, examples, and direction. In The handbook of learning analytics (pp. 54- 65). Ochoa, X., & Worsley, M. (2016). Augmenting learning analytics with multimodal sensory data. Journal of Learning Analytics, 9, 213- 219. OECD, E. (2023). Algorithmic bias, quality and data protection. https://www.youtube.com/watch?v=3ZzmjgsDM2A. youtube. Olsen, J. K., Sharma, K., Rummel, N., & Aleven, V. (2020). Temporal analysis of multimodal data to predict collaborative learning outcomes. British Journal of Educational Technology, 51, 1527- 1547. Ouhaichi, H., Spikol, D., & Vogel, B. (2023). Research trends in multimodal learning analytics: A systematic mapping study. Computers and Education: Artificial Intelligence, 4, Article 100136. Ouhaichi, H., Spikol, D., & Vogel, B. (2024). A systematic review of multimodal learning analytics design models and frameworks. In Proceedings of 16th international conference on education and new learning technologies. IATED (pp. 1- 11).

Ouyang, F., & Jiao, P. (2021). Artificial Intelligence in education: The three paradigms. Computers and Education: Artificial Intelligence, 2, Article 100020. Ouyang, F., Xu, W., & Cukurova, M. (2023). An artificial intelligence- driven learning analytics method to examine the collaborative problem- solving process from the complex adaptive systems perspective. International Journal of Computer- Supported Collaborative Learning, 18, 39- 66. Ouyang, F., Zheng, L., & Jiao, P. (2022). Artificial intelligence in online higher education: A systematic review of empirical research from 2011 to 2020. Education and Information Technologies, 27, 7893- 7925. Oviatt, S. (2018). Ten opportunities and challenges for advancing student- centered multimodal learning analytics. In Proceedings of the 20th ACM international conference on multimodal interaction (pp. 87- 94). Page, M. J., McKenzie, J. E., Bossuyt, P. M., Boulron, I., Hoffmann, T. C., Mulrow, C. D., Shamseer, L., Tetzlaff, J. M., Akl, E. A., Brennan, S. E., et al. (2021). The prisma 2020 statement: An updated guideline for reporting systematic reviews. bmj (p. 372). Payne, A. L., Compton, M., & Kennedy, S. (2023). Supporting and humanising behavioural change without the behavioralism: Digital descriptors, learning analytics and manager. In Human data interaction, disadvantage and skills in the community: Enabling cross- sector environments foræ½›igd inclusion (pp. 111- 131). Springer. Pei, B., Xing, W., & Wang, M. (2023). Academic development of multimodal learning analytics: A bibliometric analysis. Interactive Learning Environments, 31, 3543- 3561. Peng, Q., Qie, N., Yuan, L., Chen, Y., & Gao, Q. (2019). Design of an online education evaluation system based on multimodal data of learners. In Proceedings, part IICross- cultural design. Culture and society: 11th international conference, CCD 2019, held as part of the 21st HCI international conference, HCII 2019, proceedings, Part II 21 (pp. 458- 468). Springer. Peng, S., Ohira, S., & Nagao, K. (2021). Recognition of students' multiple mental states in conversation based on multimodal cues. In Computer supported education: 12th international conference, CSEDU 2020, revised selected papers 12 (pp. 468- 479). Springer. Pozdniakov, S., Brazil, j., Mohammadi, M., Dollinger, M., Sadiq, S., & Khosravi, H. (2025). AI- assisted co- creation: Bridging skill gaps in student- generated content. Journal of Learning Analytics, 12, 129- 151. Prinsloo, P., Slade, S., & Khalil, M. (2023). Multimodal learning analytics- in- between student privacy and encroachment: A systematic review. British Journal of Educational Technology, 54, 1566- 1586. Rahul Katarya, R. (2023). Deep auto encoder based on a transient search capsule network for student performance prediction. Multimedia Tools and Applications, 82, 23427- 23451. Reilly, J. M., & Schneider, B. (2019). Predicting the quality of collaborative problem solving: a legalistic analysis of discover. In International educational data mining society. Sabuncuoglu, A., & Sezgin, T. M. (2023). Developing a multimodal classroom engagement analysis dashboard for higher- education. Proceedings of the ACM on Human- Computer Interaction, 7, 1- 23. Sagi, O., & Rokach, L. (2018). Ensemble learning: A survey. In Wiley interdisciplinary reviews: Data mining and knowledge discovery, vol. 8. e1249. Sailer, M., Ninaus, M., Huber, S. E., Bauer, E., & Greiff, S. (2024). The end is the beginning is the end: The closed- loop learning analytics framework. Computers in Human Behavior, 108305. Samuelsen, J., Chen, W., & Wasson, B. (2019). Integrating multiple data sources for learning analytics- review of literature. Research and Practice in Technology Enhanced Learning, 14, Article 11. Schneider, B., Worsley, M., & Martinez- Maldonado, R. (2021). Gesture and gaze: Multimodal data in dyadic interactions. In International handbook of computer- supported collaborative learning (pp. 625- 641). Springer. Shankar, S. K., Prieto, L. P., Rodriguez- Triana, M. J., & Ruiz- Calleja, A. (2018). A review of multimodal learning analytics architectures. In 2018 IEEE 18th international conference on advanced learning technologies (ICALT). IEEE 18th, 212- 214). Shankar, S. K., Ruiz- Calleja, A., Prieto, L. P., Rodriguez- Triana, M. J., Chejara, P., & Tripathi, S. (2023). Cimla: A modular and modifiable data preparation, organization, and fusion infrastructure to partially support the development of context- aware mmla solutions. JUCS: Journal of Universal Computer Science. Sharma, K., & Giannakos, M. (2020). Multimodal data capabilities for learning: What can multimodal data tell us about learning? British Journal of Educational Technology, 51, 1450- 1484. Sharma, K., Papamitsiou, Z., & Giannakos, M. (2019). Building pipelines for educational data using ai and multimodal analytics: A "grey- box" approach. British Journal of Educational Technology, 50, 3004- 3031. Slupczynski, M., & Klamma, R. (2021). Milli- psycloud: Facilitating multimodal learning analytics by explainable ai and blockchain. In MLeS@ EC- TEL (pp. 22- 28). Som, A., Kim, S., Lopez- Prado, B., Dhamija, S., Alozie, N., & Tamrakar, A. (2020). A machine learning approach to assess student group collaboration using individual level behavioral cues. In Computer vision- EOCV 2020 workshops, proceedings, Part VI 16 (pp. 79- 94). Springer. Song, Y., Wang, T., Cai, P., Mondal, S. K., & Sahoo, J. P. (2023). A comprehensive survey of few- shot learning: Evolution, applications, challenges, and opportunities. ACM Computing Surveys, 55, 1- 40. Sumer, O., Goldberg, P., D'Mello, S., Gerjets, P., Trautwein, U., & Kasneci, E. (2023). Multimodal engagement analysis from facial videos in the classroom. IEEE Transactions on Affective Computing, 14, 1012- 1027.

Tahiru, F. (2021). Ai in education: A systematic literature review. Journal of Cases on Information Technology, 23, 1â€“20.  Tan, A. Z., Yu, H., Cui, L., & Yang, Q. (2022). Towards personalized federated learning. IEEE Transactions on Neural Networks and Learning Systems, 34, 9587â€“9603.  Thomas, C. (2018). Multimodal teaching and learning analytics for classroom and online educational settings. In Proceedings of the 20th ACM international conference on multimodal interaction (pp. 542â€“545).  Tiukhova, E., Vemuri, P., Flores, N. L., Lillind, A. S., OskarsdÃ³ttir, M., Poelmans, S., Baesens, B., & Snoeck, M. (2024). Explainable learning analytics: Assessing the stability of student success prediction models by means of explainable ai. Decision Support Systems, 182, Article 114229.  Torre- Bastida, A. I., DÃ­az- de Arcaya, J., Osilla, E., Muhammad, K., Camacho, D., & Del Ser, J. (2021). Bio- inspired computation for big data fusion, storage, processing, learning and visualization: State of the art and future directions. Neural Computing & Applications, 1â€“31.  Urbanowicz, R. J., Olson, R. S., Schmitt, P., Meeker, M., & Moore, J. H. (2018). Benchmarking relief- based feature selection methods for bioinformatics data mining. Journal of Biomedical Informatics, 85, 168â€“183.  Vatral, C., Lee, M., Cohn, C., Davalos, E., Levin, D., & Biswas, G. (2023). Prediction of students' self- confidence using multimodal features in an experiential nurse training environment. In International conference on artificial intelligence in education (pp. 266â€“271). Springer.  Viola, P., & Jones, M. J. (2004). Robust real- time face detection. International Journal of Computer Vision, 57, 137â€“154.  Wang, Y., & Gu, X. (2024). Data fusion in classroom- based multimodal learning analytics: A systematic literature review. In Proceedings of the 18th international conference of the learning sciences- ICLS, international society of the learning sciences (pp. 951â€“954).  Worsley, M., Martinez- Maldonado, R., & D'Angelo, C. (2021). A New Era in Multimodal Learning Analytics: Twelve Core Commitments to Ground and Grow MMLA. Journal of Learning Analytics, 8, 10â€“27.  Worsley, M., Abrahamson, D., Blikstein, P., Grover, S., Schneider, B., & Tissenbaum, M. (2016). Situating multimodal learning analytics. eScholarship.  Worsley, M., & Martinez- Maldonado, R. (2018). Multimodal learning analytics' past, present, and potential futures. Cross- MMLAÂ® LAK, 2.  Yan, L., Martinez- Maldonado, R., Zhao, L., Deppeler, J., Corrigan, D., & Gasevic, D. (2022). How do teachers use open learning spaces? Mapping from teachers' socio- spatial data to spatial pedagogy. In LAK22: 12th international learning analytics and knowledge conference (pp. 87â€“97).

Yan, L., Martinez- Maldonado, R., Gallo Cordoba, B., Deppeler, J., Corrigan, D., & Gasevic, D. (2022). Mapping from proximity traces to socio- spatial behaviours and student progression at the school. British Journal of Educational Technology, 53(6), 1645â€“1664.  Yan, L., Zhao, L., Gasevic, D., & Martinez- Maldonado, R. (2022). Scalability, sustainability, and ethicality of multimodal learning analytics. In LAK22: 12th international learning analytics and knowledge conference (pp. 13â€“23).  Yan, L., Gasevic, D., Echeverria, V., Jin, Y., Zhao, L., & Martinez- Maldonado, R. (2025). From complexity to parsimony: Integrating latent class analysis to uncover multimodal learning patterns in collaborative learning. In Proceedings of the 15th international learning analytics and knowledge conference (pp. 70â€“81).  Yan, L., Martinez- Maldonado, R., & Gasevic, D. (2023). Generative artificial intelligence in learning analytics. Contextualising opportunities and challenges through the learning analytics cycle. arXiv preprint arXiv:2312.00087.  Yun, H., Fortenbacher, A., Helbig, R., GeiÃŸler, S., & Pinkwart, N. (2020). Emotion recognition from physiological sensor data to support self- regulated learning. In Computer supported education: 11th international conference, CSEDU 2019, revised selected papers 11 (pp. 155â€“179). Springer.  Yusuf, A., Noor, N. M., & Bello, S. (2023). Using multimodal learning analytics to model students' learning behavior in animated programming classroom. Education and Information Technologies, 1â€“44.  Zawacki- Richter, O., MarÃ­n, V. I., Bond, M., & Gouverneur, F. (2019). Systematic review of research on artificial intelligence applications in higher educationâ€”where are the educators? International Journal of Educational Technology in Higher Education, 16, 1â€“27.  Zhao, F., Liu, G. Z., Zhou, J., & Yin, C. (2023). A learning analytics framework based on human- centered artificial intelligence for identifying the optimal learning strategy to intervene in learning behavior. Educational Technology & Society, 26, 132â€“146.  Zhao, L., Echeverria, V., Swiecki, Z., Yan, L., Alfredo, R., Li, X., Gasevic, D., & Martinez- Maldonado, R. (2024). Epistemic Network analysis for end- users: Closing the loop in the context of multimodal analytics for collaborative team learning. In Proceedings of the 14th learning analytics and knowledge conference (pp. 90â€“100).  Zhou, Q., Bhattacharya, A., Suraworachet, W., Nagahara, H., & Cukurova, M. (2023). Automated detection of students' gaze interactions in collaborative learning videos: A novel approach. In European conference on technology enhanced learning (pp. 504â€“517). Springer.  Zhou, Q., Suraworachet, W., & Cukurova, M. (2024). Detecting non- verbal speech and gaze behaviours with multimodal data and computer vision to interpret effective collaborative learning interactions. Education and Information Technologies, 29, 1071â€“1098.