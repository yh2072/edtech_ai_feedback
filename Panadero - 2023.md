# Toward a paradigm shift in feedback research: Five further steps influenced by self-regulated learning theory

Ernesto Panadero

To cite this article: Ernesto Panadero (2023) Toward a paradigm shift in feedback research: Five further steps influenced by self- regulated learning theory, Educational Psychologist, 58:3, 193- 204, DOI: 10.1080/00461520.2023.2223642

To link to this article: https://doi.org/10.1080/00461520.2023.2223642

# Toward a paradigm shift in feedback research: Five further steps influenced by self-regulated learning theory

Ernesto Panadero $^{a,b,c}$

$^{a}$ Institute of Education, St. Patrick's Campus, Dublin City University, Dublin, Ireland;  $^{b}$ Facultad de Educación y Deporte, Universidad de Deusto, Bilbao, España;  $^{c}$ IKERBASQUE, Basque Foundation for Science, Bilbao, Spain

# ABSTRACT

As the articles in this special issue on "Psychological Perspectives on the Effects and Effectiveness of Assessment Feedback" have shown, feedback is a key factor in education. Although there exists a substantial body of research on the topic, it is imperative to continue advancing the field. My aim is to outline five steps to solidify the potential paradigm shift that the feedback field may already be experiencing, while incorporating the insights gleaned from the articles within this special issue. Firstly, there is a need to develop new models that thoroughly explore and operationalize the intricacies of the feedback phenomenon. Secondly, it is essential to conceptualize feedback as a dynamic process and collect data that directly investigates this process. Thirdly, it would be advantageous to leverage insights from the self- regulated learning field, which has made significant strides in advancing measurement methods applicable to feedback research. Fourthly, employing multimodal methods can enrich our comprehension of the multifaceted nature of the feedback process. Lastly, placing the feedback agent at the core of the feedback process, with particular attention to individual differences, is of utmost importance.

A consensus among the authors of the articles in this special issue is that feedback is vital for students' academic achievement, as shown by extensive research aimed at understanding its effects (Wiliam, 2018). In fact, a report by the Educational Endowment Foundation (2018) emphasized that enhancing the quality of feedback is the most effective educational intervention due to its substantial impact on student learning, its cost- effectiveness, and its ultimate societal impact. Importantly, feedback is often delivered in nonproductive ways, which leads to it having no impact or even having negative effects (e.g., Bangert- Drowns et al., 1991; Fyfe et al., 2023/this issue; Kluger & DeNisi, 1996).

At least two reflections derive from the fact that feedback is often unsuccessful. First, researchers need to understand better how it works, which means changing how feedback is investigated in order to improve the educational implementation of feedback. In this sense, all the papers in this special issue make calls about this needed change or present evidence/models on how to improve the implementation of feedback. In this commentary, I will propose that this change is already happening. And second, it reveals the complexity of the feedback phenomenon, because the same feedback does not work equally across all students. Importantly, the field has reached the conclusion that "one size fits all" does not work, and the role of individual differences in feedback effects is now explored more extensively (e.g., Panadero & Lipnevich, 2022). This is also a common thread mentioned across the articles in this special issue.

Additionally, to learn from feedback about one's own learning is extremely challenging (Winstone et al., 2017). One of the reasons is that processing feedback involves aspects such as emotions, motivation, self- regulation, or interactional factors (Brown & Zhao, 2023/this issue; Fong & Schallert, 2023/this issue; Harber, 2023/this issue). Fortunately, students can develop the skill of utilizing feedback through deliberate practice (Fyfe et al., 2023/this issue), and there are comprehensive lists of pedagogical recommendations that teachers can implement to maximize feedback effects (Winstone & Nash, 2023/this issue). However, there is an urgent need for further research to delve into the specific mechanisms through which students process feedback. In this regard, valuable insights can be gained from the field of self- regulated learning (SRL), as SRL researchers have extensively examined similar processes and there exist interconnections between feedback and SRL (Butler & Winne, 1995).

In my opinion, the articles in this special issue represent the current wave of feedback research presenting valuable ideas on how to move the field forward, from how researchers measure feedback (Brown & Zhao, 2023/this issue), to how students develop their capacity to react to it (Fyfe et al., 2023/this issue), or how they react emotionally and motivationally to it (Fong & Schallert, 2023/this issue), among others. These articles show that feedback researchers are growing to understand exactly how feedback works so that its negative effects can be avoided. My aim here is to present these latest developments based on the articles in

the special issue. I argue that the feedback literature could be further enhanced by greater focus on: (1) the importance of models, (2) feedback as a process, (3) the measurement of feedback effectiveness, (4) multimodal methods, and (5) the centrality of the feedback actor.

# Feedback as a complex process: the importance of models

Scientific understanding has advanced enormously, from earlier views of what feedback is (e.g., teacher- driven, isolated) to our current understanding of the phenomenon (Winstone & Nash, 2023/this issue). For example, in recent years there has been a push to understand "students' agentic engagement with feedback," which conceptualizes the student as an active agent who ultimately is able to create their own feedback (Jonsson, 2013; Winstone et al., 2017); and on the intricate connections between motivation, emotion, and feedback effectiveness (Fong & Schallert, 2023/this issue). These new perspectives have been achieved by theory development which is crucial for making advances in any field (Greene, 2022), with the creation of new theoretical models as a central action of theory development.

Take for example, the articles in this special issue. One of them presents the new Model of Threat Infused Interracial Feedback (Harber, 2023/this issue); another one by Winstone and Nash (2023/this issue) presents a framework for asking about mechanisms involved in feedback effects, resembling a model for investigating feedback; and Fong and Schallert (2023/this issue) present a five- question framework on how four theories of motivation and emotion can inform the feedback literature. As can be seen, the feedback field is fruitful in theory and model development, providing new opportunities but also complexity.

The many opportunities and complexity afforded by advances in feedback theory, such as those in this special issue, can and should be integrated to achieve a better understanding of the phenomena. As one example of how to achieve this integration, here I will overview the Message Implementation Student Context and Agent (MISCA) model, which Anastasiya Lipnevich and I (Lipnevich & Panadero, 2021; Panadero & Lipnevich, 2022) developed from a review of fourteen feedback models (See Figure 1). The model characterizes and interrelates five feedback elements integrating specific concepts from other models to provide a big picture of the feedback process. We proposed that the model can be used for two important purposes (Panadero & Lipnevich, 2022). Firstly, to design professional development courses for teachers in which the elements constitute blocks of content. For example, it would be important for teachers to understand the different types of feedback messages and their possible effects, or how students process feedback from cognitive, motivational, and emotional points of view. Second, the MISCA model can serve as a valuable framework for mapping the elements that have been explored in feedback research, integrating their findings, and guiding future directions. With that purpose, I will integrate the articles of the special issue using

MISCA as a framework. This will be done by interpreting how the articles investigate the MISCA elements, which will be presented shortly, and the interactions among the elements.

The feedback message element is the information that is created and delivered to the learner, in a particular form and medium (e.g., written, oral, physical expression). Importantly, the message is highly connected to the other elements. For example, the message varies depending on aspects such as the purpose/function in mind, the audience (students' characteristics), or the context. For more on the interconnections of the elements, see Figure 6 in Panadero and Lipnevich (2022). The implementation element focuses on the purpose of the feedback message and the processes it triggers in the learner (e.g., emotional reaction, processing of the information). The instructional context element refers to the pedagogical setting in which the feedback is created/delivered in combination with the way it is presented (e.g., timing, delivery mode). The agent element represents that feedback can come from different sources (e.g., teacher, peer, self) and that the interactions among them shape feedback and its effects. Lastly, the central aspect of the model is the student and teacher characteristics. Regarding the student, it is central because if the learners do not process the feedback, despite how they might intend to use it, the information has not reached its target. Additionally, the students' characteristics are critical for how the feedback is processed. For example, high achieving students may process feedback in different ways compared to lower- achieving students and, because of this individual difference, these types of students might benefit from different feedback (Brown & Zhao, 2023/this issue). Regarding the role of the teacher, this is an addition to the original MISCA model, so as to visualize the centrality of the teacher characteristics in the feedback process. As Harber (2023/this issue) shows, individual differences among teachers can influence the feedback process greatly. For example, an experienced teacher would be able to tailor feedback to students with different achievement levels whereas a novice teacher may not individualize the feedback as successfully. Finally, the central element of MISCA represents the influence of student and teacher characteristics in the creation, reception and enactment of feedback.

The MISCA model provides a generative framework for the many important contributions in this special issue. For example, Brown and Zhao (2023/this issue), by focusing primarily on self- report instruments for measuring perceptions of feedback, frame their work within the elements of implementation, with a special focus on students' internal processing of feedback, and on agents who make use of feedback (or not). Fong and Schallert (2023/this issue) focus on the implementation element by exploring how the feedback function influences students' internal processing of feedback, while also covering the effects of different agents. Fyfe et al. (2023/this issue) review effects of corrective feedback (message) on academic performance while exploring a number of individual differences (students' characteristics). Harber (2023/this issue) reviews how (mainly) White teachers can

![](images/4ec9a92b002c145e4bf3a91b1e340b10fc17d26afe419b8e27866eaada9b13a8.jpg)  
Figure 1. Ml model w vion. t thi w vion dts th in 1) the ublicions inl in the ot f th  e the ones from this spcl iue; 2) "er has e dd to the crl et; 3) w indvidal dife chacteristics are addd to the ist -i, prous teach ing experience, race/ethnicity- in the central element; and (4) "creation" was added to the central element.

positively bias their feedback (message) to ethnic minority students based on their own individual differences (teachers' characteristics e.g., private self- image concerns, internal vs. external motives to suppress prejudice, self- esteem). According to Harber, these teacher- attributes combined with "racial anxiety" affect both the content and the style of interracial feedback and the interactions among teachers and students (agents). He also reports how this positively biased feedback affects students' self- esteem and emotional well- being, among other factors (students' characteristics). Winstone and Nash (2023/this issue) explore in detail the element of context, especially the pedagogical implications of the papers that they reviewed. Through those pedagogical lenses, other elements were touched upon such as implementation, especially internal processing, or aspects of students' characteristics, but with the final intention to provide the pedagogical context to the other elements. In sum, as a whole, this special issue's articles explore all five elements: message (two articles), implementation (three articles), students' and teachers' characteristics (three articles), context (one article), and agents (three articles) showing a balanced representation.

Additionally, the MISCA proposed interactions within the elements. For example, feedback function and internal processing (i.e., the processes learners activate to understand and enact the feedback received) interact, forming the element of implementation. That is exactly one of the aspects explored by Fong and Schallert (2023/this issue); that is, how the purpose of the feedback message influences how learners process such information. MISCA also proposed interactions between elements (see Figure 6 in Panadero & Lipnevich, 2022). For example, the type of message received interacts with the students' characteristics, as different students react differently to the same feedback message (e.g., Fong et al., 2021; Narciss et al., 2014). This interaction was explored by two papers included in the special issue: how the effects of two types of messages (i.e., corrective and positively- biased feedback) interact with a number of students' characteristics (Fyfe et al., 2023/this issue; Harber, 2023/this issue). Thus, feedback research, as a whole, is ripe for theoretical integration and elaboration, with numerous findings that advance the field and, importantly, support the reconceptualization of feedback as a process.

# Feedback as a process: a needed direction for research

One of the key changes in feedback literature has been moving from a static vision (i.e., the teacher produces feedback

and the student receives it) to one in which students actively create, process and enact feedback (e.g., Boud & Molloy, 2013). This change implies that researchers need to investigate feedback as a process, as the focus is now increasingly on both what the students do with the feedback and how the teachers create it. For example, there is research using think- aloud protocols on how students process formative feedback given by the teacher (Manez et al., 2019) or how students create feedback for a peer (Alqassab et al., 2018). Interestingly, all the articles in this special issue treat feedback as a process.

Nevertheless, there is a crucial distinction to make. I believe feedback literature on process data has two levels of research action: a purely theoretical level, and another in which the data collected is actually process data. Regarding the first, scholars conceptualize feedback as a process by pointing out aspects such as the importance of students taking up feedback or teachers providing opportunities for enactment of feedback, but do not collect data on those specific processes. In the second level of research action, scholars use research methods to truly follow the feedback process (e.g., think- aloud protocols, eye tracking) so as to turn the feedback processes into an object of empirical study. These are called "online" measures in the self- regulated learning field, which are defined as "measurements taken concurrent to task performance" (Veenman, 2011, p. 206), in contrast to off- line methods in which the students are asked about future or past performance. A clear advantage of online measurement is its representation of the actual actions and behaviors, thus increasing its validity but usually also the cost of collecting and analyzing the data.

In sum, in the first level of research action, the novel conceptualization of feedback as a process is present, but the research methods, data and operationalization of feedback are often traditional. In the second level the methods, data, and operationalization focus on the exploration of the processes concurrent to feedback creation, delivery, reception and/or enactment. This is key to understanding feedback as a process.

Let me illustrate this with an example from the field of self- assessment/self- feedback. In 2016, Panadero et al. argued that to study self- assessment processes: "research is needed into the psychological aspects of the SSA (student self- assessment) process...[employing] simultaneous data collection about thinking (e.g., think- aloud protocols) and motivational and emotional aspects... the latest advances in the measurement of self- regulated learning... could be applied to the self- assessment field" (p. 824). Following this, Yan and Brown (2017) explored the "black box" of self- assessment by interviewing 17 undergraduate students, asking them to retrospectively explain how they self- assess. This represents the first level of action, as the authors conceptualized self- assessment as a process, but the data collection method was "traditional." Obviously, this data is interesting because it is about a novel area of self- assessment, but the conclusions are based on the participants' retrospective perceptions of how they self- assess, which comes with inherent limitations to our understanding of the processes involved in self- assessment.

Colleagues and I pursued the idea we proposed in 2016, by conceptualizing self- assessment as a process and, consequently, investigating students' self- assessment in real time. In a series of studies, we have used think- aloud protocols (e.g., Panadero et al., 2022) in combination with self- reports, interviews and, in our newest development, eye- tracking and electro- dermal activity. Since the research methods and data are concurrent to students' performance of the task - i.e., self- assessment- , this research belongs to the second level of action. The type of inferences that can be extracted from online data are richer and deeper than what can be inferred from off- line data (Veenman, 2011; Winne & Perry, 2000), thus are more informative regarding the complexity of the feedback process. However, it is important to note that collecting and analyzing process data is usually a more resource- intensive process. This type of research also has a shorter tradition in terms of established methodologies and best practices. Additionally, process data can be based on different data sources (e.g., combining eye- tracking and think- aloud) known as a multimodal approach that requires careful consideration to ensure validity and avoid potential pitfalls (see section 4 of this paper).

How do the papers in this special issue inform us about feedback as a process? Brown and Zhao (2023/this issue) review self- report inventories that are not typically used for collecting process data. Nevertheless, they refer to conceptual ideas of the first level of research such as the links of feedback to internal processing. Fong and Schallert (2023/this issue) review the evidence on the relationship between feedback, motivation, and emotion, referring constantly to the student's internal processes, though they did not employ process data. Future research using Fong and Schallert's framework would be stronger if researchers employ process data techniques (e.g., facial emotion recognition, think- aloud) in order to identify the specific feedback characteristics that trigger emotional and motivational reactions. Fyfe et al. (2023/this issue) used the Five- stage model of feedback processing (Bangert- Drowns et al., 1991), which clearly shows they are portraying the first level of research action but did not discuss process data. It would be highly beneficial for researchers to utilize process data from students at varying developmental stages to examine their responses to feedback in comparable situations, albeit adapted to the students' level of maturity. Such data could be instrumental in identifying the pivotal skills that students develop across their education, which would be necessary for constructing a comprehensive theory of a developmental perspective on feedback. Harber (2023/this issue) also portrays research on interracial feedback delivery as a process in that racial anxiety leads to more positive feedback to ethnic minority students. Importantly, Harber employed process data in previous studies on the validation of the model (Harber, 2004; Harber et al., 2010, 2019), being therefore in the second level. Winstone and Nash (2023/this issue) reach the second level of research action as they wrote a specific section about process data called Outcomes variables and

measures, where it is stated: "...much less time directly measuring what learners actually do when receiving, engaging with, and acting upon feedback" (p. 118).

In sum, all the studies reach the first level of research action as they conceptualize feedback as a process. Additionally, Winstone and Nash (2023/this issue) directly mention the second level on the importance of collecting process data, and Harber (2023/this issue) actually employed process data collected via techniques used in social psychology. I believe this is a representation of current feedback field with most publications reaching the first level but fewer collecting online data and reaching the second level.

# Measurement of feedback effectiveness: a thorny issue where insights from self-regulated learning could help

Because feedback is a complex process, researchers need measurements that can capture that complexity within the process. Actually, there is one paper in this special issue that has directly explored the role of measurement in feedback research through an analysis of the self- report feedback inventories (Brown & Zhao, 2023/this issue). Interestingly, those authors propose that self- regulated learning theory can help researchers to understand feedback effects in classroom instruction. I agree with this stance and my own work has aimed to understand the intricate relationships between educational assessment and self- regulation of learning, along with many others (e.g., Allal, 2020; Andrade & Brookhart, 2020; Nicol & MacFarlane- Dick, 2006).

Based on such a relationship, next I present three reasons why lessons learned from measurement of self- regulated learning can be applied to the feedback literature. First, feedback research is highly dependent on self- report data (Brown & Harris, 2018; Winstone et al., 2017), as was SRL research during its first couple of decades, and that is why the use of self- report has been long and heatedly discussed in SRL research (e.g., Boekaerts & Corno, 2005; Fryer & Dinsmore, 2020). Second, some of the feedback models include self- regulation as a key process (e.g., Butler & Winne, 1995; Hattie & Timmerley, 2007; Nicol & McFarlane- Dick, 2006), thus it makes sense to closely follow advances in SRL research so as to properly measure SRL and feedback processes. And third, the SRL literature has been prolific in implementing new research methods to explore complex learning situations (Karoly et al., 2005; Schunk & Greene, 2018), which aligns with the complexity of feedback episodes. Because of these reasons, it is interesting to explore the development of SRL measurement, that has been claimed to have undergone three waves (Panadero et al., 2016).

The first wave, SRL through self- report lenses, was based solely on the use of self- report. In this phase, the most known SRL instruments were created, such as the Motivated Strategies for Learning Questionnaire (MSLQ) (Pintrich et al., 1993). What could the feedback field learn from the first SRL wave? First, Brown and Zhao (2023/this issue) have shown empirically that researchers need stronger and validated instruments to measure feedback processes. It is crucial to issue a call to improve the psychometric qualities of self- report measures in the field of feedback research. Only with reliable tools used under conditions that guarantee their validity can this type of data help researchers understand how feedback works. Second, our field can learn that general "trait like" instruments, such as those just mentioned, are not well suited for small interventions, as they do not detect fine- grained changes. Thus, researchers also need more specific and temporally relevant self- report measures for measuring small changes produced by feedback (e.g., task specific scales). Third, because self- reports depend exclusively on students' interpretations, these data give "inner" information from the student's point of view, albeit biased, as human beings are imperfect self- assessors by nature (Kruger & Dunning, 1999). Nevertheless, analyzing self- report data can be relevant if it helps with explaining discrepancies between the feedback giver and receiver, or if it helps with understanding the subsequent behavior of a student. Fourth, the SRL community seems to agree that self- report is "just" another research tool with its specific pros and cons (e.g., Fryer & Dinsmore, 2020), similar to any other tool, but can be a powerful method when correctly designed, implemented and validly interpreted, as Brown and Zhao have argued (2023/this issue). Educational feedback seems to still have a great dependency on traditional self- report data (e.g., questionnaires, scales) resulting in a substantial limitation to our understanding, especially as the psychometric properties of the majority of the tools have not been appropriately evaluated (Brown & Harris, 2018). Therefore, as proposed in the previous section, scholars need to move into the second wave that involves process data, which has been occurring in SRL research very strongly for the last twenty- or- so years and, to a lesser extent, for the last decade in feedback research.

The second wave, irruption of online measures, has had a deep impact on SRL research. This wave comes from a simple yet powerful idea: that researchers cannot fully understand the process students undergo unless researchers explicitly study that process (Winne & Perry, 2000). Boekaerts and Corno (2005) published an excellent overview of SRL as a process that included eight different ways to measure SRL - e.g., think- aloud protocols, traces of mental events and processes. From these earlier methods, the field has moved into more technological methods such as eye tracking, electrodermal activity, or facial recognition. Yet the philosophy behind these new technological methods is the same: researchers need to examine the students' learning process as it involves different elements, each with more optimal ways of measuring them. According to my reading, all of the Special Issue articles conceptualize feedback as a process and even some of them present ideas on using process data (Brown & Zhao, 2023/this issue; Winstone & Nash, 2023/this issue) or have reported such data (Harber, 2023/this issue). In that sense all seem to be in the second wave.

What could the feedback field learn from the second SRL wave? First, to understand how feedback works, researchers

need to study the process as it happens, from how feedback is created (e.g., a teacher writing comments), to its reception (e.g., how a student reacts to and processes the feedback from a peer), and enactment (e.g., a student's actions after receiving a rubric- based score with comments) (Panadero & Lipnevich, 2022; Winstone & Nash, 2023/this issue). Most of the online measurement methods aim to track processes directly (e.g., observations of overt behavior, think- aloud protocols, eye tracking). Second, researchers need to measure feedback using different methods as each method offers distinct views. Every data collection method comes with strengths and limitations; thus, third, researchers need to integrate them if they want to use a comprehensive approach (Brown & Zhao, 2023/this issue). For example, self- report questionnaires offer a perspective that can be complemented with think- aloud protocols (more on this in section 4 of this paper). And fourth, a cautionary word, researchers need to use these new methods for the right reasons. The goal is not to use eye tracking for the sake of using it; there must be a logical and well- founded reason to use it for exploring feedback, and for that, comprehensive and detailed theoretical models are crucial. Fortunately, feedback researchers have been doing theory- driven work using these methods in the last decade with a very rapid increase in use over the last few years (e.g., Cutumisu et al., 2019; Gregory et al., 2020).

Lastly, the third wave, a new conceptualization of SRL measurement "intervention + assessment," occurs when the measurement tool itself changes the actions that researchers are intending to measure. For instance, consider a study where students are required to maintain a learning diary on a daily basis, documenting their self- regulatory strategies. Interestingly, the act of completing the diary itself has a positive influence on the students, making them more conscious of their own strategies (Schmitz et al., 2011). Here, the measurement tool serves a dual purpose as both an evaluation and an intervention. This notion is known as the reactivity effect, which refers to the changes that individuals experience when they become consciously aware of specific aspects of their behavior through metacognitive monitoring (Zimmerman, 2002). The assessment/measurement tool then has a twofold effect: to measure and to enhance the dependent variable. Notably, this effect can occur unintentionally, even without the researcher's awareness, highlighting the significance for researchers to identify this dual impact. Thus, it is crucial to be mindful of this phenomenon to avoid drawing erroneous conclusions based on the data collected (Panadero et al., 2016). From my reading, none of the papers in this special issue have reflected about this third wave.

What could the feedback field learn from the third SRL wave? First, researchers should be aware that feedback can be their intervention but also their dependent variable. In educational contexts, feedback is often utilized as an intervention to modify learners' processes. Simultaneously, it can also be the process under observation by the researcher. For example, imagine a hypothetical study similar to the one described above, in which the students need to create a feedback diary of the assessment they deliver weekly on a peer's work. Over time the creation of such feedback should improve the quality of the feedback through deliberate practice. In addition, the researcher might then take that diary data and analyze it to follow the progression of students' feedback literacy. Here feedback acts as both intervention and dependent variable, and requires a different handling than do data from studies in which the intervention and the dependent variables are distinct. Second, researchers should be aware that, though feedback is often intended as an intervention, it is not always effective in producing the intended changes (Kluger & DeNisi, 1996). Hence, it is imperative to prioritize the collection of data beyond feedback itself in order to examine whether changes or the absence thereof in students' feedback progression are manifested in other variables, such as academic performance or self- regulated learning strategies. This sense of urgency arises from the need to gain a comprehensive understanding of the broader impact and implications of feedback on various aspects of student development and learning. This is called a multimodal approach that can allow for triangulation of data, strengthening the conclusions that can be extracted (see next section). Furthermore, researchers should formulate a hypothesis regarding the potential impact of the feedback they provide to students, to ensure accurate identification and measurement of those effects. Imagine that the feedback is supposed to improve students' learning strategies, then a measurement of such strategies should be warranted. And third, all the above points imply that researchers need to carefully plan for "third wave" interventions, because the act of asking students to monitor their feedback processes does not necessarily mean that it has a learning effect. Thus, researchers need to design their study with consideration of the reactivity effect (see Panadero et al., 2016).

# A multimodal approach to the investigation of feedback

The articles in this special issue seem to be in some agreement that, to advance our knowledge on how feedback works, it is important that researchers use different sources of data. For example, Harber's (2023/this issue) model validity has been explored via multimodal data, as described in previous studies (Harber, 2004; Harber et al., 2010). Brown and Zhao (2023/this issue) explored from a psychometric perspective the combination of self- perceptions and observed outcomes or independent measures to obtain a more accurate picture. Previous work by Winstone, Nash et al. applied multimodal data to the investigation of feedback effects combining memory and eye tracking data (Gregory et al., 2020).

Why can multimodal data improve feedback research? Regardless of the type of data, using only one data source provides a skewed perspective of the observed phenomenon. It does not matter if that one data source is self- report or eye- tracking or observation of overt behavior, all methods present a limited view of reality. For that reason, researchers ideally triangulate two or more data sources so as to reach

deeper insights into the phenomenon, in what has been called multimethods (Brewer & Hunter, 1989), multiple methods (Smith, 2006) or, more recently, multimodal methods (Azevedo et al., 2018).

Importantly, this is not a call simply for "more data sources  $=$  better data," as for this to be true a number of conditions need to be met. Among others, there should be a sound understanding of how best to measure each data source, of how the parts may fit together - i.e., triangulationreconciling different data sources, and recognition that at times employing a particular data source may be entirely sufficient for the research question one seeks to address.

Next, I propose that implementing multimodal methods that integrate three types of data sources can have a significant impact, not only for feedback research but for the broader field of educational assessment. Using an array of data collection methods, researchers can obtain data on three types of responses: (a) behavioral indicators: external actions representative of student reactions and processing; (b) physiological reactions: measures of the autonomic nervous system representative of emotional reactions and cognitive load; and (c) subjective experience: self- reported data. Table 1 presents a summary of the different methods. Importantly, some of these more innovative measures have only been applied to feedback research in the last decade, especially in the last five years.

The categories in Table 1 can be used to combine different methods of data collection. For example, if researchers want to measure students' cognitive processing while creating peer feedback, they can use a combination of eye tracking, physiological data, and think- aloud. Or for measuring affective reactions, they can use physiological data, facial emotion recognition, self- reported data, and think- aloud.

# Types of multimodal research

I propose that there are four categories of research in the feedback and assessment literature, but probably applicable to other fields such as SRL. Firstly, unimodal research refers to studies that rely solely on a single data source for evidential support. This is probably the largest bulk of empirical evidence available. A prototypical study here will be a feedback intervention with no control group and with survey data about students' perceptions of the utility of such feedback. There are also recent and innovative studies that can be placed in this category, such as Cutumisu et al. (2019), where eye tracking was the sole method used for data collection of their dependent variable.

Secondly, unimodal research + academic performance entails the utilization of a primary data source to examine feedback processes, supplemented by a measure of the impact of feedback on academic performance. A substantial portion of published research belongs to this category (see Brown & Zhao, 2023/this issue). For example, Arbel et al. (2020) delivered different types of feedback to their participants, while eye tracking their attention allocation, and measuring the changes in their academic performance. Or Alqassab et al. (2018) who conducted a study where participants were instructed to provide peer feedback, while their cognitive processing was evaluated using eye tracking data. Additionally, their performance on a subsequent task—such as proof comprehension- was measured. In my opinion, the combination of any of the methods from Table 1 plus academic performance does not constitute multimodal research, because academic performance is about an outcome (e.g., a score on a test) used to evaluate the intervention effect.

Thirdly, multimodal data without triangulation refers to the utilization of two or more data sources; however, these sources do not offer information about the same processes, making triangulation unfeasible. Typical to this category are studies employing self- reports to investigate either students' perceptions (e.g., feedback utility) or general traits (e.g., goal orientation), plus a process data source such as eye tracking or electrodermal activity, and academic performance (e.g., Berndt et al., 2018). The potential of this type of studies in comparison to the two previous ones is that they usually produce a fuller picture of the phenomenon, though not always integrated.

And fourth, multimodal data with triangulation involves the use of two or more data sources that collectively provide information about the same processes, enabling triangulation for enhanced understanding and validity. To my knowledge, there are few studies that report using multiple data sources in feedback research with such a purpose. Two examples

Table 1. Multimodal data gathering methods for upcoming feedback and assessment research.  

<table><tr><td>Category</td><td>Data collection method</td></tr><tr><td>Behavioral indicators</td><td rowspan="3">Eye-tracking: to identify students&#x27; feedback processing (cognitive patterns, triggers and reactions) researchers could use the following indicators (1) movement measures -e.g., direction, duration, time and order in areas of interest, scanpath-; (2) position measures -e.g., basic measures, position dispersion-; (3) numerosity measures -e.g., fixations, saccades-; and (4) pupil dilation. 
Observation of overt behavior: to identify the students&#x27; process and reactions to feedback, researchers could video record them while receiving feedback and how they engage with it. Later, that video would be analyzed and coded. 
Facial emotion recognition: to identify the students&#x27; emotions when they receive feedback, researchers could measure the students&#x27; facial movements using software based on Facial Action Coding System (Reisenzein et al., 2014). 
Importantly, this data needs to be combined with other sources to ensure validity. 
Academic performance: to identify the feedback effect on students&#x27; achievement. 
Electrodermal activity and Heart rate variability: to identify the students&#x27; emotional and cognitive states. Data needs to be triangulated with eye tracking or think-aloud protocols to identify triggers and type of emotions. 
Think-aloud protocols: to identify students&#x27; cognitive, emotional, and motivational processes while creating or receiving feedback. It can be concurrent or retrospective to the performance of the task. 
Self-report instruments: to identify students&#x27; perspectives on feedback (e.g., receptivity to feedback).</td></tr><tr><td>Physiological reactions</td></tr><tr><td>Subjective experience</td></tr></table>

\*Electrodermal activity (also known as galvanic skin response) is the variation of electrical conductance of the skin representative of changes in the sympathetic nervous system.

were conducted by Vanderhasselt et al. (2015, 2018) who combined eye tracking and heart rate variability over the same stimuli, exploring simple social feedback (positive vs. negative feedback about a picture). Another example would be the work by Nugteren et al. (2018) who combined eye tracking with several forms of self- reported data on the process the students were performing (e.g., judgment of learning, mental effort rating scale), and academic performance. In these studies, two sources of data were employed for direct triangulation, making stronger conclusions about the process possible.

By aiming for more multimodal data in the third and fourth categories, researchers increase their likelihood of addressing intricate questions concerning feedback and assessment processes because the research setting and participants are the same in one multimodal method study. For example, exploring the emotional effects of negatively valenced feedback, more robust conclusions can be drawn by integrating electrodermal activity and concurrent thinkaloud within the same data collection, as opposed to conducting two separate studies using either electrodermal activity or think- aloud alone. However, researchers might first need to conduct two independent studies to establish whether there is a discernable impact (e.g., effect size  $d > .20$  ) before researchers try to integrate different data sources using multimodal methods (G. T. E. Brown, personal communication, April 8, 2023). Nevertheless, my belief is that, in a significant number of feedback research areas there is enough previous empirical knowledge to move into using multimodal methods, in agreement with other articles in this special issue (Brown & Zhao, 2023/this issue).

# Challenges of multimodal research

Unfortunately, multimodal research presents a number of challenges. First, multimodal research can be extremely costly, both in terms of equipment and human resources. Regarding the equipment, hardware such as eye- trackers is expensive and the software (e.g., licenses for eye- tracking, facial recognition) can be more expensive than the hardware. In terms of human resources, it typically requires much more work than multimodal studies, especially if process data is gathered (Creswell & Plano- Clark, 2018). The researchers need training in the new methods, equipment and software that might require specific data cleaning and analysis, etc. For those researchers with fewer resources, they can achieve great insights with "low budget" multimodal data collection methods. For example, direct observation combined with think- aloud protocols can offer crucial insights when used systematically. However, process data is almost always more costly in terms of working hours than is self- reported inventory data, but only the former can provide accurate information about the processes in which the student was engaged.

Second, triangulation of different data sources is complex as it implies integrating distinct sources of information that are usually not directly connected (e.g., electro- dermal activity and think- aloud, eye tracking and questionnaire data). As put by Smith (2006) " triangulation attempts to confirm inferences made from the findings of several research methods and approaches. However, triangulation is less a method than a troublesome metaphor" (p. 465). Although there is a tradition for multimodal methods in educational psychology (Brewer & Hunter, 1989), triangulation has been less implemented in education (Harris & Brown, 2010). The probability that different methods can actually map onto or triangulate each other is lower than might be expected (Brown & Harris, 2018), but it is achievable. One of the key tensions comes from data sources misalignments that some researchers interpret as problems when the sources tell different stories (Azevedo et al., 2018), whereas others might interpret this as the power of separate methods telling their own stories (Harris & Brown, 2010).

What would be some recommendations for the researcher looking to triangulate data? First, it is important to establish a specific plan before starting data collection on the following questions: what methods will be used, why those methods, and how will you use them? Second, the researcher should hypothesize the type of triangulation and the expected directions of the data, but be prepared to change the triangulation approach if needed - i.e., convergent, explanatory or exploratory (Creswell & Plano- Clark, 2018).

Third, multimodal researchers should be prepared for the possibility of running into publication problems because of word- length limits and "radicalism attacks." Length limitations pose a challenge as many journals impose relatively short word limits for reporting empirical work. However, it is crucial for researchers to maintain precision in describing the methodological details of their work to ensure rigor and replicability. This is even more pressing for multimodal researchers as they have to report multiple data sources, analyses, and triangulation. Anecdotally, it is not uncommon for multimodal researchers to publish separate papers, with each paper focusing on a specific data source, as a means to adhere to journal length limits and address reviewers' requests. In that sense, it might be positive to distribute detailed manuals of the measurement process on open repositories such as this example: https://osf.io/4zpts/. Regarding "radicalism attacks," when the multimodal researcher opts for a mixed method approach, reviewers may sometimes critique the work for not complying with traditional quantitative or qualitative standards (Smith, 2006). To avoid such critique, the need for each method should be clearly established by the researcher including a clear plan for data integration.

In sum, multimodal methods could improve our understanding of feedback effects and processes, precisely because these methods tend to involve process data. However, multimodal methods present challenges that need to be considered and planned for. Feedback and assessment researchers should be open to multimodal data which may necessitate changes in our approach to publishing.

# The feedback actor as the central element: the effects of individual differences and more

All the papers in this special issue refer to the importance of exploring individual differences, showing this is a key area

of research. Fyfe et al. (2023/this issue) dedicated a specific section to the Moderating role of individual learner characteristics. They noted that over half of the feedback studies they reviewed investigated at least one individual difference, with a primary focus on prior knowledge or experience, as well as factors related to age or grade. Harber (2023/this issue) investigated how individual differences among teachers influenced the content and delivery of feedback, specifically focusing on the presence of a positive feedback bias. Fong and Schallert (2023/this issue), although they did not explicitly employ the term "individual differences" or refer to students' characteristics, examined how students' responses and implementation of feedback can be elucidated by four theories of motivation and emotion. This framework essentially provides insights into how individual differences influence the reception and application of feedback. Winstone and Nash (2023/this issue), in their review of 42 papers exploring key priorities and recommendations for future feedback research, identified individual differences as one of the prominent themes addressed in the literature. These individual differences were of three types: demographic, socio- motivational, and academic. Finally, Brown and Zhao (2023/this issue) referred to "recipient characteristics" relating to students. Because of their focus on self- report, which is based on the perceptions of the students, Brown and Zhao's paper can be considered as also grounded in the centrality of students to the feedback process. In summary, it is evident that students' characteristics play a central role in all of the papers included in this special issue. Additionally, Harber (2023/this issue) delved extensively into teachers' characteristics, highlighting their significance in the context of feedback.

Interestingly, this conclusion aligns with one of the findings from our review of models in the feedback literature: the role of the student has progressively taken on greater centrality Lipnevich & Panadero,2021;Panadero & Lipnevich, 2022). As discussed there, traditionally, the focus of feedback research has been on external feedback coming mostly from the teacher. The student involvement in assessment has been covered by the work in other areas namely self- assessment and peer assessment. Nevertheless, there has been a notable shift in educational research on feedback, exemplified by the articles in this special issue, which highlights the importance of considering the student as a central figure even when feedback is provided by the teacher. Examples of that shift are the work on feedback literacy Carless & Boud, 2018), new definitions of feedback Boud & Molloy, 2013), or students' agentic use of feedback (Winstone et al., 2017). The MISCA conceptualization of the student role can offer an interesting view because it covers three areas: (1) the centrality of the student characteristics, (2) the role of the student as an active feedback agent, and (3) the importance of the feedback implementation. Next, these three elements are explained to further elaborate on the relevance of this topic.

First, MISCA's central element is the students' characteristics, anchored in the idea that individual differences affect feedback creation, reception and agentic use (e.g., Winstone et al., 2017). Thus, educational interventions need to be grounded in a deep understanding of how students receive and implement feedback based on those differences, so as to tailor feedback to the students' specific needs. Whereas there is empirical research exploring these differences, it needs to be further expanded (e.g., Fyfe et al., 2023/this issue). The ultimate goal of putting the student as central in feedback is that, without the learner's processing and enactment, feedback cannot improve the learner (Willem, 2018).

Nevertheless, it is also crucial that the field investigates the creation, delivery and focus of the feedback when the agent is not the learner themselves, in other words, when it is the teacher or the peer who gives feedback. How do the personal characteristics of the peer or teacher creating the feedback influence the content and delivery of the feedback? When we formulated the MISCA our focus was primarily on the student as both the giver and receiver of feedback. However, in the updated version presented here, I have also included teachers' characteristics to underscore their significance (Figure 1). This inclusion is exemplified by Harber's (2023/this issue) model, where the combined impact of a student's race/ethnicity and certain teachers' characteristics (such as private self- image concerns) was explored.

Second, the agent element indicates that feedback can come from different sources, usually three in a classroom: teacher, peer, or self, though it is worth noting that computers can also serve as a relevant source of feedback. This implies that the student will have different roles depending on who provides the feedback: as receiver when given by the teacher, as provider/receiver in peer feedback, and as creator in self- feedback. The feedback field needs research exploring the differential effects of these three agents by integrating what is known from peer and self- assessment studies. Future studies focusing on feedback literacy or the agentic use of feedback could greatly benefit from integrating existing knowledge on topics such as student self- assessment, pedagogical recommendations for implementing self and peer assessment, and the influence of assessment instruments. Incorporating these insights can enhance the understanding and application of feedback in educational settings. An interesting example of that integration are studies from the peer feedback field that compare the effects of teachers' feedback vs. peer feedback (e.g., Paulus, 1999).

And third, the implementation element is crucial for comprehending the internal cognitive processing and affective responses of students toward feedback, particularly when it is provided by an external agent such as a teacher or a peer. Understanding these mechanisms sheds light on how feedback is assimilated and acted upon by students. In regulated learning theory, the role of others has been explored in models of co- regulation, that have started to be used in educational assessment (e.g., Allal, 2020; Andrade & Brookhart, 2020; Greene, 2020). However, further research is warranted due to the limitations of the current empirical research conducted, such as reliance on self- reports, which may not provide robust indicators of students' cognitive processes and affective reactions. This reinforces the importance of my earlier suggestion for incorporating process data, utilizing

advancements in self- regulated learning measurement, and integrating multimodal data to gain deeper insights into the feedback processes.

# Conclusion

Feedback researchers are actively developing theoretical models and refining empirical methods to enhance their understanding of the effects and effectiveness of feedback. I started by proposing that such changes potentially constitute a paradigm shift and asserted that there are five steps currently underway or that could contribute to this shift. The first step would involve employing comprehensive models that effectively capture the intricate nature of feedback and operationalize its specific elements for in- depth investigation. The second step would require researchers to collect process data to gain a deeper understanding of the actual actions and behaviors of students and teachers in feedback scenarios. The third step would involve incorporating concepts from the field of self- regulated learning, particularly in measuring the impacts of feedback, while not limiting to this area exclusively. In the fourth step, researchers would employ multimodal methods to gather data from different data sources and triangulating them so as to understand the multifaceted aspects of feedback. The fifth step would require the design of research endeavors that recognize the pivotal role of students and teachers' actions, particularly considering their characteristics and individual differences. The sequence in which these steps are presented should not be construed as a strict order to be followed; rather, these steps should be pursued simultaneously, not consecutively.

This might seem an overwhelming agenda but, as the papers in this special issue show, the field is already well on its way in these five steps. Returning to the aim of this special issue, the approaches discussed in the papers significantly enhance our ability to generate compelling insights into the "black box" of feedback, paving the way for valuable discoveries in the years to come. In my opinion, two areas show special promise: the central role of the student (i.e., self- feedback) and how students process, react to, and enact the feedback they receive and generate. Let's get to it.

# Acknowledgments

This manuscript has received considerable amounts of feedback from colleagues. Thanks to the following colleagues that commented and even edited the full manuscript: Phil Winne (1st version), Anders Jonsson (2nd version), Matthew Fuller- Tyszkiewicz (3rd version), and Gavin Brown (3rd version) and extensive help with editing the text. For providing very detailed feedback and extensive edits on the versions of the manuscript, to the editors of the special issue Naomi Winstone and Robert Nash. Special thanks to the reviewer that challenged me to "push harder" giving excellent ideas on how to improve the paper. Thanks to the authors of the papers in the special issue for helping me interpret their work, apart from the already mentioned: Emily Fyfe, Carlton Fong, and Kent Harber who triggered very interesting conversations. Finally, thanks to Javier Fernandez for editing the MISCA model figure.

# Disclosure statement

Responsible use of AI Tools in Academic Content Creation: In the third resubmission of this manuscript, I used Chat GPT 3.5 in sixteen occasions to edit lengthy sentences. I did not use it to create new content. In case interested, I kept a list with all the edits.

# Funding

FundingSpanish Ministry of Economy and Competitiveness (Ministerio de Economía y Competitividad) National I+D Call (Convocatoria Excelencia) project reference PID2019- 108982GB- 100. Funded by contribution of the Basque Government [Ref. IT1624- 22] to the group Education Regulated Learning and Assessment.

# ORCID

ORCIDErnesto Panadero  $\oplus$  http://orcid.org/0000- 0003- 0859- 3616

# References

ReferencesAllal, L. (2020). Assessment and the co- regulation of learning in the classroom. Assessment in Education, 27(4), 332- 349. https://doi.org/10.1080/0969594X.2019.1609411Alqassab, M., Strijbos, J. W., & Ufer, S. (2018). The impact of peer solution quality on peer- feedback provision on geometry proofs: Evidence from eye- movement analysis. Learning and Instruction, 58, 182- 192. https://doi.org/10.1016/j.learninfocuc.2018.07.003Andrade, H. L., & Brookhart, S. M. (2020). Classroom assessment as the co- regulation of learning. Assessment in Education, 27(4), 350- 372. https://doi.org/10.1080/0969594X.2019.1571992Arbel, Y., Feeley, E., & He, X. (2020). The effect of feedback on attention allocation in category learning: An eye tracking study. Frontiers in Psychology, 11(3114), 559334. https://doi.org/10.3389/fpsyg.2020.559334Azevedo, R., Taub, M., & Mudrick, N. V. (2018). Understanding and reasoning about real- time cognitive, affective, and metacognitive processes to foster self- regulation with advanced learning technologies. In D. H. Schunk & J. A. Greene (Eds.), Handbook of self- regulation of learning and performance (pp. 254- 270). Routledge.Bangert- Drowns, R. L., Kulik, C. L. C., Kulik, J. A., & Morgan, M. T. (1991). The instructional effect of feedback in test- like events. Review of Educational Research, 61(2), 213- 238. https://doi.org/10.3102/00346543061002213Berndt, M., Strijbos, J. W., & Fischer, F. (2018). Effects of written peer- feedback content and sender's competence on perceptions, performance, and mindful cognitive processing. European Journal of Psychology of Education, 33(1), 31- 40. https://doi.org/10.1007/s10212- 017- 0343- zBoekaerts, M., & Corno, L. (2005). Self- regulation in the classroom: A perspective on assessment and intervention. Applied Psychology, 54(2), 199- 231. https://doi.org/10.1111/j.1464- 0597.2005.00205. xBoud, D., & Molloy, E. (2013). Rethinking models of feedback for learning: The challenge of design. Assessment & Evaluation in Higher Education, 38(6), 698- 712. https://doi.org/10.1080/02602938.2012.691462Brewer, J., & Hunter, A. (1989). Multimethod research: A synthesis of styles. SAGE.Brown, G. T. L., & Harris, L. R. (2018). Methods in feedback research. In A. A. Lipnevich & J. K. Smith (Eds.), The Cambridge handbook of instructional feedback. Cambridge University Press.Brown, Zhao. (2023). In defence of psychometric measurement: a systematic review of contemporary self- report feedback inventories. Educational Psychologist, 58(3), 178- 192. https://doi.org/10.1080/00461520.2023.2208670Butler, D. L., & Winne, P. H. (1995). Feedback and self- regulated learning: A theoretical synthesis. Review of Educational Research, 65(3), 245- 281. https://doi.org/10.3102/00461543065003245

Carless, D., & Boud, D. (2018). The development of student feedback literacy: Enabling uptake of feedback. Assessment & Evaluation in Higher Education, 43(8), 1315- 1325. https://doi.org/10.1080/02602938.2018.1463554Creswell, J. W., & Plano- Clark, V. L. (2018). Designing and conducting mixed methods research (3rd ed.). SAGE.Cutumisu, M., Turgeon, K. L., Saiyera, T., Chuong, S., Gonzalez Esparza, L. M., MacDonald, R., & Kokhan, V. (2019). Eye tracking the feedback assigned to undergraduate students in a digital assessment game. Frontiers in Psychology, 10, 1931. https://doi.org/10.3389/fpsyg.2019.01931Educational Endowment Foundation. (2018). Teaching & Learning Toolkit. https://educationendowmentfoundation.org.uk/public/files/Toolkit/complete/EEF- Teaching- Learning- Toolkit- October- 2018. pdfEtelapetoba, A., Kykyria, V. I., Penttonella, M., Holkkaa, P., Palomelma, S., Vahasantanena, K., Etelapetob, T., & Lappalainenb, V. (2018). A multi- componential methodology for exploring emotions in learning: Using self- reports, behaviour registration, and physiological indicators as complementary data. Frontline Learning Research, 6(3), 6- 36. https://doi.org/10.14786/flr.v6i3.379Fong, C. J., & Schallert, D. L. (2023). "Feedback to the future": Advancing motivational and emotional perspectives in feedback research. Educational Psychologist, 1- 16. https://doi.org/10.1080/00461520.2022.2134135Fong, C. J., Schallert, D. L., Williams, K. M., Williamson, Z. H., Lin, S., Kim, Y. W., & Chen, L. H. (2021). Making feedback constructive: The interplay of undergraduates' motivation with perceptions of feedback specificity and friendliness. Educational Psychology, 41(10), 1241- 1259. https://doi.org/10.1080/01443410.2021.1951671Fryer, L. K., & Dinsmore, D. J. (2020). The promise and Pitfalls of self- report. Frontline Learning Research, 8(3), 1- 9. https://doi.org/10.14786/flr.v8i3.623Fyfe, E. R., Borriello, G. A., & Merrick, M. (2022). A developmental perspective on feedback: How corrective feedback influences children's literacy, mathematics, and problem solving. Educational Psychologist, 1- 16. https://doi.org/10.1080/00461520.2022.2108426Greene, J. A. (2020). Building upon synergies among self- regulated learning and formative assessment research and practice. Assessment in Education, 27(4), 463- 476. https://doi.org/10.1080/0969594X.2020.1802225Greene, J. A. (2022). What can educational psychology learn from, and contribute to, theory development scholarship? Educational Psychology Review, 34(4), 3011- 3035. https://doi.org/10.1007/s10648- 022- 09682- 5Gregory, S. E. A., Winstone, N. E., Ridout, N., & Nash, R. A. (2020). Weak memory for future- oriented feedback: Investigating the roles of attention and improvement focus. Memory, 28(2), 216- 236. https://doi.org/10.1080/09658211.2019.1709507Harber, K. D. (2004). The positive feedback bias as a response to outgroup unfriendliness. Journal of Applied Social Psychology, 34(11), 2272- 2297. https://doi.org/10.1111/j.1559- 1816.2004. tb01977. xHarber, K. D. (2023). The model of threat- infused intergroup feedback: Why, when, and how feedback to ethnic minority learners is positively biased. Educational Psychologist, 2023, 1- 16. https://doi.org/10.1080/00461520.2023.2170377Harber, K. D., Reeves, S., Gorman, J. L., Williams, C. H., Malin, J., & Pennebaker, J. W. (2019). The conflicted language of interracial feedback. Journal of Educational Psychology, 111(7), 1220- 1242. https://doi.org/10.1037/edu0000326Harber, K. D., Stafford, R., & Kennedy, K. A. (2010). The positive feedback bias as a response to self- image threat. The British Journal of Social Psychology, 49(Pt 1), 207- 218. https://doi.org/10.1348/014466609X473956Harris, L. R., & Brown, G. T. L. (2010). Mixing interview and questionnaire methods: Practical problems in aligning data. Practical Assessment, Research & Evaluation, 15(1). http://pareonline.net/getvn.asp?v=15&n=1Hattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational Research, 77(1), 81- 112. https://doi.org/10.3102/003465430298487

Jonsson, A. (2013). Facilitating productive use of feedback in higher education. Active Learning in Higher Education, 14(1), 63- 76. https://doi.org/10.1177/1469787412467125Karoly, P., Boekaerts, M., & Maes, S. (2005). Toward consensus in the psychology of self- regulation: How far have we come? How far do we have yet to travel? Applied Psychology- an International Review- Psychologie Appliquee- Revue Internationale, 54(2), 300- 311. https://doi.org/10.1111/j.1464- 0597.2005.00211. xKluger, A. N., & DeNisi, A. (1996). The effects of feedback interventions on performance: A historical review, a meta- analysis, and a preliminary feedback intervention theory. Psychological Bulletin, 119(2), 254- 284. https://doi.org/10.1037/0033- 2909.119.2.254Kruger, J., & Dunning, D. (1999). Unskilled and unaware of it: How difficulties in recognizing one's own incompetence lead to inflated self- assessments. Journal of Personality and Social Psychology, 77(6), 1121- 1134. https://doi.org/10.1037/0022- 3514.77.6.1121Lipnevich, A. A., & Panadero, E. (2021). A review of feedback models and theories: Descriptions, definitions, and conclusions. Frontiers in Education, 6(481), 720195. https://doi.org/10.3389/feduc.2021.720195Lui, A. M., & Andrade, H. L. (2022). The Next Black Box of formative assessment: A model of the internal mechanisms of feedback processing [Hypothesis and Theory]. Frontiers in Education, 7, 51548. https://doi.org/10.3389/feduc.2022.751548Máñez, I., Vidal- Abarca, E., Kendeou, P., & Martínez, T. (2019). How do students process complex formative feedback in question- answering tasks? A think- aloud study. Metacognition and Learning, 14(1), 65- 87. https://doi.org/10.1007/s11409- 019- 09192- wNarciss, S., Sosnovsky, S., Schnaubert, L., Andrès, E., Eichelmann, A., Goguadze, G., & Melis, E. (2014). Exploring feedback and student characteristics relevant for personalizing feedback strategies. Computers & Education, 71, 56- 76. https://doi.org/10.1016/j.compedu.2013.09.011Nicol, D. J., & Macfarlane- Dick, D. (2006). Formative assessment and self- regulated learning: A model and seven principles of good feedback practice. Studies in Higher Education, 31(2), 199- 218. https://doi.org/10.1080/03075070600572090Nugteren, M. L., Jarodzka, H., Kester, L., & Van Merriënboer, J. J. G. (2018). Self- regulation of secondary school students: Self- assessments are inaccurate and insufficiently used for learning- task selection. Instructional Science, 46(3), 357- 381. https://doi.org/10.1007/s11251- 018- 9448- 2Panadero, E., Brown, G. T. L., & Strijbos, J. W. (2016). The future of student self- assessment: A review of known unknowns and potential directions. Educational Psychology Review, 28(4), 803- 830. https://doi.org/10.1007/s10648- 015- 9350- 2Panadero, E., García- Pérez, D., Fernández- Ruiz, J., Fraile, J., Sánchez- Iglesias, I., & Brown, G. T. L. (2022). University students' strategies and criteria during self- assessment: Instructor's feedback, rubrics, and year level effects. European Journal of Psychology of Education, https://doi.org/10.1007/s10212- 022- 00639- 4Panadero, E., Klug, J., & Järvelä, S. (2016). Third wave of measurement in the self- regulated learning field: When measurement and intervention come hand in hand. Scandinavian Journal of Educational Research, 60(6), 723- 735. https://doi.org/10.1080/00313831.2015.1066436Panadero, E., & Lipnevich, A. A. (2022). A review of feedback typologies and models: Towards an integrative model of feedback elements. Educational Research Review, 33, 100416. https://doi.org/10.1016/j.edurev.2021.100416Paulus, T. M. (1999). The effect of peer and teacher feedback on student writing. Journal of Second Language Writing, 8(3), 265- 289. https://doi.org/10.1016/S1060- 3743(99)80117- 9Pintrich, P. R., Smith, D. A. F., Garcia, T., & Mckeachie, W. J. (1993). Reliability and predictive validity of the Motivated Strategies for Learning Questionnaire (MSLQ). Educational and Psychological Measurement, 53(3), 801- 813. https://doi.org/10.1177/0013164493053003024Rosenzein, R., Junge, M., Studtmann, M., & Huber, O. (2014). Observational approaches to the measurement of emotions. In R.

Pekrun & L. Linnenbrink- Garcia (Eds.), International handbook of emotions in education (pp. 580- 606). Routledge. Schmitz, B., Klug, J., & Schmidt, M. (2011). Assessing self- regulated learning using diary measures with university students. In B. J. Zimmerman & D. H. Schunk (Eds.), Handbook of self- regulation of learning and performance. (pp. 251- 266). Routledge. Schunk, D. H., & Greene, J. A. (2018). Handbook of Self- Regulation of Learning and Performance. Routledge. Smith, M. L. (2006). Multiple methodology in education research. Handbook of Complementary Methods in Education Research, 457- 475. Vanderhasselt, M. A., De Raedt, R., Nasso, S., Puttevils, L., & Mueller, S. C. (2018). Don't judge me: Psychophysiological evidence of gender differences to social evaluative feedback. Biological Psychology, 135, 29- 35. https://doi.org/10.1016/j.biopsych.2018.02.017 Vanderhasselt, M. A., Remue, J., Ng, K. K., Mueller, S. C., & De Raedt, R. (2015). The regulation of positive and negative social feedback: A psychophysiological study. Cognitive, Affective & Behavioral Neuroscience, 15(3), 553- 563. https://doi.org/10.3758/s13415- 015- 0345- 8 Veenman, M. (2011). Alternative assessment of strategy use with self- report instruments: A discussion. Metacognition and Learning, 6(2), 205- 211. https://doi.org/10.1007/s11409- 011- 9080- x

Wiliam, D. (2018). Feedback: At the heart of - but definitely not all of- formative assessment. In A. A. Lipnevich & J. K. Smith (Eds.), The Cambridge handbook of instructional feedback (pp. 3- 28). Cambridge University Press. Winne, P. H., & Perry, N. E. (2000). Measuring self- regulated learning. In M. Boekaerts, P. R. Pintrich, & M. Zeidner (Eds.), Handbook of self- regulation (pp. 531- 566). Academic Press. Winstone, N. E., & Nash, R. A. (2023). Toward a cohesive psychological science of effective feedback. Educational Psychologist, 58(3), 111- 129. Winstone, N. E., Nash, R. A., Parker, M., & Rowntree, J. (2017). Supporting learners' agentic engagement with feedback: A systematic review and a taxonomy of recurrence processes. Educational Psychologist, 52(1), 17- 37. https://doi.org/10.1080/00461520.2016.1207538 Yan, Z., & Brown, G. T. L. (2017). A clinical self- assessment process: Towards a model of how students engage in self- assessment. Assessment & Evaluation in Higher Education, 42(8), 1247- 1262. https://doi.org/10.1080/02602938.2016.1260091 Zimmerman, B. J. (2002). Becoming a self- regulated learner: An overview. Theory into Practice, 41(2), 64- 70. https://doi.org/10.1207/s15430421tip4102_2