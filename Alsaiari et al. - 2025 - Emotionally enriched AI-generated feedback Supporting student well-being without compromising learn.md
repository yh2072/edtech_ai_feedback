# Emotionally enriched AI-generated feedback: Supporting student well-being without compromising learning

Omar Alsaiari  $^{a,b}$ , Nilufar Baghaei  $^{a}$ , Hatim Lahza  $^{c}$ , Jason M. Lodge  $^{d}$ , Marie Boden  $^{a}$ , Hassan Khosravi  $^{e}$

$^{a}$  School of Electrical Engineering and Computer Science, The University of Queensland, St Lucia, Brisbane, 4072, QLD, Australia   $^{b}$  Department of Computer Science, College of Science and Arts, Najran University, Sharurah, Najran, 68341, Saudi Arabia   $^{c}$  Department of Computer Science, Umm Al- Qura University, Makkah, Makkah al- Mukarramah, Saudi Arabia   $^{d}$  School of Education, The University of Queensland, St Lucia, Brisbane, 4072, QLD, Australia   $^{e}$  Institute for Teaching and Learning Innovation, The University of Queensland, St Lucia, Brisbane, 4072, QLD, Australia

# ARTICLE INFO

# ABSTRACT

Keywords:  Generative AI  Feedback  Emotional responses  Student engagement  Control- value theory

The use of AI- generated feedback in higher education has received growing attention, with most existing research emphasizing its accuracy, usefulness in improving student work, and scalability. However, little attention has been paid to the role of emotional cues such as encouragement, praise, and empathetic language in shaping how students perceive and respond to feedback. This study addresses this gap by investigating whether enriching AI- generated feedback with motivational language influences students' emotional responses and engagement. Drawing on the Control- Value Theory of Achievement Emotions, we conducted a randomized controlled experiment involving 395 participants, in which the experimental group received AI feedback enhanced with motivational elements, while the control group received neutral feedback. Our results show that the enriched feedback was perceived as more helpful and significantly reduced negative emotions — particularly anger — towards receiving feedback. However, it did not significantly affect students' engagement with the feedback or the quality of their revised work. These findings highlight the potential of emotionally enriched AI feedback to foster more supportive and emotionally attuned learning environments without compromising learning outcomes, and underscore the importance of designing affective feedback systems that balance emotional well- being with sustained improvements in performance.

# 1. Introduction

The importance of feedback in learning has been widely acknowledged, with much research focusing on identifying criteria for effective feedback, including being detailed, actionable, and constructive (Hattie & Timperley, 2007; Ryan et al., 2024; Wisniewski et al., 2020). Feedback processes also go far beyond the mere provision of information. Jensen et al. (2023) introduce the notion of "feedback encounters" to emphasize that feedback is not a simple, one- directional transmission of information from instructors to students. Instead, it is a dynamic, relational, and context- dependent process unfolding through multiple interactions. By focusing on these encounters, they underscore how both instructors and learners co- construct meaning, with emotional and social factors

significantly shaping the way feedback is perceived and acted upon, thereby encapsulating the complex nature of feedback in higher education settings.High- quality feedback is therefore critical, but also inherently complicated.

A key challenge in digital learning environments is the loss of emotional cues that are essential to effective feedback encounters. In physical settings, facial expressions, tone, and gestures help convey intent and foster emotional engagement, influencing how students process and act on feedback (Pong & Schallert, 2023). In contrast, automated feedback systems often struggle to replicate this emotional nuance, relying on rigid templates that may feel impersonal and disconnected from student experiences (Ranalli, 2021). The absence of these affective elements can weaken trust and motivation, thereby reducing the overall effectiveness of feedback (Grawemeyer et al., 2022). While human feedback can adapt to individual needs, it is also subject to variability due to instructors' moods, biases, and time constraints, which can lead to inconsistencies in quality (Brickett et al., 2013; Stough & Emmer, 1998). In such contexts, feedback encounters can become transactional, falling short of contemporary expectations that emphasize relational engagement and multifaceted processes like student sensemaking and planning (Boud & Molloy, 2013). These limitations highlight the need for scalable, AI- driven feedback that maintains emotional sensitivity while ensuring consistency and engagement in digital learning environments.

The control- value theory (CVT) of achievement emotions (Pekrun et al., 2007) provides a robust framework for understanding the role of emotion in feedback processes. Positive emotions, such as enjoyment and hope, enhance motivation and academic success, while negative emotions, like anger and anxiety, can hinder learning outcomes. By incorporating emotional elements such as encouragement, praise, and empathetic language into feedback encounters facilitated by AI, we hypothesize that students will experience reduced negative emotions and increased engagement. This study leverages emerging AI technologies to address challenges in engaging students with effective feedback in digital settings.

This study aims to investigate the potential of generative AI to enhance engagement with feedback processes through emotional enrichment. Our research is grounded in the CVT of achievement emotions, which posits that students' academic emotions are influenced by their perceived control over learning activities and the value they attach to these activities. We conducted a field- based randomized controlled experiment in a first- year web design course with 381 actively participating students. In this course, students engaged in co- creation by developing study resources (e.g., multiple- choice questions) and providing peer feedback on each other's creations. Large language models (LLMs) were used to deliver instant feedback on both the students' content and their peer evaluations. The control group received feedback in a neutral tone, while the experimental group received feedback generated from the same prompt, with the LLM instructed to incorporate positive elements including praise, encouragement, and visual cues in the form of emojis, in an overall uplifting tone. The impact of this intervention will be evaluated through a mixed- methods approach, assessing measures including student engagement with feedback, perceived feedback helpfulness, emotional responses, and the quality of their work.

Distinct from prior studies that have focused either on the efficiency of automated feedback or on the qualitative nuances of human- provided feedback, our research makes several novel contributions to the existing body of knowledge. First, we integrate generative AI with emotional enrichment strategies, thereby bridging the gap between the consistency of automated systems and the affective depth of human interaction. Second, our thorough field- based randomized controlled design in a real- world educational setting provides robust, empirical evidence of how emotionally enriched AI feedback influences both students' perception and emotional well- being. Third, by employing a mixed- methods evaluation, our study offers comprehensive insights into the cognitive, behavioral, and emotional dimensions of feedback encounters advancing theoretical understanding based on the CVT of achievement emotions and informing practical applications in educational technology design.

To investigate these contributions, our research is guided by the following research questions:

- RQ1. Engagement with Feedback: What is the impact of AI emotionally enriched feedback on student engagement with feedback processes? A critical challenge with feedback is that students often do not engage with it. Investigating the extent to which students engage with feedback is crucial because it highlights the effectiveness of the feedback in fostering active learning and participation, which are key to academic success (Winstone et al., 2017). Here we explore whether the experimental group would demonstrate more engagement with the provided feedback compared to the control group.- RQ2. Perceived Helpfulness: How do students perceive the helpfulness of emotionally enriched AI feedback? Understanding students' perceptions of the feedback is essential, as it influences their motivation to act on it. If students find the feedback valuable, they are more likely to incorporate it into their learning process (Gentlem & Smith, 2013). Here we explore whether the experimental group will perceive the AI-based feedback to be more helpful compared to the control group.- RQ3. Emotional response to feedback: What is the effect of AI emotionally enriched feedback on students' emotional responses to feedback? The emotional response to feedback can significantly impact students' attitudes towards learning and their willingness to engage with educational content. Exploring the emotional responses to feedback is essential as it sheds light on how feedback influences students' emotional well-being and learning engagement (D'Errico et al., 2016). Here we explore how emotionally enriched AI feedback will impact students' positive emotions such as (a) enjoyment, (b) hope, and (c) pride, as well as negative emotions such as (a) anger, (b) anxiety, (c) shame, (d) hopelessness, and (e) boredom, compared to students in the control group.- RQ4. Impact on outcome: How does AI emotionally enriched feedback affect the quality of student work? Understanding the impact of feedback on the quality of student work is essential for assessing its overall effectiveness. Here we explore whether students receiving emotionally enriched feedback will produce higher quality resources compared to those receiving neutral feedback (Faulconer et al., 2021).

# 2. Literature review

# 2.1. Artificial Intelligence in Education (AIEd)

Artificial Intelligence (AI) in education has gained considerable attention for its ability to personalize learning, streamline administrative tasks, and provide adaptive feedback (Chen et al., 2020). AI encompasses a range of technologies (e.g., machine learning, natural language processing, neural networks) that collectively offer opportunities to tailor instruction to individual student needs, thereby potentially improving motivation and learning outcomes (Zawacki- Richter et al., 2019).

One promising development is the advent of generative AI, which can deliver highly targeted, instantaneous feedback (Phung et al., 2023). For instance, GPT- 4 has shown the capacity to approach human- level detail and nuance in providing feedback to students in introductory programming tasks, suggesting a shift toward more dialogic and relational feedback (Sharples, 2023). Indeed, AI feedback has been perceived as more thorough than traditional feedback, although over- reliance on AI raises concerns about students' critical thinking and self- regulation (Darvishi et al., 2024). As generative AI tools become more sophisticated, empirical research is essential to establish responsible best practices and to optimize the integration of AI within educational frameworks (Shaik et al., 2022).

# 2.2. Enhancing feedback through AI

Feedback constitutes a collaborative process in which both instructors and learners actively exchange perspectives, with the aim of developing students' evaluative skills and promoting self- regulation (Boud & Molloy, 2013). In online learning, AI- generated feedback can supplement or replace certain instructor- led comments, offering immediacy and personalization at scale (Darvishi et al., 2024). Studies have reported that AI- based feedback may be as or more valuable than human feedback in some contexts, while requiring minimal modification by instructors (Wan & Chen, 2023).

Despite clear advantages, AI feedback also poses challenges. Students may become dependent on automated guidance, potentially diminishing their autonomy and sense of agency (Darvishi et al., 2024). Moreover, although AI feedback can be highly specific, it might not consistently account for the emotional and motivational dimensions essential to sustaining student engagement and initiative. Consequently, careful system design is needed to ensure that feedback not only addresses correctness but also supports students broader learning processes, including the management of emotions and self- regulation (Afzaal et al., 2023). Overall, while AI feedback offers clear benefits, including immediate responses, personalization, and scalability, it is crucial to design these systems carefully to avoid compromising students' independent learning and critical thinking skills.

# 2.3. The Control-Value Theory (CVT) of achievement emotions

The Control- Value Theory (CVT) developed by Pekrun offers a salient lens through which to understand the emotional underpinnings of student engagement (Pekrun, 2006). CVT identifies two central appraisals - control and value - that substantially shape emotional responses in academic contexts. Control reflects students' perceived ability to influence their performance, whereas value signifies the importance or relevance they attach to tasks and outcomes. These appraisals interact to elicit diverse emotions, ranging from positive activating emotions such as enjoyment and hope, to negative activating emotions such as anger or anxiety, and deactivating emotions such as boredom or hopelessness (Pekrun et al., 2004, 2002).

Empirical evidence shows that positive emotions can enhance motivation and academic achievement, while negative emotions can suppress engagement or even impair performance (Fong & Schallert, 2023; Pekrun et al., 2007). However, some emotions (e.g., anxiety) can have complex, dual effects, simultaneously undermining enjoyment and elevating effort. By situating emotional experiences within the dual lenses of control and value, CVT highlights the potential for instructional interventions, such as feedback, to influence students' emotional states in ways that support their learning trajectories.

In this study, the central tenets of CVT inform the design of emotionally enriched AI feedback. Specifically, reinforcing students' sense of control (e.g., through acknowledging effort and praising competence) and emphasizing task value (e.g., by highlighting the real- world significance of assignments) are hypothesized to foster more adaptive emotional responses. This framework thus underlies the decision to integrate explicit emotional cues and motivational statements into AI feedback, aiming to reduce negative emotions and enhance positive ones.

# 2.4. The role of feedback in shaping emotions and academic outcomes

Feedback has a direct impact on students emotional states: positive or constructive critiques can promote feelings of competence and interest, whereas poorly conveyed or invalidated critiques may elicit frustration or shame (Hill et al., 2021; Sargeant et al., 2008). Studies indicate that students emotional reactions often depend less on the content of the feedback itself and more on how they interpret it; a student open to critique, for instance, might view critical feedback as beneficial, whereas another might interpret the same feedback as an affront to ability (Fong et al., 2018).

Recent empirical work by Weidlich et al. (2024) highlights that personalized, automated formative feedback significantly enhances students emotional and motivational responses, with tailored designs fostering more positive reactions compared to traditional feedback approaches. These findings underscore the importance of designing AI feedback systems that account for both cognitive and emotional dimensions of learning.

While individual interpretation plays a role in shaping emotional responses to feedback, it introduces variability that can lead to inconsistent student experiences. Instead of relying on these differences, this study takes a structured approach to systematically enriching AI feedback with emotional cues, ensuring a consistent and intentional impact across learners. By embedding supportive language, explicit praise, and reminders of academic value, the intervention aligns with Control- Value Theory (CVT) to reinforce students' sense of control and motivation in a standardized and scalable manner. This approach fosters positive emotional states (e.g., enjoyment, hope) while reducing negative reactions like frustration or hopelessness, providing a more reliable means of enhancing engagement and learning outcomes (Fredrickson, 2001; Frondozo & Yang, 2023).

Despite increasing evidence on how AI- driven feedback can improve performance, relatively little attention has been directed toward intentional emotional enrichment grounded in CVT. Existing studies frequently emphasize cognitive aspects (e.g., detailed corrections, clarity of guidance) or variations in student interpretation, rather than harnessing a theory- based design to enhance learners' emotional experiences. This gap persists even though emotions demonstrably influence how students process and act on feedback.

Accordingly, this study seeks to address the following gaps: (1) the scarcity of empirical work examining how AI feedback might embed control and value cues to foster beneficial emotional states, (2) the need to move beyond documenting students' varied interpretations toward deliberately creating feedback that modulates emotional responses, and (3) the paucity of controlled evaluations of such emotionally enriched AI feedback in real educational settings. By designing and testing AI feedback that purposefully leverages CVT principles, this research aims to illuminate new pathways for improving not only the quality of feedback but also the emotional and motivational contexts in which feedback is received. In doing so, it contributes a theoretically grounded perspective on how generative AI can be utilized to promote both academic progress and positive emotional engagement.

# 3. Research methods

# 3.1. Research tool: The RiPPLE system

We utilized RiPPLE, an adaptive educational system that employs a constructivist epistemology- driven learnersourcing (Gyamfi et al., 2022; Khosravi et al., 2017, 2019) mechanism to engage students in socio- cognitive and socio- cultural- based learning through three main activities: content authoring, peer review, and drill and practice. Constructivism holds several assumptions; central and common among constructivists is the belief that learners have an active role in constructing their own learning (Sjoberg, 2010). In the initial stage of the learnersourcing mechanism in RiPPLE, students create various learning resources including, but not limited to, worked examples, general notes, and multiple- choice questions (MCQs). This activity takes students through several steps that engage them cognitively with course materials. For example, when creating MCQs, students draft question content, tag relevant topics, generate potential answers and plausible distractors, and formulate explanations and rationales regarding correct and incorrect answers.

Moving forward, the learnersourcing mechanism considers the socio- cultural aspect, such as Vygotsky's theory, which posits that social interaction and environment are essential for cognitive development (Vygotsky, 1978). Social interaction can occur in various forms, such as student- to- peer, student- to- teacher, or student- to- machine (e.g., computer- based scaffolding) (see Nardo (2021)). In RiPPLE, through a machine learning- enhanced peer review process, students, their peers, and instructors collaboratively refine the created learning resources to meet the quality standards provided on the platform. Furthermore, the learnersourcing mechanism also considers the socio- cognitive aspect (Bandura, 1999), which, similar to the socio- cultural perspective, stresses the role of environmental factors as part of a broader reciprocal interaction system that includes the environment, behaviors, cognition, and personal factors (Schunk, 1989). From this viewpoint, learning experiences can take two forms: enactive when performing tasks or vicarious when observing models (Schunk, 1989). In RiPPLE, students engage in vicarious learning when they review their peers' work and when they participate in drill and practice sessions (also seen as retrieval- based learning (Karpicke, 2017)), in which they answer questions or view notes and worked examples created by their peers.

As the learning content authored by students varies in quality, a significant line of research in learnersourcing aims to control the quality of students' contributions. Hence, while RiPPLE does not follow a strict design- based research (DBR) approach, it has several initiatives grounded in various literatures, including crowdsourcing, adaptive learning environments, learning sciences, and educational technology, to improve the moderation mechanism from the perspectives of content creators and content reviewers. An example of a study that aimed to control the quality of content authored by students is Lahiza et al. (2022)'s work, where different self- regulated learning (SRL) scaffolds were provided to content creators to enhance their content quality and engagement. On the other hand, (Gyamfi et al., 2022) conducted a controlled experiment to investigate the effectiveness of incorporating proficiency- based or data- driven rubrics and criteria to improve the quality of written feedback provided by students via the peer review activity. In addition, Darvishi et al. (2022) has investigated the impact of involving instructors through an AI- based spot- checking feature to review only a subset of resources created by students. Moreover, the platform has recently integrated AI feedback. This feedback is currently used before either an authored resource or peer- written feedback is submitted (Fig. 1).

It is important to note that not all learning resources are accepted. Some resources that do not pass an AI- based consensus algorithm are denied. Therefore, learning resources that pass the moderation process are added to course repositories and become available for others in the course to use, attempt, and provide feedback on. Additionally, the platform aims to motivate students to engage more and value their role in the platform by allowing them to rate and comment on any of the learning resources and by assigning them ability ratings based on their performance on the platform, which are presented in a leaderboard (Khosravi et al., 2020).

![](images/2677b0e825f24365c5cf6690c93166b7a752bdd4fdab195fe0d68ad37957ac54.jpg)  
Fig. 1. UML Diagram: RIPPLE's AI Feedback Process and Student Rating Mechanism.

# 3.2. Participants and learning context

Approved by the Human Ethics Committee at The University of Queensland, this study was conducted within an introductory web design course at the same institution. The cohort consisted of 425 students, including 172 international and 253 domestic students from various disciplines, with 404 undergraduates and 21 graduate students. The gender distribution was 321 males and 104 females, with ages ranging from 17 to 56 years  $\mathbf{M} = 20.9$  years). Use of RiPPLE was embedded into the assessment design of the course, contributing  $10\%$  of the final grade through four rounds of tasks and activities. Each round spanned three weeks and included the tasks of creating one resource (e.g., a multiple- choice question, a note, or a flip card), providing peer feedback on five resources, and engaging in drill- and- practice study sessions involving at least 10 resources created by peers. Students primarily accessed RiPPLE remotely, receiving task instructions via lectures and course announcements on Blackboard. Additionally, students received AI- generated feedback, provided by ChatGPT- 3.5, on their created resources or moderations before submission. They were required to click the "Get AI Feedback" button to receive this feedback, after which they had the opportunity to revise their resource or moderation before final submission.

# 3.3. Conditions

In this study, feedback prompts were carefully designed to examine the influence of emotionally enriched feedback on student responses (see Appendix A). The control group received straightforward, constructive feedback with a neutral tone. In contrast, feedback for the experimental group was augmented with motivational phrases, praise, and emojis, aimed at creating a supportive and positive emotional environment (Fig. 2, Table 1). These feedback elements were not merely decorative; emojis were strategically used as visual cues to enhance clarity and foster emotional connections (Bai et al., 2019; Erle et al., 2021). Moreover, the feedback leveraged social norms to set higher standards and promote peer accountability (Caraban et al., 2019), thus nurturing a sense of community and positive social interactions that are essential for increased engagement and mitigating negative emotions (Frondozo & Yang, 2023; Pekrun, 2006).

# 3.4. Experimental settings and data collection strategy

We carried out an in- the- field randomized controlled experiment initially involving 425 participants. Students were randomly allocated to one of the two aforementioned conditions, experiencing either neutral feedback or emotionally enhanced feedback.

Table 1 Comparison of feedback design between control and experimental groups.  

<table><tr><td></td><td>Feedback aspect/section</td><td>Control group</td><td>Experimental group</td></tr><tr><td rowspan="5">Aspects</td><td>Tone</td><td>Neutral</td><td>Enthusiastic and supportive, using positive reinforcements and relevant emojis.</td></tr><tr><td>Content Focus</td><td>Detailed and constructive criticism without emotional elements.</td><td>Similar content focus as control but includes motivational language and praise to enhance engagement.</td></tr><tr><td>Feedback Style</td><td>Emphasis on actionable and specific feedback. Examples: “Avoid using loaded language in the options and explanations.”</td><td>Maintains actionable and specific feedback but with an uplifting and fun tone, incorporating emojis. Examples: “Great job including explanations for each option.”</td></tr><tr><td>Use of Emojis</td><td>None used.</td><td>Strategic use to enhance emotional connection and tone of the feedback.</td></tr><tr><td>Feedback (Word Count)</td><td>Mean = 190.9</td><td>Mean = 278.6</td></tr><tr><td rowspan="5">Sections</td><td>Introduction</td><td>Not applicable</td><td>Friendly Introduction: Engages with a warm and inviting tone, setting a positive stage for feedback. Example: “Hey there! I’m RIPPLE AI, your friendly AI helper, ready to explore and enhance this question with you! 1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789</td></tr><tr><td>Summary</td><td>Summary as a section: 
Example: “The aim of this resource is to assess knowledge about HTML tags for creating hyperlinks.”</td><td>Summary Included in the Introduction: 
Example: “Let&#x27;s evaluate your resource: The question clearly tests HTML attributes related to frame loading.”</td></tr><tr><td>Positive</td><td>Positive: Identifies strengths in the resource. 
Example: “The question is clearly written and addresses a fundamental concept in HTML.”</td><td>Positive: Emphasizes strengths using motivational language and emojis. 
Example: “Your question is clear and directly engages learners with a practical HTML scenario. This hands-on approach can enhance understanding and retention. Awesome job! 1345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901</td></tr><tr><td>Constructive Feedback</td><td>Considerations: 
Example: “Consider rephrasing to ‘Which HTML tag creates hyperlinks?’ for clarity.”</td><td>Suggestions for Improvement: 
Example: “Try rephrasing the question to be more concise, such as ‘Which value for the ‘target’ attribute loads a linked page into the topmost frame in HTML?’</td></tr><tr><td>Last section</td><td>Not applicable</td><td>Acknowledgment of Contribution: Expresses gratitude, encouraging ongoing improvement. 
Example: “Thank you for your diligent work in crafting this educational content. 14567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123</td></tr></table>

In addition, during the platform sign- in stage, students were required to review a consent form to use their data for research and platform development purposes. Only data from students who provided consent were obtained and included in the analysis. Overall, we used data from 405 students across the two groups. Of these, 395 students authored resources, while the remaining 10 students did not author resources but participated in peer feedback moderation tasks. These details are presented in Table 2.

We collected and analyzed data from several sources following these steps:

1. Participant Selection and Group Allocation: Students enrolled in a web design course were randomly allocated to either the experimental or control group within the RIPPLE platform, ensuring a balanced distribution. 
2. Survey Administration: Following the initial three-week period (Round 1), an online survey was made available via Blackboard. Students were also given the option to participate during tutorial sessions, where a fifteen-minute window was allocated for completion. Participation in the survey was entirely voluntary, and students were assured that their decision to complete or not complete the survey would have no impact on their marks. 
3. Data Collection from RIPPLE: At the conclusion of the course, data and log files were collected from the platform.

# 3.5. Data analysis and measures

A mixed- methods approach was employed to examine both quantitative and qualitative responses to AI- generated feedback. Survey data were collected via Qualtrics, and all statistical analyses were performed in R (v. 4.4.1, Vienna, Austria). To analyze the data, we utilized a variety of statistical methods tailored to the nature of our data and the specific research questions. For comparing distributions between the control and experimental groups, particularly when data were non- normally distributed, we employed the Mann- Whitney U test. This non- parametric method was chosen because it does not assume normality, making it ideal

![](images/184fa622db85fe898c33d84e5ec46227d1529e35863b23b767d6f092149d1e44.jpg)  
Fig. 2. Examples of AI-generated feedback provided to the Experimental group (A) and Control group (B) during resource creation.

for our skewed engagement time and rating data. The Mann- Whitney U test provides a robust preliminary assessment of group differences for independent samples, identifying significant variations before applying more complex models, such as Generalized Linear Mixed Models (GLMMs), which account for repeated measures and individual variability, we used (GLMMs). Specifically: for engagement time (RQ1), we applied a Gamma GLMM to model right- skewed continuous data; for perceived helpfulness ratings (RQ2), we used Bayesian ordinal regression (sometimes referred to as an ordinal GLMM in a Bayesian framework) to handle ordinal data; and for resource quality scores (RQ4), we employed a Beta GLMM to model bounded, negatively skewed data. These methods allowed us to capture nuanced effects while accounting for data complexity. For emotional responses (RQ3), which lacked repeated measures, we relied solely on the Mann- Whitney U test.

# 3.5.1. RQ1. Engagement with feedback.

To address RQ1, this study examines how AI- generated feedback influences student engagement on the platform. Engagement time was measured as the duration between students' receipt of feedback and their subsequent submission or moderation of resources, serving as a proxy for reflection and action on the provided feedback.

Data Processing and Cleaning. Engagement times, originally recorded in milliseconds, were converted to seconds for interpretability. To mitigate the influence of extreme values, we applied winsorization at the 5th and 95th percentiles. Instances where students did not interact further with the system (13 data points for creation tasks and 4 for moderation tasks) were excluded from the analysis.

Descriptive and Preliminary Analysis. We conducted both descriptive and inferential statistical analyses. Mean, median, and standard deviation were reported to summarize engagement time across groups in Table 2. Initial data exploration revealed that engagement time was non- normally distributed with a right- skewed distribution. To account for this, we first conducted a Mann- Whitney U test, a non- parametric approach that does not rely on distributional assumptions, to assess differences in engagement between groups. While this test provided an intuitive measure of group differences, it did not account for the repeated measures structure of the data, where individual students contributed multiple engagement instances.

Model- Based Analysis: Gamma Generalized Linear Mixed Model (GLMM). Given the non- normal, right- skewed distribution of engagement time, we applied a Gamma GLMM using the lme4 package in R. Treatment group (control vs. experimental) served as a fixed effect, and student ID as a random intercept accounted for repeated observations per student. This robust method effectively managed skewed, positive- only data and individual variability.

# 3.5.2. RQ2. Perceived helpfulness.

To address RQ2, this study examines students' perceptions of the helpfulness of AI- generated feedback provided during resource creation and moderation tasks on RIPPLE. Perceived helpfulness was measured using a 1- 5 star rating scale, completed immediately

after receiving feedback, supplemented by qualitative comments where students reflected on their experiences. Students assessed feedback helpfulness by responding to the question, "How helpful was this feedback?" and provided additional input through a free- text field prompted by "Please provide feedback on the AI response". Both quantitative ratings and qualitative responses were collected within the same interface (see Fig. 2 for the user interface where responses were submitted, and Fig. 1 for an overview of the entire feedback process).

Data Processing and Cleaning. To ensure data reliability, we removed duplicate or incomplete responses where students failed to provide a rating. The final dataset included 437 perceived helpfulness ratings and 79 qualitative comments, providing a comprehensive view of students' perceptions. The ordinal nature of the rating scale required appropriate statistical modeling to account for the ranking structure of the data.

Descriptive and Preliminary Analysis. We conducted both descriptive and inferential statistical analyses. The mean, median, and standard deviation of perceived helpfulness ratings were reported to summarize overall student perceptions across groups (see Table 2). A preliminary exploration of the data showed that the distribution of ratings was ordinal and non- normally distributed, making traditional linear modeling approaches unsuitable. To account for this, we first conducted a Mann- Whitney U test.

Model- Based Analysis: Bayesian Ordinal Regression. To rigorously address ordinal data with repeated measures, Bayesian ordinal regression via the brms package in R was employed. This method was preferred over Cumulative Link Mixed Models (CLMM) for its superior stability, especially with smaller samples (e.g., 98 creation task observations).

# 3.5.3. RQ3. Emotional response to feedback.

To address RQ3, this study examines students' emotional responses to AI- generated feedback using a 36- item survey designed to assess eight distinct emotions: enjoyment, hope, pride, anger, anxiety, shame, hopelessness, and boredom. Each emotion was measured across four dimensions - affective, motivational, physical, and cognitive - using an adapted version of the Achievement Emotions Questionnaire- Short (AEQ- S). While the original AEQ- S distinguishes between three contexts (learning, classroom, and test), we adopted the learning setting as it best aligns with the AI feedback environment examined in this study (Bieleke et al., 2021). Minor wording adaptations were made to reference AI- generated feedback explicitly (e.g., "I feel helpless" was revised to "I feel helpless when reviewing AI feedback") (see Table 5).

Data Processing and Cleaning. A total of 103 students participated in the survey (49 in the control group and 54 in the emotionally enriched feedback group). To ensure data quality, two responses were excluded due to extremely short completion times (under 93 s), resulting in a final dataset of 101 valid responses. Of these, 70 students also provided qualitative responses to optional open- ended questions. Reliability analysis using Cronbach's alpha demonstrated strong internal consistency across the emotional scales  $(\alpha = 0.84 - 0.93)$

Analysis Approach. Descriptive statistics, including mean, median, and standard deviation, were reported for each emotional response across groups in Table 2. Given the non- normally distributed nature of the data, we employed Mann- Whitney U tests to compare emotional responses between the control and experimental groups.

No Need for Repeated Measures Modeling. Unlike RQ1 and RQ2, where students could engage with multiple AI feedback instances, RQ3 does not involve repeated measures since each student completed the emotional response survey only once. As a result, there was no need for mixed- effects modeling, and the Mann- Whitney U test was sufficient to assess group differences in emotional responses.

The full survey instrument, including all 36 items and the open- ended questions, is provided in Appendix B.

# 3.5.4. RQ4. Impact on outcome.

To address RQ4, this study examines the impact of AI- generated feedback on the quality of resources created by students in the experimental and control groups. Resource quality was assessed using a 0- 5 quality score, integrating student ratings, instructor evaluations, and algorithmic assessments. This multi- dimensional scoring approach, as detailed by Abdi et al. (2021), ensures a comprehensive evaluation of resource quality by incorporating multiple perspectives and computational adjustments.

Data Processing and Cleaning. Quality scores were extracted from RippLE, incorporating student ratings based on relevance and overall quality, instructor moderation for expert validation, and algorithmic refinement through Expectation- Maximization and graph- based trust propagation models. These computational techniques adjust for user reliability biases, enhancing the robustness of the final quality scores.

Descriptive and Preliminary Analysis. We conducted both descriptive and inferential statistical analyses. Mean, median, and standard deviation were reported to summarize resource quality across groups in Table 2. Initial exploration of the data revealed that quality scores were continuous but negatively skewed. To address this, we applied a Beta Generalized Linear Mixed Model (Beta GLMM), as it accounts for bounded, skewed data while maintaining interpretability. The response variable was transformed into a (0,1) range using a small constant and modeled using a beta distribution with a logit link function.

Model- Based Analysis: Beta Generalized Linear Mixed Model (Beta GLMM). Resource quality data, bounded and negatively skewed, were modeled using a Beta GLMM via the glmmTMB package in R. Treatment group (control vs. experimental) served as a fixed effect, with student ID as a random intercept to accommodate repeated measures.

![](images/acfb4e3dfe488bcd799db2a60b03b86db2e1641fdf69e0801eba15fc135d745b.jpg)  
Fig.3. t conditions.

# 3.5.5. Qualitative analyses.

The qualitative data from both RQ2 and RQ3 were synthesized and analyzed using Reflexive Thematic Analysis (Braun et al., 2023). This approach emphasizes an iterative, reflective, and interpretative engagement with the data, prioritizing the researchers' active role in evolving themes. Initially, the transcripts were read multiple times to ensure familiarity and to manually identify significant features for coding. Two researchers independently generated lists of these features and then met regularly to discuss discrepancies and converge on a shared interpretation. In keeping with the principles of Reflexive Thematic Analysis, our consensus- building process enhanced the depth and richness of the analysis rather than focusing on quantifiable agreement measures. Similar codes were clustered into initial themes, which were then refined, distinctly named, and defined through collaborative review. Finally, the authors jointly developed the analysis, presenting evidence and interpretations that capture the essence of students' perception.

# 4. Results

# 4.1. RQ1: Impact on student feedback engagement

Table 2 and Fig. 3 present the engagement metrics for AI feedback during the creation and moderation tasks. These metrics include the duration (in seconds) that students interacted with the feedback. The  $N$  values represent the number of resources or moderation instances evaluated. In the creation task, the control group exhibited a median engagement time of  $(Mdn = 77.51, N = 709)$ , while the experimental group had a median time of  $(Mdn = 71.49, N = 704)$ . This difference was statistically significant  $(p = 0.04)$  with a small effect size  $(|r| = 0.059)$ . For the moderation task, the engagement times were nearly identical, with the control group at  $(Mdn = 3.45, N = 2709)$  and the experimental group at  $(Mdn = 3.46, N = 2671)$  showing no statistically significant difference  $(p = 0.92)$ . Additionally, further results on engagement from the survey data revealed no significant differences between the control group  $(Mdn = 3.29, N = 50)$  and the experimental group  $(Mdn = 3.25, N = 51), (p = 0.82)$ .

The results of the Gamma GLMM analysis indicate model demonstrated reasonable fit  $(\mathrm{AIC} = 15393.80, \mathrm{BIC} = 15414.80)$ . However, the fixed effect of experimental group did not reach significance  $(B = - 0.13, \mathrm{SE} = 0.07, z = - 1.74, p = .08)$ , indicating no statistically reliable difference in mean engagement time between the two groups. The random intercept variance for students was 0.25  $(\mathrm{SD} = 0.50)$ , with an intraclass correlation (ICC) of 0.42, suggesting that approximately 42% of the variance in engagement time was attributable to individual differences among students.

For the moderation task, a similar Gamma GLMM was fitted. Model indices indicated reasonable fit  $(\mathrm{AIC} = 34149.00, \mathrm{BIC} = 34175.40)$ . The experimental group was not a significant predictor of engagement time  $(B = 0.01, \mathrm{SE} = 0.07, z = 0.11, p = .90)$ . The random intercept variance for students was 0.67  $(\mathrm{SD} = 0.82)$ , with an ICC of 0.40, indicating that 40% of the variance in moderation task engagement time was explained by individual differences among students (see Table 3).

The Mann- Whitney test identified a statistically significant, although small, difference in engagement time for the create task, favoring the control group. However, the GLMM results did not support a significant effect of the experimental condition on engagement time while accounting for student- level variability. This suggests that while there may be slight distributional differences, these do not translate into robust differences when accounting for inter- student variance. In the moderation task, both analyses confirmed no significant differences between the groups, reinforcing the notion that feedback engagement time remained stable regardless of the feedback type.

Table 2 Descriptive statistics and effect sizes for engagement time, perceived helpfulness, resource quality, and emotional responses.  

<table><tr><td rowspan="2">Metric</td><td rowspan="2">Dimension</td><td colspan="2">N</td><td colspan="2">Obv</td><td colspan="2">M</td><td colspan="2">Mdn</td><td colspan="2">SD</td><td rowspan="2">p</td><td rowspan="2">|r|</td></tr><tr><td>Ctrl</td><td>Exp</td><td>Ctrl</td><td>Exp</td><td>Ctrl</td><td>Exp</td><td>Ctrl</td><td>Exp</td><td>Ctrl</td><td>Exp</td></tr><tr><td rowspan="2">Engagement Time</td><td>Creation</td><td>197</td><td>198</td><td>709</td><td>704</td><td>115.72</td><td>103.28</td><td>77.51</td><td>71.49</td><td>102.89</td><td>91.76</td><td>0.041*</td><td>0.06</td></tr><tr><td>Moderation</td><td>202</td><td>203</td><td>2713</td><td>2670</td><td>9.94</td><td>9.73</td><td>3.46</td><td>3.46</td><td>15.74</td><td>15.46</td><td>0.851</td><td>0.00</td></tr><tr><td rowspan="2">Perceived Helpfulness</td><td>Creation</td><td>35</td><td>27</td><td>53</td><td>45</td><td>4.15</td><td>4.56</td><td>4.00</td><td>5.00</td><td>1.05</td><td>0.72</td><td>0.045*</td><td>0.20</td></tr><tr><td>Moderation</td><td>47</td><td>54</td><td>138</td><td>201</td><td>4.07</td><td>4.51</td><td>4.00</td><td>5.00</td><td>1.00</td><td>0.95</td><td>&amp;lt;0.001***</td><td>0.29</td></tr><tr><td>Resource Quality</td><td></td><td>197</td><td>198</td><td>739</td><td>750</td><td>4.10</td><td>4.07</td><td>4.10</td><td>4.10</td><td>0.56</td><td>0.60</td><td>0.500</td><td>0.02</td></tr><tr><td rowspan="8">Emotions</td><td>Enjoyment</td><td>49</td><td>52</td><td>-</td><td>-</td><td>3.16</td><td>3.21</td><td>3.31</td><td>3.12</td><td>0.80</td><td>0.80</td><td>0.984</td><td>0.00</td></tr><tr><td>Hope</td><td>49</td><td>52</td><td>-</td><td>-</td><td>3.33</td><td>3.38</td><td>3.38</td><td>3.33</td><td>0.83</td><td>0.76</td><td>0.773</td><td>0.03</td></tr><tr><td>Pride</td><td>49</td><td>52</td><td>-</td><td>-</td><td>3.32</td><td>3.33</td><td>3.42</td><td>3.31</td><td>0.79</td><td>0.84</td><td>0.854</td><td>0.02</td></tr><tr><td>Anger</td><td>49</td><td>52</td><td>-</td><td>-</td><td>2.66</td><td>2.26</td><td>2.61</td><td>1.89</td><td>0.87</td><td>0.99</td><td>0.013*</td><td>0.25</td></tr><tr><td>Anxiety</td><td>49</td><td>52</td><td>-</td><td>-</td><td>2.03</td><td>1.85</td><td>1.82</td><td>1.62</td><td>0.92</td><td>0.88</td><td>0.235</td><td>0.12</td></tr><tr><td>Shame</td><td>49</td><td>52</td><td>-</td><td>-</td><td>2.01</td><td>1.81</td><td>1.71</td><td>1.46</td><td>0.89</td><td>0.81</td><td>0.179</td><td>0.13</td></tr><tr><td>Hopelessness</td><td>49</td><td>52</td><td>-</td><td>-</td><td>1.93</td><td>1.81</td><td>1.72</td><td>1.51</td><td>0.83</td><td>0.80</td><td>0.452</td><td>0.07</td></tr><tr><td>Boredom</td><td>49</td><td>52</td><td>-</td><td>-</td><td>2.77</td><td>2.49</td><td>2.81</td><td>2.47</td><td>1.17</td><td>1.09</td><td>0.212</td><td>0.12</td></tr></table>

Note.  $N =$  number of students;  $M =$  mean;  $Mdn =$  median;  $SD =$  standard deviation;  $p =$  Mann-Whitney  $p$  -value;  $|r| =$  effect size.  $^\ast p < .05$ $^{**}p < .01$ $^{**}p<$  .001.

Table 3 Generalized Linear Mixed Model (GLMM) and Bayesian Ordinal Regression results.  

<table><tr><td>Effect</td><td>Estimate</td><td>SE</td><td>95% CI LL</td><td>95% CI UL</td><td>p</td></tr><tr><td>Fixed Effects (Engagement Time - Gamma GLMM)</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Create Task</td><td>-0.127</td><td>0.073</td><td>-0.269</td><td>0.016</td><td>0.081</td></tr><tr><td>Moderate Task</td><td>0.008</td><td>0.068</td><td>-0.125</td><td>0.141</td><td>0.909</td></tr><tr><td>Fixed Effects (Perceived Helpfulness - Bayesian Ordinal Regression)</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Create Task</td><td>0.70</td><td>0.77</td><td>-0.85</td><td>2.19</td><td>-</td></tr><tr><td>Moderate Task</td><td>2.01</td><td>0.81</td><td>0.46</td><td>3.65</td><td>-</td></tr><tr><td>Fixed Effects (Resource Quality - Beta GLMM)</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Intercept</td><td>1.668</td><td>0.051</td><td>1.568</td><td>1.768</td><td>&amp;lt;.001</td></tr><tr><td>Experimental Group</td><td>-0.053</td><td>0.070</td><td>0.169</td><td>0.065</td><td>0.448</td></tr><tr><td>Random Effects (Student ID Variance)</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Create Task (Engagement Time - GLMM)</td><td>0.253</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Moderate Task (Engagement Time - GLMM)</td><td>0.668</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Perceived Helpfulness (Create Task - Bayesian)</td><td>3.74</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Perceived Helpfulness (Moderate Task - Bayesian)</td><td>3.89</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Resource Quality (Beta GLMM)</td><td>0.286</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Model Fit Statistics</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>AIC (Engagement Time - Create)</td><td>15393.80</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>AIC (Engagement Time - Moderate)</td><td>34149.00</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>WAIC (Perceived Helpfulness - Create)</td><td>146.58</td><td>14.04</td><td>-</td><td>-</td><td>-</td></tr><tr><td>WAIC (Perceived Helpfulness - Moderate)</td><td>459.67</td><td>29.82</td><td>-</td><td>-</td><td>-</td></tr><tr><td>AIC (Resource Quality)</td><td>-2521.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></table>

Note. GLMM  $=$  Generalized Linear Mixed Model, Bayesian  $=$  Bayesian Ordinal Regression.  $\mathrm{SE} =$  Standard Error.  $\Omega =$  Confidence Interval. WAIC  $=$  Watanabe-Akaike Information Criterion. Bayesian models do not have p-values; instead, credible intervals are provided.

# 4.2. RQ2: Perceived helpfulness of AI feedback

Descriptive statistics, as presented in Table 2 and Fig. 3, highlight differences in perceived helpfulness ratings between the control and experimental groups for both creation and moderation tasks.

For the creation task, the control group reported a mean rating of  $M = 4.15$ $SD = 1.05$ $N = 35$ $Mdn = 4.00$ $Mdn\mathrm{Rank} = 20.0)$  while the experimental group rated the feedback higher with  $M = 4.56$ $SD = 0.72$ $N = 27$ $Mdn = 5.00$ $Mdn\mathrm{Rank} = 30.5)$  . This difference was statistically significant  $(p = .045)$  but had a small effect size  $(r = .20)$

For the moderation task, the control group rated the feedback with  $M = 4.07$ $SD = 0.99$ $N = 47$ $Mdn = 4.00$ $Mdn\mathrm{Rank} =$  58.5), while the experimental group provided a higher rating of  $M = 4.51$ $SD = 0.95$ $N = 54$ $Mdn = 5.00$ $Mdn\mathrm{Rank} = 131.0)$  The difference was statistically significant  $(p < .001)$  and had a moderate effect size  $(r = .29)$

These findings indicate that emotionally enriched AI feedback was perceived as more helpful than standard feedback, particularly in moderation tasks, where the effect was stronger.

Mann- Whitney  $U$  tests revealed significant differences between groups for both creation tasks  $(W = 942$ $p = .045)$  and moderation tasks  $(W = 9700.5$ $p < .001)$ , suggesting statistically significant group effects.

Table 4 Analysis of four dimensions measuring anger.  

<table><tr><td rowspan="2">Question</td><td rowspan="2">Dimension</td><td colspan="2">Median</td><td colspan="2">SD</td><td rowspan="2">p</td><td rowspan="2">|r|</td></tr><tr><td>Ctrl</td><td>Exp</td><td>Ctrl</td><td>Exp</td></tr><tr><td>I feel frustrated when the AI feedback points out errors or shortcomings in my task.</td><td>Affective</td><td>2.956</td><td>2.526</td><td>1.164</td><td>1.041</td><td>.062</td><td>.186</td></tr><tr><td>I get annoyed about having to address AI suggestions for improvements on my task.</td><td>Motivational</td><td>3.223</td><td>2.582</td><td>1.141</td><td>1.082</td><td>.005*</td><td>.279</td></tr><tr><td>The AI feedback angers me to the point where I feel like throwing the keyboard out of the window.</td><td>Physical</td><td>2.142</td><td>1.890</td><td>1.248</td><td>1.209</td><td>.153</td><td>.141</td></tr><tr><td>Receiving critical AI feedback can trigger restlessness or discomfort</td><td>Cognitive</td><td>2.313</td><td>2.048</td><td>1.136</td><td>1.120</td><td>.132</td><td>.149</td></tr></table>

Bayesian ordinal regression models provided deeper insights into group differences. For creation tasks, the emotionally enriched group's coefficient was 0.71 (  $95\%$  CI [0.82, 2.22]), with the credible interval including zero, suggesting the effect is not credibly different from zero. The random effect (student- level variation) had a standard deviation of 3.71 (  $95\%$  CI [2.18, 5.55]), reflecting considerable variability among students (see Table 3).

For moderation tasks, the emotionally enriched group showed a significant positive coefficient of 2.00 (  $95\%$  CI [0.37, 3.70]), excluding zero and indicating a credibly positive group effect. The random effect had a standard deviation of 3.90 (  $95\%$  CI [2.90, 5.16]), similar to creation tasks, suggesting substantial variability.

# 4.3. RQ3: Emotional responses to AI feedback

As shown in Table 2 and Fig. 3, both groups reported similar median levels of enjoyment, hope, and pride (all  $p > .05$  . Notably, the experimental group reported significantly lower anger  $\mathrm{Mdn} = 1.89$  vs. 2.61;  $p = .013$ $p = .246)$  . Anxiety, shame, hopelessness, and boredom were comparable across groups.

Analysis of anger subdimensions (Table 4) revealed that control group students experienced significantly higher annoyance in the motivational dimension  $(p = .005$ $p = .279)$

# 4.4. RQ4: Effect on the quality of student work

Table 2 summarizes the descriptive statistics for resource quality scores across both the experimental and control groups. Results indicated comparable resource quality, with the control group  $M = 4.11$ $SD = 0.57$ $Mdn = 4.10$ $N = 751$  ) and the experimental group  $M = 4.08$ $SD = 0.60$ $Mdn = 4.10$ $N = 738$  see Fig.3.

The Beta GLMM model demonstrated reasonable fit  $\mathrm{AIC} = - 2521.3$ $\mathrm{BIC} = - 2500.0)$  . The fixed effect for the experimental group was not statistically significant (  $B = - 0.05$ $\mathrm{SE} = 0.07$ $z = - 0.76$ $p = .448)$  . Thus, AI- generated feedback did not significantly alter resource quality scores compared to the control group. The random intercept variance was significant (Variance  $= 0.29$ $\mathrm{SD} = 0.54)$  indicating substantial variability in resource quality across individual students. The dispersion parameter of the Beta distribution was also significant (7.61), highlighting considerable within- group variability (see Table 3).

A Mann- Whitney test further confirmed no statistically significant difference between groups  $(p = .470)$

# 4.5. Qualitative results

Theme 1: Positive Impact on Learning Experience. Nearly half of the responses  $(49\%)$  indicated that the AI feedback was perceived positively, noting that it enhanced their learning experience by providing valuable insights and increasing engagement. For instance, one student said, "Made me more engaged in the tasks" (Experimental group (Exp)), while another commented, "It gave me precise advice on which part I am doing wrong. According to its feedback, I could finish my task more quickly and correctly" (Control group (Ctrl)). Students highlighted how the feedback made them more motivated to improve, with comments such as "It makes me more motivated to improve my questions and more engaging to do RIPPLE" (Exp) and "It has allowed me to improve my questions before release, which increases my confidence" (Exp). There was widespread agreement with the feedback, as many students felt that the AI's suggestions were relevant and useful for task improvement.

Theme 2: Neutral or Minimal Impact. Around  $9\%$  of responses indicated that the AI feedback had little to no impact on the students' learning experience. Comments such as "Not affected my engagement" (Ctrl) and "I generally don't feel anything towards the AI feedback" (Ctrl) reflect this sentiment. These responses suggest that the feedback neither enhanced nor hindered their progress, offering minimal support in their academic tasks.

Theme 3: Emotional Impact of AI Feedback.  $21\%$  of the comments reflected students' emotional reactions to the AI feedback, which ranged from positive to negative.

Positive emotions like satisfaction and confidence were reported by  $11\%$  of the students' responses. For example, one student expressed, "It made me feel sort of relieved as it pointed out things that I could improve on to make good feedback" (Ctrl). Another noted, "I feel a bit confident if it gives me a good comment" (Ctrl). Some students appreciated the non- judgmental nature of the AI, with one stating, "I like how the feedback is not from a person and lacks the judgement associated with that. I find the feedback more easily absorbed as I don't feel critically judged" (Exp). Another student mentioned, "The AI feedback made me feel positive about my feedback when it gave me positive feedback as well" (Exp).

Negative emotions were reported by  $10\%$ , particularly in response to delays in feedback or perceived inaccuracies. One student mentioned, "I find much less enjoyment doing the RIPPLE tasks as I get annoyed when waiting for the AI feedback to be typed out" (Exp), reflecting frustration with the system's response time. Another student expressed dissatisfaction, stating, "It hasn't always been accurate and hasn't always been able to pick up on the tone that I was going for with my feedback/my questions" (Ctrl).

Theme 4: Issues with Feedback Quality and Relevance. Nearly  $18\%$  of responses raised concerns about the quality and relevance of the feedback. Redundancy was a common complaint, with students stating that the AI often repeated suggestions or failed to provide new insights. One student remarked, "The AI is asking me to do things I have already done" (Ctrl). Lengthy feedback was also a source of frustration, with comments like "The feedback is a bit too long" (Exp) and "It was a lot to read so I did not read all of it" (Ctrl). Additionally, misalignment between the feedback and students' input caused confusion. For instance, a student stated, "It has not always been accurate and hasn't always been able to pick up on the tone that I was going for with my feedback/my questions" (Ctrl). Another student mentioned, "Sometimes it suggests excessive things that would not be required for the level of the task" (Ctrl). Instances where the AI overlooked key user input were also noted, contributing to confusion and mistrust. One student reported, "It never seems to add anything of value, sometimes even missing points that you have already made" (Ctrl).

# Theme 5: Technical Challenges

Only  $3\%$  of responses mentioned technical difficulties with the AI feedback system. These operational issues included delays in feedback generation and system failures. One student noted, "I had to wait for the feedback generation, and it is usually a long paragraph" (Exp), while another reported, "Sometimes it fails to work" (Ctrl). These challenges contributed to a sense of frustration and diminished the overall effectiveness of the feedback.

# 5. Discussion

Learnersourcing fosters socio- cultural and socio- cognitive activities that support students' cognitive growth (see Section 3.1). The socio- cognitive perspective views students as agents who construct knowledge through interactions with their environments. Hence, in learnersourcing platforms, students must develop self- regulated learning (SRL) skills (Schunk, 1989). From a socio- cultural viewpoint, SRL can be supported through scaffolding strategies such as monitoring approaches (see Azevedo et al., 2022).

The RIPPLE platform, used in this study, has explored multiple learning scaffolds for SRL. For example, Lahza et al. (2022) evaluated four SRL scaffolding strategies: (1) self- monitoring, (2) planning, (3) self- assessing, and (4) combining all three. One finding was that more complex strategies elicited stronger negative emotions, emphasizing the need to sustain motivation and affect. Building on past work and a new feature that uses LLMs for automated feedback, we addressed the emotional aspect of learning via the CVT of achievement emotions. Our study not only contributes to the development and enhancement of RIPPLE but also partially responds to the call made by Tabak and Kyza (2018), questioning whether technological adaptive scaffolding is inherently limited by the absence of human affective components. In what follows, we discuss our findings in relation to the RQs presented in Section 1.

# 5.1. RQ1: Engagement with feedback

The literature on feedback engagement identifies several forms of student interaction with feedback, including cognitive, behavioral, and emotional engagement (Mao & Lee, 2024; Zhang & Hyland, 2018). While previous studies have explored student engagement with feedback, challenges remain in optimizing how students interact with AI- generated feedback (Burner et al., 2025; Dann et al., 2024; Sutherland et al., 2023). RQ1 aimed to examine student engagement with emotionally enriched LLMs- based written feedback from a behavioral perspective. We measured the time students spent reading and reflecting on the AI feedback they received while authoring or reviewing learning content. Regarding the peer review task, the findings revealed no significant difference between the groups. However, in the content authoring task, the control group exhibited significantly longer engagement with feedback. Although this effect was small, it did not persist when analyzed using a model that accounts for repeated measures. At first glance, this finding may seem counterintuitive, as the experimental group received longer feedback, which would typically require more reading time. However, this suggests that students in the experimental group may have processed the feedback more efficiently, potentially due to the added visual elements (emojis), which may have facilitated comprehension and reduced cognitive load (Chen et al., 2024; Clark & Paivio, 1991).

However, this finding can be explained in the context of previous research and from the perspective of the CVT. First, Cloude et al. (2021) found that fluctuations in negative deactivating emotions over six- time points were negatively associated with the time spent utilizing cognitive strategies. Hence, the students in the experimental group might have had unstable emotions due to the added emotional features in the feedback, where different parts of the feedback conveyed different emotions through emojis and praises. Second, CVT implies that emotions are associated with motivation and cognition (Pekrun, 2006). Students experiencing negative emotions might not focus on the task at hand and consume cognitive resources (Pekrun, 2006). In our study, this might have been the case with the control group, where they might have had negative emotions in response to the feedback comments.

Third, central to CVT are the control and value appraisals based on which students perceive their control over success and failures and the value of outcomes. Hence, in the case of the experimental group, the students might have felt satisfied when evaluating the feedback, believing that their initial efforts were adequate due to the emotional features that praised their effort (Kakinuma et al., 2022).

# 5.2. RQ2: Perceptions of feedback helpfulness

While there are currently several discussions on the effectiveness of incorporating LLMs in the learning process to support students (Kasneci et al., 2023), how students perceive AI feedback is still under investigation. Examining students' perceptions is important because it can impact various learning factors such as online learning satisfaction, intrinsic motivation, and feedback processing and revision (Noroozi et al., 2024). Recent research findings are mixed: some studies found that students perceived human feedback (e.g., peers) as more helpful than AI feedback (e.g., Oiga et al., 2023), while other research found that students' preferences were split between both sources of feedback (e.g., Escalante et al., 2023). However, complementing human feedback with LLMs was perceived as more helpful than human or AI feedback alone. RQ2 contributes to this knowledge by examining student perceptions of the helpfulness of emotionally enriched LLMs- based written feedback. We measured students' perceptions using helpfulness ratings presented after they received LLMs- based written feedback on content authoring and peer review tasks. The findings revealed a significant difference between the two groups, with students who received emotionally enriched feedback finding the AI feedback more helpful than their peers in the control group.

The RQ2 findings align with the interpretation of the RQ1 findings, where we posited that the students in the experimental group were more satisfied with the outcomes of their work on content authoring. However, this finding might raise concerns regarding feedback enactment; one might assume that students who received feedback complemented with praise and emojis are less motivated to revise their initial work (see Brophy, 1981). We argue that students who receive emotionally enriched feedback might be more willing to reengage with learning tasks. For instance, a student might complete a content authoring task and follow it with another content authoring task on the learning platform; hence, spending less time on the task at hand but more time on the platform than students who do not receive emotionally enriched feedback. This argument is supported by research in Human- Computer Interaction. For example, O'Brien and Toms (2008) described user engagement as a cyclic process in which positive prior experiences with an application increase the likelihood of reengaging with the application.

# 5.3. RQ3: Emotional response to feedback

The CVT of achievement emotions can provide valuable insights into emotion generation, emotion sources, and associated processes in complex, intelligent learning environments (e.g., Azevedo et al., 2022). One of the essential assumptions of CVT is that scaffolding the regulation of value and control appraisals can indirectly support student emotional development (Pekrun, 2006). Hence, RQ3 examined student emotions by utilizing the CVT of achievement emotions. Specifically, we followed the CVT assumptions to scaffold positive emotions in both content authoring and peer review tasks, then compared the scaffolded group (experimental) against the control group across several emotional dimensions. We measured these emotions using a survey instrument after an extended period of time, unlike in RQ2.

The findings revealed relatively high levels of positive emotions across both groups with no significant differences between them. Analysis of negative emotions, however, uncovered a more nuanced pattern. While students generally reported low levels of negative emotions, anger and boredom emerged at moderately higher levels. Notably, students in the control group demonstrated significantly higher levels of anger compared to the experimental group, particularly when responding to an item measuring motivational orientation, specifically, whether they felt annoyed about addressing AI- generated suggestions for improvement. This finding suggests that our emotional scaffolding technique effectively reduced students' negative emotional responses, such as anger, and potentially influenced their motivational stance when engaging with feedback.

Nevertheless, these results should not be interpreted as evidence that the intervention necessarily enhanced motivation in the experimental group receiving AI feedback. On the contrary, as our earlier analysis indicated, students in the experimental group may have developed a premature sense of satisfaction with their initial work, which paradoxically diminished their motivation to undertake substantive revisions. When integrating the findings across RQ1, RQ2, and RQ3, a coherent pattern emerges that aligns with established research on emotion- behavior relationships in educational contexts (e.g., Cheng et al., 2014). The control group, despite experiencing higher levels of anger and frustration, demonstrated more thorough engagement with the feedback, dedicating greater cognitive resources and time to the revision process. Conversely, the experimental group, which reported higher satisfaction and lower frustration, invested comparatively less effort in revisions.

This result can also be interpreted through the lens of attribution theory, which suggests that following task completion, students may experience emotional reactions to perceived outcomes, such as happiness or sadness in response to success or failure, and reflect on these experiences. These reflections can give rise to more complex emotions, such as anger or pride, that influence subsequent motivation and behavior (Pintrich, 2000).

# 5.4. RQ4: Quality of student work

Students' learnersourced product quality can indirectly reflect various learning phenomena. Generally, high- quality products (i.e., authored learning resources and written reviews) can indicate cognitive engagement with the tasks, thus, learning occurs. For instance, in the case of content authoring, students could engage in high- level cognitive processes reflected in Bloom's taxonomy (e.g., Moore et al., 2022). In the context of AI feedback, such engagement could occur because students reflect on the feedback they receive. Hence, high- quality products might indicate that students have developed feedback literacy, where they value their role in the feedback process. For instance, students could feel responsible for taking action to incorporate the received feedback to improve the quality of their work (see Carless & Boud, 2018). RQ4 examined the impact of incorporating emotions into AI feedback. We used the quality rating provided by RiPPLE to determine each learning product's quality. The findings demonstrated that both the control and experimental groups generated learning products of equivalent quality, which might indicate that feedback uptake was similar in both groups. This finding is surprising, as students in the control group had more behavioral engagement (see the discussion of RQ1). A study on the effect of student engagement on writing tasks by Jin et al. (2022) found that behavioral engagement was associated with writing performance. However, in our study, the higher level of behavioral engagement in the control group was accompanied by negative emotions (i.e., anger), which might explain why their performance was not higher than that of the experimental group.

The findings of RQ4 contribute to the line of research concerning the effect of AI feedback on learning. Recent studies have shown how AI feedback can complement student- generated content to improve the quality of their work (Moore et al., 2022). Additionally, other studies have demonstrated that AI feedback is equivalent to human feedback in terms of learning outcomes (e.g., Escalante et al., 2023). Our study's findings imply that the effect of emotionally enriched AI feedback on learning outcomes is complex. This aligns with previous research on the effect of positive and negative emotions on effort and learning (see Schunk, 2005). However, linking the findings of RQ4 to those of RQ2 and RQ3, what is significant about the findings of RQ4 is that our intervention could maintain students' emotional well- being without compromising their learning outcomes.

# 5.5. Implications for educational practice

This study enriches the CVT of Achievement Emotions by elucidating how emotionally enriched feedback shapes students emotional experiences in digital learning contexts. By demonstrating that such feedback can reduce negative emotions like anger and enhance perceived helpfulness, it underscores the critical role of emotional appraisals - control and value - in mediating feedback effectiveness. Beyond theory, these findings suggest that integrating emotional intelligence into AI- driven feedback systems has the potential to foster supportive learning environments, enhancing student well- being and receptivity to feedback, even if immediate gains in engagement or work quality remain modest. This highlights a promising direction for educational technologies to bridge emotional gaps in digital settings, ultimately supporting more holistic student learning outcomes.

Based on the evidence gathered in this study, the following implications can be drawn:

- Improving Students' Emotional Climate Without Harming Performance. Students who received emotionally enriched feedback reported lower levels of negative affect (particularly anger) and rated the feedback as more helpful. Although their work quality did not statistically differ from that of the control group, the capacity to reduce negative emotions is not trivial. Maintaining a more positive emotional environment could foster better well-being and more favorable perceptions of feedback. These factors may help sustain student motivation over time.- Uncertain Return on Investment for Performance Gains. Despite its emotional benefits, the enriched feedback did not yield measurable advantages in students' performance or resource quality. Educators and institutions, therefore, need to weigh whether the additional effort and resources required to produce emotionally enriched feedback are justified if the primary goal is to boost performance. Based on our data, it remains unclear whether more extensive work on emotional scaffolding would eventually lead to better quality or long-term achievement gains.- Designing for Positive Reception. Many learners viewed the enriched feedback as helpful and engaging. In contexts where student well-being and emotional support are prime concerns, such as first-year transition courses or settings with high anxiety, introducing supportive language and encouraging elements could foster a more welcoming atmosphere. Even if immediate performance improvements are hard to see, creating a less stressful learning environment may still be worthwhile.- Targeting Specific Negative Emotions. Our intervention particularly reduced anger, an emotion that can trigger disengagement or defensive reactions toward feedback. This suggests that there is merit in designing feedback systems that specifically aim to mitigate negative emotional responses.

# 5.6. Limitations and future directions

This study offers valuable insights into emotionally enriched AI feedback, but several limitations should be acknowledged. First, the emotional feedback was presented as a combined package of pictorial and linguistic elements, making it difficult to isolate the individual effects of each. Future studies should examine these components separately to better understand their distinct contributions.

Second, the RiPPLE platform lacked real- time emotional tracking and detailed interaction logs, which limited the personalization of feedback and our ability to fully capture how students engaged with it. Incorporating affective sensing and more advanced tracking tools in future research could address these gaps.

Third, while time- on- task served as a practical proxy for behavioral engagement, it may not fully reflect the depth or quality of cognitive and emotional engagement. Moreover, our study did not include data on whether or how students revised their work in response to feedback. This narrowed our understanding of how feedback translated into meaningful improvements in student output.

Fourth, the emotional response survey involved a relatively small sample size and was administered after a delay. These factors may have impacted the accuracy and reliability of the reported emotional experiences, limiting the strength of related conclusions. Future studies should aim for larger and more timely samples to better capture students' emotional responses.

Finally, feedback length was not controlled and may have influenced perceptions of helpfulness. Future research should account for this variable. The short duration of the study also restricted the ability to examine long- term effects. Longitudinal designs are recommended to explore the sustained emotional and academic impact of emotionally enriched AI feedback.

# 6. Conclusion

This study underscores the significant potential of integrating emotionally enriched feedback via generative AI in educational settings, particularly in enhancing the emotional aspects of student feedback encounters. Through a randomized controlled trial involving web design course students, we assessed the impact of AI feedback enhanced with motivational elements on student engagement and emotional responses.

The findings indicate that students who received emotionally enriched AI feedback reported a noticeable reduction in negative emotions, particularly anger, and perceived the feedback as more helpful compared to those who received neutral feedback. However, the emotional enhancements did not significantly impact the overall level of engagement with the feedback or the quality of student work. Nonetheless, the incorporation of emotional elements fostered a more positive feedback environment.

These results suggest that while emotionally enriched AI feedback may not directly enhance academic performance or engagement levels, it plays a crucial role in improving students' emotional well- being and their receptivity to feedback. This is particularly important in digital learning environments, where the absence of physical cues can lead to emotional disconnect and decreased motivation.

In conclusion, this research contributes to the evolving landscape of educational technology by highlighting the importance of incorporating emotional considerations into AI- driven feedback systems. By fostering a more empathetic and supportive feedback environment, educational technologies can better support student learning, engagement, and emotional well- being, thereby enhancing the overall educational experience.

# CRediT authorship contribution statement

Omar Alsaiari: Writing - original draft, Writing - review & editing, Validation, Methodology, Formal analysis, Data curation, Conceptualization. Nilufar Baghaei: Writing - review & editing, Supervision, Methodology, Conceptualization. Hatim Lahza: Writing - review & editing. Jason M. Lodge: Writing - review & editing, Supervision, Conceptualization. Marie Boden: Writing - review & editing, Supervision. Hassan Khosravi: Writing - review & editing, Validation, Supervision, Methodology, Conceptualization.

# Declaration of Generative AI and AI-assisted technologies in the writing process

During the preparation of this work, the main author used ChatGPT in order to improve the readability of the manuscript. After using this tool, the authors reviewed and edited the content as needed and take full responsibility for the content of the published article.

# Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

# Acknowledgments

We thank all participants who took part in this study. The first author extends sincere thanks to the Ministry of Education and the Saudi Cultural Mission (SACM) for their PhD scholarship and ongoing support. Special thanks go to the RiPPLE platform team, particularly Nick Joseph, for their support.

# Appendix A. Prompts for AI feedback conditions

# A.1. Control condition

You are an expert exam question writer and tutor.

Your student has been tasked with creating a multiple- choice question.

First, work out your own solution to the multiple- choice question asked. Then compare your solution to the option the student said was the answer and evaluate if the option the student said was correct is the one best answer out of all the options presented. Don't deride if the option the student marked as the correct option is actually the most correct answer until you have done the question yourself. If you evaluate the student has selected the correct answer mention this in the positives section. Remember a multiple- choice question should have one or more options (distractors) that are factually incorrect. Do not comment if a distractor or option marked as incorrect is factually incorrect.

Your job is to then provide feedback to the student to help improve their question using the criteria below.

- You must be indirect and unsure about your considerations; use tentative statements like "could", "may", "perhaps", "consider".  
- Your feedback must be kind, constructive, specific, and very actionable.  
- For each suggestion, you must provide only one example of how the suggestion can be applied with direct reference to the input provided.  
- Write "question" instead of "stem".  
- Write "options" instead of "distractors".  
- You must mention if the answer is correct and the one best answer OR if it was incorrectly selected.  
- You must check if the distractors are truly inaccurate or not the one best answer.

Evaluate the student's submission based on the following criteria:

Criterion 1: Quality of question (or question body)

1. The question is clearly written and grammatically correct.  
2. The question should be meaningful without having to read all the options first.  
3. The question is not negatively worded or includes double negatives.

Criterion 2: Quality of options (or distractors)

1. The options use common mistakes/hisconceptions.

2. The options are plausible and from the same category.

3. The options do not include "None of the above."

4. The option marked as correct is the best answer.

5. Options not marked as correct are factually inaccurate.

6. Options should avoid clues that give away the correct answer, such as grammatical cues, word repetition, absolute terms, or negative wording. If an option is not marked as correct, the explanation should outline why the option is incorrect.

Criterion 3: Quality of explanation

1. The explanations are accurate.

2. The explanations are easy to understand.

3. The explanations use an encouraging tone.

4. The explanations clearly explain why the option is correct or incorrect.

Use the following template to structure your response but do not copy it verbatim:

<p>The aim of this resource is to assess [x].</p>

<h1><strong><span style="font- size: 12pt;"> Positives</span></strong></h1>

<ul>

<li>Positive aspect and why this has learning benefits.</li>  <li>Positive aspect and why this has learning benefits.</li>  <li>Positive aspect and why this has learning benefits.</li>  </ul>

<p><strong><span style="font- size: 12pt;"> Considerations</span></strong></p>

<ul>  <li><input type="checkbox"> Specific, actionable, tentative suggestion. Explanation.</li>  <li><input type="checkbox"> Specific, actionable, tentative suggestion. Explanation.</li>  <li><input type="checkbox"> Specific, actionable, tentative suggestion. Explanation.</li>  </ul>

# A.2. Experimental condition

You are an expert exam question writer and tutor.

Your student has been tasked with creating a multiple- choice question.

First, work out your own solution to the multiple- choice question asked. Then compare your solution to the option the student said was the answer and evaluate if the option the student said was correct is the one best answer out of all the options presented. Don't decide if the option the student marked as the correct option is actually the most correct answer until you have done the question yourself. If you evaluate the student has selected the correct answer mention this in the positive section. Remember a multiple- choice question should have one or more options ( distractors) that are factually incorrect. Do not comment if a distractor or option marked as incorrect is factually incorrect.

Your job is to then provide feedback to the student to help improve their question using the criteria below.

- You must be indirect and unsure about your considerations; use tentative statements like "could", "may", "perhaps", "consider".

- Your feedback must be kind, constructive, specific, and very actionable. 
- For each suggestion, you must provide only one example of how the suggestion can be applied with direct reference to the input provided.

- Write "question" instead of "stem".

- Write "options" instead of "distractors".

- You must mention if the answer is correct and the one best answer OR if it was incorrectly selected.

- You must check if the distractors are truly inaccurate or not the one best answer. 
- NEVER use inappropriate or suggestive emojis such as hearts or winking faces.

The overall tone of the feedback should be highly positive, uplifting, and encouraging, enriched with relevant and meaningful emojis at the end of phrases or sentences (do NOT place emojis at the start of sentences; this is harder to read). You should be fun.

In the "Suggestions for improvement" section, make sure to apply positive reinforcement generously and offer high praise to students for the positives found in their submission.

End with a thank- you note to the student for their contribution and effort. Remind the student in an encouraging, friendly tone that their resources will be reviewed and rated by peers, stressing the importance of high- quality submissions. Include a relevant emoji.

Use the following template to structure your response, enriched with positive and fun language along with appropriate emojis:

<p>Hi I'm [Your Name].</p> <br>

<p>The aim of this resource is to [explain the aim].</p>

<h1><strong><span style="font- size: 12pt;"> Positives</span></strong></h1>

<ul>

<li>Positive aspect and why this has learning benefits with emoji.</li>

<li>Positive aspect and why this has learning benefits with emoji.</li>

<li>Positive aspect and why this has learning benefits with emoji.</li>

</ul>

<p><strong><span style="font- size: 12pt;"> Considerations for improvement</span></strong></p>

<ul>

<li>Suggestion with emoji. Example of suggestion. Justification.</li>

<li>Suggestion with emoji. Example of suggestion. Justification.</li>

<ul>

<p>Thank you for your brilliant contribution! Your resource will be reviewed and rated by peers. Keep up the fantastic work! [relevant emoji]</p>

# Appendix B. Survey items

This appendix presents the complete set of survey items used to evaluate participants' responses to the AI- generated feedback on tasks in RiPPLE. Participants indicated their level of agreement with each statement using a Likert scale (1 = Strongly Disagree to 5 = Strongly Agree). Additionally, several open- ended questions were included to elicit qualitative feedback (see Table 5).

Table 5 Survey items by category.  

<table><tr><td>Category</td><td>Survey item</td></tr><tr><td>Enjoyment</td><td>I enjoy the challenge of reviewing AI-generated feedback on my task in RIPPLE.
I find dealing with the AI-generated feedback insightful and engaging.
The positive progress indicated in the AI feedback motivates me to continue refining my work.
When I receive positive feedback from the AI on my task, I get a rush.</td></tr><tr><td>Hope</td><td>I feel confident about my task performance in RIPPLE when reviewing AI feedback.
I feel confident that I can improve my task based on the insights provided by the AI feedback.
I feel optimistic about making good progress by utilizing AI feedback on my tasks.
My sense of confidence motivates me to review and adapt AI suggestions.</td></tr><tr><td>Pride</td><td>I am proud of the positive aspects highlighted in the AI feedback on my task.
The AI feedback reinforces my belief in my ability to produce quality work in RIPPLE.
The sense of accomplishment conveyed in the AI feedback fuels my motivation to excel further.
Receiving positive AI feedback on my tasks fills me with pride.</td></tr><tr><td>Anger</td><td>I feel frustrated when the AI feedback points out errors or shortcomings in my task.
I get annoyed about having to address AI suggestions for improvements on my task.
The AI feedback angers me to the point where I feel like throwing the keyboard out of the window.
Receiving critical AI feedback can trigger restlessness or discomfort.</td></tr><tr><td>Anxiety</td><td>I get tense and nervous while reviewing AI feedback on my task.
I worry whether I am able to deal with AI feedback on all my tasks.
While reviewing AI feedback on my task, I feel compelled to distract myself to alleviate my anxiety.
Worrying about not addressing AI feedback that suggests areas for improvement makes me sweat.</td></tr><tr><td>Shame</td><td>I feel ashamed if the AI feedback suggests areas of improvement in my task.
I feel ashamed when I realize that I lack the ability to complete my tasks in RIPPLE without AI pointing out areas for improvement.
Because I have had considerable difficulty with the AI feedback, I avoid discussing it.
When someone observes the extent to which AI has offered me suggestions, I avoid eye contact.</td></tr><tr><td>Helplessness</td><td>I feel helpless when reviewing AI feedback.
I am resigned to the fact that I do not have the capacity to complete a task in RIPPLE without AI identifying areas for improvement.
I feel so helpless that I cannot fully commit to improving my tasks in RIPPLE based on AI suggestions.
My lack of confidence exhausts me before I even begin addressing AI suggestions.</td></tr><tr><td>Boredom</td><td>Reviewing AI feedback on my tasks in RIPPLE bores me.
The AI feedback is so monotonous that I find myself daydreaming.
I would prefer to postpone dealing with the AI feedback until tomorrow.
While reviewing AI feedback, I often find my attention drifting.</td></tr><tr><td>Open-Ended Questions</td><td>Describe how the AI feedback affected your feelings toward tasks on RIPPLE.
How has the AI-generated feedback affected your engagement with tasks on the RIPPLE platform?
What did you like about RIPPLE&#x27;s AI feedback? In what ways did these elements enhance your experience?
What aspects of RIPPLE&#x27;s AI feedback did you not like?</td></tr></table>

# Data availability

The authors do not have permission to share data.

# References

Abdi, S., Khosravi, H., Sadiq, S., & Demartini, G. (2021). Evaluating the quality of learning resources: A learnersourcing approach. IEEE Transactions on Learning Technologies, 14(1), 81- 92. Afzal, M., Zia, A., Nouri, J., & Fors, U. (2023). Informative feedback and explainable AI- based recommendations to support students' self- regulation. Technology, Knowledge and Learning, 1- 24. Azevedo, R., Bouchet, F., Duffy, M., Harley, J., Taub, M., Trevors, G., Cloude, E., Dever, D., Wiedbusch, M., Wortha, F., & Cerezo, R. (2022). Lessons learned and future directions of MetaTutor: Leveraging multichannel data to scaffold self- regulated learning with an intelligent tutoring system. Frontiers in Psychology, 13, URL https://www.frontiersin.org/articles/10.3389/fpsyg.2022.813632.

Bai, Q., Dan, Q., Mu, Z., & Yang, M. (2019). A systematic review of emoji: Current research and future perspectives. Frontiers in Psychology, 10, http://dx.doi.org/10.3389/fpsyg.2019.02221. Bandura, A. (1999). Social cognitive theory: An agentic perspective. Asian Journal of Social Psychology, 2(1), 21- 41. http://dx.doi.org/10.1111/1467- 839X.00024, eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467- 839X.00024. Bieleke, M., Gogol, K., Goetz, T., Daniels, L., & Pokrun, R. (2021). The AEQ- s: A short version of the achievement emotions questionnaire. Contemporary Educational Psychology, 65, Article 101940. Boud, D., & Molloy, E. (2013). Rethinking models of feedback for learning: the challenge of design. Assessment & Evaluation in Higher Education, 38(6), 698- 712. http://dx.doi.org/10.1080/02602938.2012.691462, arXiv:DOI:10.1080/02602938.2012.691462. Brackett, M. A., Floman, J. L., Ashton- James, C., Cherkasskiy, L., & Salovey, P. (2013). The influence of teacher emotion on grading practices: A preliminary look at the evaluation of student writing. Teachers and Teaching, 19(6), 634- 646. Braun, V., Clarke, V., Hayfield, N., Davey, L., & Jeckelson, E. (2023). Doing reflexive thematic analysis. In Supporting research in counselling and psychotherapy: qualitative, quantitative, and mixed methods research (pp. 19- 38). Springer. Brophy, J. (1981). Teacher Praise: A functional analysis. Review of Educational Research, 51(1), 5- 32. http://dx.doi.org/10.3102/00346543051001005, Publisher: American Educational Research Association. Burner, T., Lindvig, Y., & Wae mess, J. I. (2025). "We should not be like a dinosaur" Using AI technologies to provide formative feedback to students. Education Sciences, 19(1), 50. Caraban, A., Karapanos, E., Goncalves, D., & Campos, P. (2019). 23 ways to nudge: A review of technology- mediated nudging in human- computer interaction. In Proceedings of the 2019 CHI conference on human factors in computing systems (pp. 1- 15). Carless, D., & Boud, D. (2018). The development of student feedback literacy: enabling uptake of feedback. Assessment & Evaluation in Higher Education, 43(8), 1315- 1325. http://dx.doi.org/10.1080/02602938.2018.1463354, Publisher: Routledge eprint. Chen, L., Chen, P., & Lin, Z. (2020). Artificial intelligence in education: A review. Ieee Access, 8, 75264- 75278. Chen, Y- J., Hsu, L., & Lu, S- w. (2024). How does emoji feedback affect the learning effectiveness of EFL learners? Neuroscientific insights for CALL research. Computer Assisted Language Learning, 37(7), 1857- 1880. Cheng, K- H., Hou, H- T., & Wu, S- Y. (2014). Exploring students emotional responses and participation in an online peer assessment activity: a case study. Interactive Learning Environments, 22(3), 271- 287. http://dx.doi.org/10.1080/10494820.2011.649766, Publisher: Routledge eprint. Clark, J. M., & Paivio, A. (1991). Dual coding theory and educational. Educational Psychology Review, 3, 149- 210. Cloude, E. B., Wortha, F., Dever, D. A., & Azevedo, R. (2021). Negative emotional dynamics shape cognition and performance with MetaTutor: Toward building affect- aware systems. In 2021 9th international conference on affective computing and intelligent interaction (pp. 1- 8). http://dx.doi.org/10.1109/ACII52823.2021.9597462. Dann, C., Redmond, P., Fanshawe, M., Brown, A., Getenet, S., Shaik, T., Tao, X., Galligan, L., & Li, Y. (2024). Making sense of student feedback and engagement using artificial intelligence. Australasian Journal of Educational Technology, http://dx.doi.org/10.14742/ajet.8903. Darvishi, A., Khosravi, H., Sadiq, S., & Gasevic, D. (2022). Incorporating AI and learning analytics to build trustworthy peer assessment systems. British Journal of Educational Technology, 53(4), 844- 875. Darvishi, A., Khosravi, H., Sadiq, S., Gasevic, D., & Siemens, G. (2024). Impact of AI assistance on student agency. Computers & Education, 210, Article 104967. D'Errico, F., Paciello, M., & Cerniglia, L. (2016). When emotions enhance students' engagement in e- learning processes. Journal of E- Learning and Knowledge Society, 12(4). Erle, T., Schmid, K., Goslar, S. H., & Martin, J. D. (2021). Emojis as social information in digital communication. Emotion, http://dx.doi.org/10.1037/emo0000992. Escalante, J., Pack, A., & Barrett, A. (2023). AI- generated feedback on writing: insights into efficacy and ENL student preference. International Journal of Educational Technology in Higher Education, 20(1), 57. http://dx.doi.org/10.1186/s41239- 023- 00425- 2. Faulconer, E., Griffith, J., & Gruss, A. (2021). The impact of positive feedback on student outcomes and perceptions. Assessment & Evaluation in Higher Education, 47, 259- 268. http://dx.doi.org/10.1080/02602938.2021.1910140. Fong, C. J., & Schallert, D. L. (2023). "Feedback on the future": Advancing motivational and emotional perspectives in feedback research. Educational Psychologist, 58(3), 146- 161. http://dx.doi.org/10.1080/00461520.2022.2134135, arXiv:DOI:10.1080/00461520.2022.2134135. Fong, C. J., Williams, K. M., Williamson, Z. H., Lin, S., Kim, Y. W., & Schallert, D. L. (2018). "Inside out": Appraisals for achievement emotions from constructive, positive, and negative feedback on writing. Motivation and Emotion, 42, 236- 257. Fredrickson, B. L. (2001). The role of positive emotions in positive psychology: The broaden- and- build theory of positive emotions. American Psychologist, 56(3), 218. Frondozo, C. E., & Yang, L. (2023). Feedback orientation and learning- related academic emotions: An exploratory study in Filipino university students. In Positive psychology and positive education in Asia: understanding and fostering well- being in schools (pp. 57- 75). Springer. Gamlem, S. M., & Smith, K. (2013). Student perceptions of classroom feedback. Assessment in Education: Principles, Policy & Practice, 20(2), 150- 169. Grawemeyer, B., Halloran, J., England, M., & Croft, D. (2022). Feedback and engagement on an introductory programming module. In Proceedings of the 6th conference on computing education practice. http://dx.doi.org/10.1145/3498343.3498348. Gyamfi, G., Hanna, B. E., & Khosravi, H. (2022). The effects of rubrics on evaluative judgement: a randomised controlled experiment. Assessment & Evaluation in Higher Education, 47(1), 126- 143. http://dx.doi.org/10.1080/02602938.2021.1887081, Publisher: Routledge eprint. Hattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational Research, 77(1), 81- 112. Hill, J., Berlin, K., Choate, J., Cravens- Brown, L., McKendrick- Calder, L., & Smith, S. (2021). Exploring the emotional responses of undergraduate students to assessment feedback: Implications for instructors. Teaching and Learning Inquiry: The ISSOTL Journal, 9(1), 294- 316. Jensen, L. X., Bearman, M., & Boud, D. (2023). Feedback encounters: towards a framework for analysing and understanding feedback processes. Assessment & Evaluation in Higher Education, 48(1), 121- 134. http://dx.doi.org/10.1080/02602938.2022.2059446, arXiv:DOI:10.1080/02602938.2022.2059446. Jin, X., Jiang, Q., Xiong, W., Feng, Y., & Zhao, W. (2022). Effects of student engagement in peer feedback on writing performance in higher education. Interactive Learning Environments, 1- 16. http://dx.doi.org/10.1080/10494820.2022.2081209, Publisher: Routledge eprint. Kakinuma, K., Nakai, M., Hada, Y., Kizawa, M., & Tanaka, A. (2022). Praise affects the "Praiser": Effects of ability- focused vs. effort- focused praise on motivation. The Journal of Experimental Education, 90(3), 634- 655. Karpicke, J. D. (2017). 2.27 - Retrieval- based learning: A decade of progress. In J. H. Byrne (Ed.), Learning and memory: a comprehensive reference (second edition) (2nd ed.). (pp. 487- 514). Oxford: Academic Press, http://dx.doi.org/10.1016/B978- 0- 12- 8093245.21055- 9, URL https://www.sciencedirect.com/science/article/pii/B9780128093245210559. Kasneci, E., Sessler, K., Kichemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser, U., Groh, G., Gunnemann, S., Hüllermeier, E., Krusche, S., Kutyniok, G., Michael, T., Nerdel, C., Pfeffer, J., Poque, O., Sailer, M., Schmidt, A., Seidel, T., ... Kasneci, G. (2023). ChatGPT for good? On opportunities and challenges of large language models for education. Learning and Individual Differences, 103, Article 102274. http://dx.doi.org/10.1016/j.lindif.2023.102274, URL https://www.sciencedirect.com/science/article/pii/S1041608023000195. Khosravi, H., Cooper, K., & Kitto, K. (2017). Triple: Recommendation in peer- learning environments based on knowledge gaps and interests. arXiv preprint arXiv:1704.00556. Khosravi, H., Kitto, K., & Williams, J. J. (2019). Ripple: A crowdsourced adaptive platform for recommendation of learning activities. arXiv preprint arXiv:1910.05522.

Kho sr 2020   f  t  t 51st ACM technical symposium on computer science education (pp. 58- 64). New York, NY, USA: Association for Computing Machinery, http://dx.doi.org/10.1145/3328778.3366900. Lahza, H, Khosravi, H, Demartini, G, & Gasevic, D. (2022). Effects of technological interventions for self- regulation: A control experiment in learnersourcing. In LAK22, LAK22: 12th international learning analytics and knowledge conference (pp. 542- 548). New York, NY, USA: Association for Computing Machinery, http://dx.doi.org/10.1145/3506860.3506911. Mao, Z., & Lee, I. (2024). Student engagement with written feedback: Critical issues and way forward. RELC Journal, 55(3), 810- 818. Moore, S, Nguyen, H. A, Bier, N, Domadia, T, & Stamper, J. (2022). Assessing the quality of student- generated short answer questions using GPT- 3. In I. Hilliger, P. J. Munoz Merino, T. De Laet, A. Ortega- Arranz, & T. Farrell (Eds.), Educating for a new future: making sense of technology- enhanced learning adoption (pp. 243- 257). Cham: Springer International Publishing, http://dx.doi.org/10.1007/978- 3- 031- 16190- 9_18. Nardo, A. (2021). Exploring a Vygotskian theory of education and its evolutionary foundations. Educational Theory, 71(3), 331- 352. http://dx.doi.org/10.1111/edth.12485, .eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/edth.12485. Noroozi, O., Alqassab, M., Taghizadeh Kerman, N., Banihashem, S. K., & Panadero, E. (2024). Does perception mean learning? Insights from an online peer feedback setting. Assessment & Evaluation in Higher Education, 1- 15. http://dx.doi.org/10.1080/02602938.2024.2345669, Publisher: Routledge .eprint. O'Brien, H. L, & Toms, E. G. (2008). What is user engagement? A conceptual framework for defining user engagement with technology. Journal of the American Society for Information Science and Technology, 59(6), 988- 988. http://dx.doi.org/10.1002/asn20005, .eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.20801. Olga, A, Tzirides, Saini, A., Zapata, G., Searsmith, D., Cope, B., Kalantzis, M., Castro, V., Kourkoulou, T., Jones, J., da Silva, R. A., Whiting, J., & Kastania, N. P. (2023). Generative AI: Implications and applications for education. arXiv:2305.07605. Pekrun, R. (2006). The control- value theory of achievement emotions: Assumptions, corollaries, and implications for educational research and practice. Educational Psychology Review, 18, 315- 341. Pekrun, R., Frenzel, A. C., Goetz, T., & Perry, R. P. (2007). The control- value theory of achievement emotions: An integrative approach to emotions in education. In Emotion in education (pp. 13- 36). Elsevier. Pekrun, R., Goetz, T., Perry, R. P., Kramer, K., Hochstadt, M., & Molfenter, S. (2004). Beyond test anxiety: Development and validation of the test emotions questionnaire (TEQ). Anxiety, Stress & Coping, 17(3), 287- 316. Pekrun, R., Goetz, T., Titz, W., & Perry, R. P. (2002). Academic emotions in students' self- regulated learning and achievement: A program of qualitative and quantitative research. Educational Psychologist, 37(2), 91- 105. Phung, T., Padurean, V- A., Cambronero, J., Guhwani, S., Kohn, T., Majumdar, R., Singla, A., & Soares, G. (2023). Generative ai for programming education: Benchmarking chatgpt, gpt- 4, and human tutors. In Proceedings of the 2023 ACM conference on international computing education research- vol. 2 (pp. 41- 42). Pintrich, P. R. (2000). The role of goal orientation in self- regulated learning. In Handbook of self- regulation (pp. 451- 502). Elsevier. Ranalli, J. (2021). L2 student engagement with automated feedback on writing: Potential for learning and issues of trust. Journal of Second Language Writing, 52, Article 100816. http://dx.doi.org/10.1016/J.JSLW.2021.100816. Ryan, T., Henderson, M., Ryan, K., & Kennedy, G. (2024). Feedback in higher education: aligning academic intent and student sensemaking. Teaching in Higher Education, 29(4), 860- 875. http://dx.doi.org/10.1080/13562517.2022.2029394. Sargeant, J., Mann, K., Sinclair, D., Van der Vleeten, C., & Metsemakers, J. (2008). Understanding the influence of emotions and reflection upon multi- source feedback acceptance and use. Advances in Health Sciences Education, 13, 275- 288. Schunk, D. H. (1989). Social cognitive theory and self- regulated learning. In B. J. Zimmerman, & D. H. Schunk (Eds.), Self- regulated learning and academic achievement: theory, research, and practice (pp. 83- 110). New York, NY: Springer New York, http://dx.doi.org/10.1007/978- 1- 4612- 3618- 4. Schunk, D. H. (2005). Self- regulated learning: The educational legacy of Paul R. Pintrich. Educational Psychologist, 40(2), 85- 94. http://dx.doi.org/10.1207/s15326985ep4002_3, Publisher: Routledge. Springer. Shaik, T., Tao, X., Li, Y., Dann, C., McDonald, J., Redmond, P., & Galligan, L. (2022). A review of the trends and challenges in adopting natural language processing methods for education feedback analysis. IEEE Access, 10, 56720- 56739. Sharples, M. (2023). Towards social generative AI for education: theory, practices and ethics. Learning: Research and Practice, 9(2), 159- 167. Sjoberg, S. (2010). Constructivism and Learning. In P. Peterson, E. Baker, & B. McGaw (Eds.), International encyclopedia of education (3rd ed.). (pp. 485- 490). Oxford: Elsevier, http://dx.doi.org/10.1016/B978- 0- 08- 044994- 7.00467- X, URL https://www.sciencedirect.com/science/article/pii/B97800804489470- 467X. Stough, L. M., & Emmer, E. (1998). Teachers' emotions and test feedback. International Journal of Qualitative Studies in Education, 11, 341- 361. http://dx.doi.org/10.1080/095183998236809. Sutherland, S. C., Machado, T., Mahajan, S., Mohaddesi, O., Matuk, C., Smith, G., & Harteveld, C. (2023). Exploring the role of AI- generated feedback tangential to learning outcomes. In 2023 IEEE Conference on Games (CoG) (pp. 1- 8). http://dx.doi.org/10.1109/CoG7401.2023.10333239. Tabak, I., & Kyza, E. A. (2018). Research on scaffolding in the learning sciences: A methodological perspective. In International handbook of the learning sciences (pp. 191- 200). Routledge. Vygotsky, L. S. (1978). Mind in society: The development of higher psychological processes: vol. 86, Harvard University Press. Wan, T., & Chen, Z. (2023). Exploring generative AI assisted feedback writing for students' written responses to a physics conceptual question with prompting engineering and few- shot learning. arXiv preprint: arXiv:2311.06180. Weidlich, J., Fink, A., Jivet, I., Yau, J., Giorgiashvili, T., Drachsler, H., & Frey, A. (2024). Emotional and motivational effects of automated and personalized formative feedback: The role of reference frames. Journal of Computer Assisted Learning. Winstone, N. E., Nash, R. A., Parker, M., & Rowntree, J. (2017). Supporting learners' agentic engagement with feedback: A systematic review and a taxonomy of reciprocity processes. Educational Psychologist, 52(1), 17- 37. Wisniewski, B., Zierer, K., & Hattie, J. (2020). The power of feedback revisited: A meta- analysis of educational feedback research. Frontiers in Psychology, 10, Article 487662. Zawacki- Richter, O., Marín, V. L., Bond, M., & Gouverneur, F. (2019). Systematic review of research on artificial intelligence applications in higher education- where are the educators? International Journal of Educational Technology in Higher Education, 16(1), 1- 27. Zhang, Z. V., & Hyland, K. (2018). Student engagement with teacher and automated feedback on L2 writing. Assessing Writing, 36, 90- 102.