# Students' Perceptions of Generative AI-Powered Learning Analytics in the Feedback Process: A Feedback Literacy Perspective

Flora Ji- Yoon Jin<sup>1*</sup>, Bhagya Maheshi<sup>2</sup>, Wenhua Lai<sup>3</sup>, Yuheng Li<sup>4</sup>, Danijela Gasevic<sup>5</sup>, Guanliang Chen<sup>6</sup>, Nicola Charwat<sup>7</sup>, Philip Wing Keung Chan<sup>8</sup>, Roberto Martinez- Maldonado<sup>9</sup>, Dragan Gašević<sup>10</sup>, Yi- Shan Tsai<sup>11</sup>

# Abstract

This paper explores the integration of generative AI (GenAI) in the feedback process in higher education through a learning analytics (LA) tool, examined from a feedback literacy perspective. Feedback literacy refers to students' ability to understand, evaluate, and apply feedback effectively to improve their learning, which is crucial for fostering self- regulated learning and academic growth. GenAI has the potential to open new avenues of research and design in augmenting feedback practices by providing innovative, personalized, and scalable feedback solutions. The study investigates how GenAI functionalities, specifically ChatGPT explanation features and GenAI- powered dashboard visualizations, can support students in engaging with feedback. Using feedback literacy theory, a thematic analysis was conducted and triangulated with usage trace data to assess students' perceptions of these functionalities. The study involved three key activities: introductory lab sessions, in- semester use of the GenAI- powered LA feedback tool, and post hoc interviews, with data collected from 18 students from various disciplines (information technology, education, business and economics, and engineering) throughout all phases. Initial findings from the lab sessions showed positive perceptions of the GenAI functionalities. However, trace data from in- semester use indicated modest engagement with GenAI. Post hoc interviews revealed that reduced engagement was due to a mismatch between the GenAI outputs and student expectations. While some students appreciated the GenAI functionalities, others found them redundant when they perceived the feedback as clear and easy to understand. This study highlights the potential of GenAI in the feedback process and underscores the challenges of aligning AI tools with diverse student needs. Future developments should focus on creating adaptive and discipline- specific GenAI solutions.

# Notes for Practice

- Generative AI (GenAI) has the potential to support students' sense-making and action-taking processes in the feedback processes, which are key components of student feedback literacy.- The study highlights the discrepancy between students' initial positive perceptions of GenAI-powered functionalities as a first encounter and their actual use of those functionalities.- GenAI within the learning analytic tools may enable active student engagement with feedback by simplifying and clarifying lengthy and complex feedback across assessments and units, making it more understandable and actionable.Designing GenAI tools that are discipline specific to address the unique needs of different subject areas may enhance the relevance and effectiveness of the feedback.

# Keywords

Generative AI, learning analytics, feedback engagement, feedback interaction, feedback literacy.

Submitted: 08/08/2024 — Accepted: 18/02/2025 — Published: 18/03/2025

5Email: danijela.gasevic@monash.edu Address: School of Public Health and Preventive Medicine, Monash University, Melbourne, Australia. ORCID ID: https://orcid.org/0000- 0001- 5976- 4011 6Email: guanliang.chen@monash.edu Address: Faculty of Information Technology, Monash University, Melbourne, Australia. ORCID ID: https://orcid.org/0000- 0002- 8236- 3133 7Email: nicola.charwat@monash.edu Address: Faculty of Business & Economics, Monash University, Melbourne, Australia. ORCID ID: https://orcid.org/0000- 0002- 7519- 276X 8Email: philip.k.chan@monash.edu Address: Faculty of Education, Monash University, Melbourne, Australia. ORCID ID: https://orcid.org/0000- 0003- 1605- 1665 9Email: Roberto.MartinezManonado@monash.edu Address: Faculty of Information Technology, Monash University, Melbourne, Australia. ORCID ID: https://orcid.org/0000- 0002- 8375- 1816 10Email: Dragan.Gasevic@monash.edu Address: Faculty of Information Technology, Monash University, Melbourne, Australia. ORCID ID: https://orcid.org/0000- 0001- 9265- 1908 11Email: Yi.Shen.Teei@monash.edu Address: Faculty of Information Technology, Monash University, Melbourne, Australia. ORCID ID: https://orcid.org/0000- 0001- 8967- 5327

# 1. Introduction

Feedback processes are foundational in educational development, encompassing the various ways in which information about student performance is generated, communicated, and used to enhance learning (Hattie & Timperley, 2007; Henderson et al., 2019). In higher education, the feedback processes extend beyond simple assessment, often acting as a dialogue that aids learners in identifying their strengths and areas for improvement (Yang & Carless, 2013). Yet, for feedback processes to be effective, it is essential that students not only receive feedback information but also understand and apply it to enhance their learning. This required capability for students to effectively engage with feedback is known as student feedback literacy, which involves the ability to actively seek, understand, and effectively apply feedback (Carless & Boud, 2018). Developing feedback literacy is crucial as it transforms students from passive recipients of information into active agents in their learning processes. By fostering skills such as self- assessment, actively seeking clarification from educators, self- reflection, and strategic planning, feedback literacy empowers students to take ownership of their feedback processes (Carless & Boud, 2018; Molloy et al., 2020). This active engagement leads to enhanced self- regulation, enabling students to monitor their progress, adjust their strategies, and ultimately improve their academic performance (Carless & Boud, 2018; Molloy et al., 2020).

As the educational landscape evolves, the potential of using educational technologies such as learning analytics (LA) to support the feedback process and the development of feedback literacy is increasingly recognized (Carless & Boud, 2018; H. Jin et al., 2022). LA tools, leveraging large data sets to analyze student learning patterns, offer a promising avenue to enhance students' engagement with feedback processes (Lim et al., 2021; Pardo et al., 2019). With the growing hype in the field of AI, generative AI (GenAI), such as ChatGPT, has further expanded the possibilities within this domain (Dai et al., 2023; Yan, Sha, et al., 2024; Yan, Greiff, et al., 2024; Dai et al., 2024). GenAI tools are now increasingly integrated into educational settings to provide timely feedback, potentially reducing teachers' workload and enhancing the feedback experience (Conijn et al., 2022; Rad et al., 2023; Jurgensmeier & Skiera, 2024). These AI- powered tools can deliver immediate, personalized responses to students, which is particularly valuable in large or resource- constrained educational environments (Rad et al., 2023). Furthermore, AI is increasingly recognized as playing a crucial role in feedback processes as a feedback agent. As Pardo (2018) pointed out in his feedback model for data- rich learning experiences, algorithms can be integral components of the feedback loop. GenAI extends this concept by offering more context- aware feedback generation that can potentially enhance the quality and relevance of feedback (Dai et al., 2024). This evolution in AI's role highlights the importance of exploring how GenAI can be effectively integrated into feedback processes to support both educators and learners.

Despite the growing enthusiasm for using GenAI technologies to enhance the feedback process, the research base is nascent. In particular, there is a lack of studies focusing on how students directly interact with GenAI within student- facing tools to manage their own feedback. Currently, GenAI applications are primarily designed to assist educators in drafting and automating feedback (Conijn et al., 2022; Tubino & Adachi, 2022), yet only a few studies have directly engaged students in interacting with GenAI applications to manage their own feedback. This highlights a need for empirical research to explore how students perceive and interact with GenAI- powered functionalities aimed at supporting the feedback process in educational settings. Employing feedback literacy literature as a theoretical lens can offer a structured approach to examine how students engage with, interpret, and act upon feedback facilitated by GenAI- powered functionalities within an LA tool (F. Jin et al., 2024; Tsai, 2022). This approach not only can bridge the gap between traditional feedback theories and emerging AI technologies but also may provide a deeper understanding of the potential impacts of GenAI technologies on students' ability to effectively make sense of, engage with, and use feedback—key components of student feedback literacy.

The current study aimed to bridge this gap by exploring students' perceptions after using the GenAI- powered functionalities integrated within a student- facing LA tool called PolyFeed. The focus was on understanding how these functionalities could support students' sense- making and action- taking processes (e.g., self- reflection, monitoring) in the context of the feedback

processes. By focusing on students' perceptions of GenAI within PolyFeed, namely a ChatGPT explanation feature and the derivation of common themes from labelled feedback for dashboard visualizations, this research explores the role of GenAI in the feedback process through the lens of students' feedback literacy. Data were collected through (i) introductory lab sessions, to capture initial perceptions of GenAI in the feedback processes; (ii) in- semester trace data, to observe actual use and engagement; and (iii) post hoc interviews, to explore changes in perceptions over time. This approach provided a comprehensive view of how students perceive and interact with GenAI- powered functionalities during the semester, offering insights into both the perceived value and the practical use of these functionalities in an educational setting. The contribution of this paper lies in providing empirical insights into how students perceive and interact with GenAI- powered functionalities after using an LA tool as a part of their feedback process. This study highlights the potential of GenAI to facilitate meaningful interaction with feedback received from educators, helping students understand and use feedback effectively.

# 2. Background

# 2.1 Foundations of Feedback and Feedback Literacy

Feedback processes are essential for learning, providing students with insights into their performance and pinpointing areas for improvement (Hattie & Timperley, 2007; Henderson et al., 2019). Traditionally, feedback processes in higher education were largely unidirectional, focusing on the delivery of feedback information from educator to student. However, recent research advocates for a more dialogic approach to feedback, promoting a two- way communication process that is responsive to individual student needs and fosters deeper engagement (Boud & Molloy, 2013; Sutton, 2009; Yang & Carless, 2013). Effective feedback processes extend beyond traditional assessments and transmission of information, acting as a vital interaction that supports learners in recognizing their strengths, weaknesses, and areas for improvement. This interactive process is crucial because the impact of feedback is realized only when students not only receive but also make sense of and act upon the feedback they are given. This requirement is central to the concept of student feedback literacy, defined as the capability to actively seek, understand, and apply feedback in a constructive way (Carless & Boud, 2018). Developing feedback literacy involves more than processing information; it requires engaging in a dialogic feedback process where feedback is not merely given but part of an ongoing dialogue that enhances students' ability to self- regulate and optimize their learning strategies (Boud & Molloy, 2013; Sutton, 2009).

Student feedback literacy comprises four key elements: appreciating feedback, making judgments, managing affect, and taking action (Carless & Boud, 2018). More specifically, students should be able to recognize the value of feedback and understand their active role in the feedback process (appreciating feedback). They should be able to evaluate the quality of feedback and make judgments about their own work (making judgments), be emotionally capable of dealing with both positive and negative feedback (managing affect), and know the strategies necessary to act on feedback (taking action) (Carless & Boud, 2018). Molloy and colleagues (2020) have proposed a comprehensive framework that includes seven capabilities, enhancing these traditional components by detailing specific behaviours and attitudes necessary for developing feedback literacy. These capabilities are (1) committing to feedback as improvement, (2) appreciating feedback as an active process, (3) eliciting information to improve learning, (4) processing feedback information, (5) acknowledging and working with emotions, (6) acknowledging feedback as a reciprocal process, and (7) enacting outcomes of processing of feedback information. Each capability highlights the participatory nature of feedback and emphasizes the importance of viewing feedback as a tool for improvement.

Taking a feedback literacy lens is crucial for understanding how students engage with, interpret, and act on feedback, particularly given the importance of feedback literacy skills within the feedback processes (Carless & Boud, 2018; Molloy et al., 2020). This perspective can provide a structured way to explore the complexities of the feedback process and to identify areas where students may need additional support. Providing personalized interventions and supporting students in the feedback process is challenging for educators due to their limited time and resources (H. Jin et al., 2022). In this context, LA and GenAI have been proposed as potential technologies that streamline and facilitate a more manageable and scalable approach to feedback processes (Lim et al., 2021; Rad et al., 2023; Pardo et al., 2019; Escalante et al., 2023).

# 2.2 LA and GenAI in the Feedback Process

LA has emerged as a promising technology- enhanced approach to facilitating the feedback process (Carless & Boud, 2018; Yang & Carless, 2013; Winstone, 2019; Tsai, 2022). Specifically, LA can be used to generate timely and personalized feedback by using extensive learning data such as click stream, discussion forum, and academic performance (i.e., assessment marks, class attendance), which not only improves students' experiences but also alleviates the burdens faced by educators in managing large cohorts of students (Lim et al., 2021; Pardo et al., 2019). Existing LA tools primarily focus on using diverse engagement and performance data from platforms such as learning management systems to enhance the content, frequency, and timeliness of feedback. This includes the delivery of automated feedback in written form or through visualizations (e.g., dashboards) (Knight et al., 2020; Sedrakyan et al., 2020; Whitelock et al., 2015; Zheng et al., 2022).

Recent advancements in AI, specifically GenAI, have begun to influence the feedback process by facilitating dialogic interactions that can potentially mirror human- to- human communication. Technologies such as ChatGPT have shown potential in providing flexible and contextually relevant feedback through human- to- chatbot interactions, enhancing both the learning experience and real- time student engagement (Kim et al., 2021). For example, a case study by Rad and colleagues (2023) explored the use of the Wordtune application in writing tasks. Wordtune, a GenAI writing tool, uses natural language processing (NLP) to aid in writing. The study found that the experimental group using Wordtune significantly improved their writing outcomes, engagement, and writing feedback literacy compared to the control group. Students responded positively to the Wordtune application, highlighting its effectiveness in enhancing their writing skills. This emphasizes the role of GenAI in providing personalized feedback, promoting student engagement, and developing writing feedback literacy—a set of skills that enables students to understand, interpret, and use feedback to improve their writing outcomes in educational settings (Rad et al., 2023). However, the application of GenAI tools in this case study is too task specific to language education and may not necessarily be suitable for a broader range of feedback processes to support students' overall feedback literacy.

GenAI holds the potential to generate and explain feedback. Research by Dai and colleagues (2023, 2024) explored the potential use of ChatGPT in feedback generation, evaluating the quality and depth of its output compared to feedback provided by educators. The findings suggested that ChatGPT could produce feedback that was more detailed and readable than that from human educators. These results imply ChatGPT's potential to assist educators in delivering more readable and consistent feedback, thereby potentially enhancing students' sense- making process. This potential of ChatGPT in feedback processes has been further corroborated by recent research. Research by Li and colleagues (2025) demonstrated that students were willing to follow and comply with feedback generated by GPT- 4 based on analytics of their learning behaviours. This study highlights the promising potential of GenAI for feedback generation and learners' receptiveness to implementing feedback. Additionally, Yan and colleagues (2024) highlight that large language models (LLMs), such as GPT, can generate text- based content to summarize complex datasets and visualizations, providing explanatory narratives and crafting data stories with minimal human oversight. This capability is particularly valuable for helping students interpret and understand LA visualizations. The study emphasizes the potential of GenAI to extend beyond traditional LA by offering more interactive and explanatory capabilities, thereby enhancing student engagement with feedback processes. A recent follow- up study explored two different implementations of VizChat: a "reactive" condition, where students asked questions independently, and a "proactive" condition that scaffolded students in asking questions as they interpreted dashboard visualizations (Yan, Martinez- Maldonado, Echeverria, et al., 2024). Both conditions showed significant improvement in students' comprehension and accuracy of interpreting the analytics displayed in the dashboard. Students in the "proactive" condition continued improving even after chatbot access was removed, while those in the "reactive" condition maintained their performance level. These findings have significant implications for the use of GenAI in feedback processes, which could lead to more sustained improvements in students' ability to interpret and act on feedback.

Despite these technological advancements, there is a significant gap in how LA and GenAI are currently applied in feedback processes. Primarily, LA and GenAI have been used to aid educators in delivering feedback, whether semi- automated or fully automated (Conijn et al., 2022; Tubino & Adachi, 2022; Knight et al., 2020; Sedrakyan et al., 2020; Whitelock et al., 2015; Zheng et al., 2022; Jurgensmeier & Skiera, 2024). Yet, little attention has been paid to the importance of empowering students to actively manage their own feedback in this process. Understanding students' perceptions of using GenAI functionalities in the feedback process is crucial because it can reveal how these tools promote active student engagement, awareness, and initiative in managing their feedback. Research in this area can demonstrate how direct interaction with LA and GenAI tools can support students in the feedback processes, which involve actively seeking, understanding, reflecting on, and applying feedback effectively—key components of feedback literacy (Carless & Boud, 2018; Molloy et al., 2020). This understanding is essential for designing systems that truly support and empower students in their learning journeys.

# 2.3 Contribution and Research Questions

The current study aimed to address the aforementioned gap by exploring how students perceive and interact with the GenAI- powered functionalities within a student- facing LA tool aimed at supporting the feedback process. Specifically, this study focused on students' perceptions of GenAI's role in supporting their understanding and use of feedback. To investigate this, the study used PolyFeed, which integrated GenAI in two key functionalities: ChatGPT explanations, which allow students to ask for further explanations on feedback received from educators, and a GenAI- powered dashboard that visualizes common themes (i.e., strengths and weakness) across assessments and units summarized by ChatGPT. Through introductory lab sessions, in- semester use of the tool, and post hoc interviews, the study captured students' initial perceptions, their actual engagement, and changes in their perceptions over time. These activities aimed to provide insights into how GenAI could potentially support students' interactions with feedback. The analysis was framed through the lens of feedback literacy, addressing the following overarching question: What may be the role of GenAI in the feedback process from a feedback literacy perspective? Investigating the role of GenAI from this perspective is crucial because feedback literacy emphasizes the skills and understanding necessary for students to make effective use of feedback (Carless & Boud, 2018; Molloy et al., 2020). By

examining GenAI's impact on these areas, we aimed to understand how LA technologies could support students in becoming more proactive and competent in engaging with feedback. This overarching question was explored through two research questions to capture students' engagement and perceptions.

RQ1 aimed to capture the initial impressions and immediate reactions of students as they are introduced to the GenAI- powered functionalities, setting the baseline for their engagement:

RQ1: What are students' initial perceptions about using GenAI- powered functionalities within an LA tool aimed at supporting the feedback process?

Going beyond a first encounter with the LA tool (Q. Li et al., 2021), RQ2 aimed at understanding students' perceptions from their ongoing experiences during the semester and their reflective insights captured in the post- interviews, providing a empirical perspective on the continuous use of GenAI- powered functionalities in their feedback and learning processes, as follows:

RQ2: What are students' perceptions after the actual use of GenAI- powered functionalities within an LA tool aimed at supporting the feedback process?

The current study contributes to the LA field by offering empirical insights into students' perceptions and interactions with GenAI in feedback processes, highlighting both the potential and challenges of integrating these technologies into educational settings. The focus on student perceptions is intentional and particularly valuable at this early stage of research, providing insights into the potential of GenAI to support feedback processes from the learners' perspective, identifying areas where GenAI tools align with or fall short of student needs and expectations and informing future directions for both tool design and more targeted studies in scaffolding feedback literacy.

# 3. Methodology

The study presented in this paper was based on the data collected during the pre- implementation testing phase of an LA tool aimed at supporting the feedback process. Ethics approval was obtained from the ethics committee of Monash University for the design, development, and piloting of PolyFeed.

# 3.1 Tool Features and Functionalities

PolyFeed is an LA tool designed to support students in managing and interacting with web- based feedback through a unified platform. PolyFeed consists of a browser extension and a dashboard. The browser extension enables students to interact with their feedback by highlighting and labelling feedback as strengths, weaknesses, suggestions (action points), confusions, or other categories (Figure 1—steps 1 and 2). Students can then reflect (Figure 1—step 3) or create action plans (Figure 1—step 4) based on the extracted feedback. The highlighted and labelled feedback is stored in the PolyFeed database and used to derive common themes with support from ChatGPT. Initially, the annotated feedback extracts are sent to ChatGPT, which is then prompted to derive common themes emerging from the feedback extracts. Subsequently, new feedback extracts and pre- identified common themes are sent to ChatGPT to map the feedback extracts into the common themes. In the case of a missing common theme from the pre- identified list, ChatGPT is prompted to add new themes to match. Therefore, PolyFeed leverages ChatGPT for inductive qualitative coding. The identified common themes are then visualized in the dashboard, showcasing common strengths and weaknesses across assessments and units (i.e., equivalent to courses in other educational institutions) (Figure 2).

Additionally, PolyFeed allows students to seek further explanation on feedback they received using ChatGPT (see Figure 3 for steps). The primary purpose of this feature is to integrate ChatGPT to reformat feedback into simpler language and present it as a summary. The use of ChatGPT to seek further explanation is limited to two attempts. This design decision was made to ensure that students would not receive unreliable responses from ChatGPT due to the loss of context if they select a smaller portion of feedback. Specifically, PolyFeed provides one explanation per feedback item from ChatGPT (Figure 3—step 1). If a student is not satisfied with the ChatGPT response, they are given one additional chance to interact with ChatGPT to seek further explanation (see Figure 3—step 5).

In summary, this study focused on two features supported by GenAI, both using version 3.5 of ChatGPT: (i) the ability for students to request ChatGPT explanations if they were confused by specific feedback sentences and (ii) the visualization of common strengths and weaknesses across assessments and units, derived from students' feedback annotations, such as highlights and labels. Thematic analysis conducted by ChatGPT on these feedback extractions was displayed on a dashboard.

The rationale for integrating two ChatGPT functionalities in the tool is as follows:

![](images/ae5bd74cd5a239acbca10d956b45dbd2ed195b273d9797d9453f64ef9bf46a04.jpg)  
Figure 1. Steps of PolyFeed's functions: (1) Students highlight the feedback text in Moodle; (2) students label the highlighted feedback as strength, weakness, action point, confused, or other; (3) students select to add a note; or (4) students select to make action plan.

ChatGPT explanations: This functionality allows students to request further clarification on specific feedback sentences they receive from educators. ChatGPT explanations were designed to support students in seeking feedback information, particularly in resource- constrained teaching and learning environments where educators may have limited time and resources (H. Jin et al., 2022). By providing immediate clarification on feedback, this functionality addresses the making sense of feedback component of feedback literacy (see Section 2.1 for more details). The ability to quickly obtain explanations enhances students' understanding of the feedback, potentially reducing emotional barriers (i.e., managing emotions) to engagement and fostering more effective interpretation. This improved comprehension can, in turn, support the using feedback information component of feedback literacy, as students are better equipped to take actionable steps based on their enhanced understanding of the feedback received (Carless & Boud, 2018).

GenAI- powered visualizations: This functionality provides students with a summarized view of their annotated feedback, highlighting strengths and weaknesses across multiple assessments and units (see Figure 2). This consolidated view can support students in understanding their feedback more holistically, aligning with the making sense of feedback component of feedback literacy. By prompting reflection on these patterns, these visualizations can potentially encourage students to engage in strategic planning and take action based on their feedback, which corresponds to the using feedback information component of feedback literacy. This approach aims to deepen students' comprehension of their overall performance and foster more effective use of feedback across their academic journey.

Both functionalities align with multiple components of feedback literacy, including seeking feedback information, making

![](images/ce9f1701bc595c25b81d7c87e3ccc867c8482133feac9c7e7cff469a0e18a034.jpg)  
Figure 2. PolyFeed's dashboard.

sense of feedback, managing emotions, and using feedback information. These feedback literacy components are bolded and discussed in more detail in Section 3.3.

# 3.2 Study Design

The overall study protocol for the pilot of PolyFeed involved three main activities: an introductory lab session, in- semester use, and a post hoc interview (Figure 4). Twenty- two students were recruited for the introductory lab session through the professional networks of the PolyFeed project's researchers, including unit coordinators from various faculty disciplines. The participants in the introductory lab sessions consisted of students from the following disciplines: information technology  $(50\%)$ , education  $(22.7\%)$ , business and economics  $(13.6\%)$ , and engineering  $(13.6\%)$ .

Activity 1. During the introductory lab session, 22 students used PolyFeed to interact with at least three pieces of feedback they had received in the previous semester across units they took and assessments within them, engaging in both free and guided exploration (Figure 4—Introductory lab session). The first interaction was self- directed, allowing students to explore the tool's functionalities without guidance to gain an initial understanding of its features. The subsequent two interactions were guided, with specific steps provided to help students effectively use the tool's main functionalities for the remaining feedback. These three interactions enabled students to engage with all GenAI- powered functionalities within the tool and share their positive or negative feedback, providing insights into their experiences with each functionality. For the ChatGPT explanation function, students were encouraged to rate the output from ChatGPT on a scale from 1 to 4, providing quantitative data on their satisfaction with the responses. This 4- point scale was chosen to encourage students to make a clear decision rather than defaulting to a neutral middle option, thereby providing more definitive feedback about their experiences with the GenAI tool

![](images/d73ca669d7611969fd2bcbc7af4905bf4407f8168210b03b0229c2cf3e45b1de.jpg)

![](images/90a82fc1613f7d774f01c9de45b13e860a688ed83e4c9ed0af72b922e939289a.jpg)  
Figure 3. Steps of using ChatGPT within PolyFeed to seek further clarification: (1) Students click the Explanation from ChatGPT button in the extension; (2) students highlight the text; (3) students click the ASK CHATGPT button; (4) students rate the response received from ChatGPT; and (5) students seek further clarification for a second time.

(Filius et al., 2018). All sessions were audio- recorded using Zoom, a popular video conferencing tool that also offers recording and transcribing services, to capture detailed insights into students' experiences and opinions of each functionality.

Activity 2. During the in- semester use activity, 18 students actively and continuously used PolyFeed to interact with feedback received during the current semester (Figure 4- in- semester use). Participants were required to interact with at least six pieces of feedback received in their current semester across units and assessments. Contents students created during their interactions (e.g., highlight extractions, label, notes, action plans, ChatGPT explanations, ratings on feedback and ChatGPT explanations) and trace data were collected.

Activity 3. By the end of the semester, all 18 students who participated in Activity 2 also took part in the post hoc interviews (Figure 4- Post- study interview). Due to the voluntary nature of participation, we were not able to capture broader issues faced by students who did not continue to use the tool after their initial lab session. The post hoc interview questions (refer to the study protocol) were carefully designed by two researchers who discussed and agreed upon the questions needed to cover various dimensions of feedback literacy (Carless & Boud, 2018; Molloy et al., 2020). This approach aimed to capture students' general experiences within the feedback processes and understand their perceptions of PolyFeed's role in supporting these dimensions of feedback literacy. During these interviews, the students were asked whether the ChatGPT explanation function encouraged them to seek clarification on the feedback they received and whether the GenAI- powered visualizations (i.e., common strengths and weaknesses across assessment and units) in the dashboard were useful for their learning. All interviews were conducted and audio- recorded by Zoom. Additionally, data from the 18 students who participated in all activities were considered for subsequent data analysis, with students labelled from P01 to P18 for presenting their direct quotes in the results section. For clarity and grammatical correctness, some words in students' direct quotes have been modified or added, as indicated by square brackets. These modifications were made carefully to preserve the original meaning of the statements.

# 3.3 Data Analysis

In this paper, we analyzed students' interaction data and trace data stored in the PolyFeed database from in- semester use (Activity 2) (Figure 4- In- semester use) and their interview responses in the introductory lab sessions (Activity 1) and post hoc interviews (Activity 3) (Figure 4- Post- study interview). The data analysis of Activity 1 was aimed at answering RQ1, while the analyses from Activity 2 and Activity 3 are aimed at answering RQ2.

![](images/b5e4bf8d0e019d24b44b6cce749cb1c1eb0aac4892f394db599d2c857853c2e6.jpg)  
Figure 4. Overview of PolyFeed pilot study activities.

Activity 2. We present descriptive statistics regarding the number of participants who used ChatGPT to seek further explanations, along with the percentage of students using this GenAI- powered functionality. Additionally, this paper presents the results of the analysis of trace data related to the frequency with which different visualizations in the dashboard were accessed, including Strengths across Units, Weaknesses across Units, Strengths across Assessments, and Weaknesses across Assessment (Figure 2).

Activity 1 and Activity 3. In analyzing the interviews with students, the audio recordings were initially transcribed using Zoom's automatic transcription service and corrected by research team members by cross- referencing the audio file and the transcripts. Thematic analysis was used to analyze these transcripts. A coding scheme was developed using both inductive and deductive methods. The inductive codes were developed based on the themes emerged from the dataset. Conversely, the deductive codes were informed by the dimensions of the feedback literacy framework discussed in Section 2.1 (Carless & Boud, 2018; Molloy et al., 2020).

Initially, codes were developed to explore how GenAI within PolyFeed can potentially support feedback literacy skills (Code: 1.0 Feedback Literacy). This was further dissected into five components by synthesizing key elements from two seminal frameworks (Carless & Boud, 2018; Molloy et al., 2020). Appreciation of feedback is highlighted in both frameworks as recognizing the value of feedback for learning, which is essential for meaningful engagement. Seeking feedback information involves proactively eliciting additional feedback or clarifications to better make sense of feedback. Making sense of feedback combines Carless and Boud's emphasis on evaluating feedback with Molloy and colleagues' focus on accurate interpretation. Managing emotion, discussed in both frameworks, involves maintaining a positive attitude toward feedback, even when it is critical. Lastly, using feedback information encapsulates applying feedback constructively to improve work and learning strategies through self- reflection and monitoring, a culmination of processing and understanding of feedback. Additionally, codes were developed to capture students' positive and negative perceptions and concerns toward the use of GenAI- enhanced functionalities (Code: 2.0 Perceptions of GenAI functionalities). The complete code scheme used in this study can be accessed at https://docs.google.com/document/d/e/2PACX- 1vTZ0ueP4YPO3I5iQk- i55jF1o09jmYIEpGJ4IhjnJGV93JMWa4yLQqRVBmXI- VAmrraGPLuNZtqEb7/pub.

Once the coding scheme was developed, an inter- rater reliability (IRR) test was carried out among three coders (R1, R2,

and R3). The first round of IRR testing between R1 and R2 yielded a Kappa score of 0.63. The coders then met to discuss discrepancies and reach a consensus on the most appropriate codes. This process involved

identifying specific instances where codes differed between coders (i.e., R1 and R2); each coder explaining their rationale for the chosen code; and engaging in collaborative discussion to reach a consensus.

Following this discussion, a second round of coding was conducted, resulting in an improved Kappa score of 0.83, which indicates "near perfect agreement" (Lazar et al., 2017). After achieving this satisfactory level of agreement, R1 and R2 divided the remaining transcripts from the introductory lab session equally for coding.

For R1 and R3, who coded the post hoc interview transcripts, only one round of IRR testing was conducted, achieving a Kappa score of 0.73, indicating a range from "satisfactory to near satisfactory agreement" (Lazar et al., 2017). The coders then met to discuss any discrepancies and reach a consensus, following a similar process as described above. After resolving discrepancies, R1 and R3 divided the post hoc interview transcripts equally for coding. This iterative process of coding, discussion, and refinement contributed to improving IRR and ensuring consistency in the analysis of both the introductory lab session and post hoc interview data. In reporting the findings, the number associated with a specific code is indicated (e.g.,  $n = 10$ , meaning 10 participants mentioning a particular code). Finally, all codes were bolded to improve readability.

# 4. Results

# 4.1 Students' Initial Dispositions toward Using GenAI-Powered Functionalities (RQ1)

During the introductory lab sessions, students explored PolyFeed and shared their perceptions of the GenAI- powered functionalities within the tool. The results from the qualitative analysis (see Table 1) highlighted various aspects of how students engaged with and perceived the GenAI- powered functionalities (i.e., ChatGPT explanation and GenAI- powered visualization) within PolyFeed. In terms of overall initial perceptions of the GenAI functionalities within PolyFeed, all students  $(n = 18, 100\%)$  reacted positively to the explanations (i.e., at least one trial out of three) and 16 (89%) to the GenAI- powered visualizations. When discussing how the two GenAI- powered functionalities within PolyFeed may support their learning in the feedback process, nine students (50%) mentioned that the explanation function may help them better understand the feedback (Code 1.3: making sense of feedback), noting that the responses from ChatGPT are more simplified, summarized in bullet points, than the original feedback provided by their educators. This can be particularly useful for clarifying vague feedback. For example, P18 said, "The reason I use ChatGPT here is mainly because sometimes the feedback [can be] a bit vague, so it's really awesome for me to have a button to ask for explanation from ChatGPT." Additionally, two students (11%) said that the explanation function helped them actively seek clarification on confusing parts of the feedback to enhance their understanding (Code 1.2: seeking feedback information). For instance, P04 valued this function because it was a more convenient way to seek clarifications directly with ChatGPT: "[It's] like grouping in and clarifying with ChatGPT directly, which is quite useful. It saves everything."

Table 1. Student responses to GenAI functionalities during introductory lab sessions and post hoc interviews.  

<table><tr><td rowspan="2"></td><td colspan="2">Introductory Lab Sessions</td><td colspan="2">Post Hoc Interviews</td></tr><tr><td>ChatGPT Explanation</td><td>Visualizations (GenAI)</td><td>ChatGPT Explanation</td><td>Visualizations (GenAI)</td></tr><tr><td>1. Feedback Literacy</td><td>10</td><td>15</td><td>6</td><td>12</td></tr><tr><td>1.1 Appreciation of Feedback</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1.2 Seeking Feedback Information</td><td>2</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1.3 Making Sense of Feedback</td><td>9</td><td>8</td><td>5</td><td>6</td></tr><tr><td>1.4 Managing Emotions</td><td>0</td><td>0</td><td>3</td><td>3</td></tr><tr><td>1.5 Using Feedback Information</td><td>0</td><td>14</td><td>1</td><td>10</td></tr><tr><td>2. Perceptions of GenAI Functionalities</td><td>18</td><td>17</td><td>12</td><td>14</td></tr><tr><td>2.1 Positive (GenAI)</td><td>18</td><td>16</td><td>8</td><td>13</td></tr><tr><td>2.2 Negative (GenAI)</td><td>11</td><td>6</td><td>6</td><td>3</td></tr><tr><td>2.3 Concerns (GenAI)</td><td>2</td><td>4</td><td>1</td><td>0</td></tr></table>

For GenAI- powered visualizations, 14 students mentioned that seeing their common strengths and weaknesses displayed may assist them in effectively using feedback information. These visualizations prompted them to reflect on their previous work and can potentially provide an overview of their performance across assignments and units, enabling them to use feedback information to improve their learning. Student P05 illustrated this point clearly: "I believe each unit has a common running

theme. If I identify those strengths and weaknesses in the initial assignments, I'll be well- prepared to specifically tackle them going forward. I can further show off my strengths and target my weaknesses more effectively. As I move forward, I can address these areas in [upcoming] assessment." Additionally, eight students  $(44\%)$  noted that visualizing common strengths and weaknesses in the dashboard can help them better make sense of feedback because \*It summarizes everything into one word. Instead of having too many words, it is very brief and short" (P17).

The GenAI functionalities also received some criticism, with 11 students  $(61\%)$  sharing negative feedback about the explanations compared to six  $(33\%)$  expressing dissatisfaction with the visualizations. Students mainly reacted negatively because the explanations from ChatGPT were often \*the same as the educators' feedback" (P02) and sometimes \*irrelevant to their work" (P18). They expected to receive additional information beyond what their educators provided, particularly advice on how to improve and effectively address the weaknesses that were identified. For example, student P10 hoped that ChatGPT would advise on \*What should I have done so that I could have actually fixed my assignment to not receive that negative feedback?" Concerns raised by students were relatively minimal, with only two students  $(11\%)$  expressing concerns about the explanations and four  $(22\%)$  about the visualizations. The main concerns regarding GenAI included potential reliability issues due to the use of an earlier version of ChatGPT, as well as privacy concerns, specifically whether their usernames or student details were being sent to ChatGPT.

Overall, the results showed a strong initial disposition toward using GenAI- powered functionalities within the LA tool, with students perceiving its effectiveness in seeking, making sense of, and using feedback. However, there was a mismatch between the output from the GenAI- powered functionality and students' expectations.

# 4.2 Students' Perceptions after Actual Use of GenAI-Powered Functionalities (RQ2)

To understand how students' perceptions of using the GenAI- powered functionalities changed based on their ongoing experiences, we analyzed their actual usage during the semester and their reflective insights gathered during the post hoc interviews.

# 4.2.1 In-Semester Use

Throughout the semester, trace data captured how students interacted with PolyFeed at home, particularly focusing on their use of GenAI- powered functionalities. The analysis of these data provides insights into students' practical engagement with the tool's features (see Table 2).

A total of nine students  $(50\%)$  used the ChatGPT explanation function, with their ratings on the responses averaging a score of 3.23 out of a maximum of 4. This score indicates a moderate level of satisfaction with the AI- generated explanations. In terms of navigating through GenAI- powered visualizations, 12 students  $(67\%)$  engaged with the common strengths across units graph, and 11 students  $(61\%)$  engaged with the common weaknesses across units graph, demonstrating a higher interaction with these visualizations. The common strengths across assessment graph and the common weaknesses across assessment graph were navigated by nine students  $(50\%)$  each. These data showed that engagement with GenAI- powered functionalities varied, with approximately  $50–60\%$  of students using these features.

Table 2. Number of participants that used the GenAI functionalities.  

<table><tr><td>Feature</td><td>Number of Participants (%)</td></tr><tr><td>ChatGPT explanation function</td><td>9 (50%)</td></tr><tr><td>Common strengths across units graph</td><td>12 (67%)</td></tr><tr><td>Common weaknesses across units graph</td><td>11 (61%)</td></tr><tr><td>Common strengths across assessment graph</td><td>9 (50%)</td></tr><tr><td>Common weaknesses across assessment graph</td><td>9 (50%)</td></tr></table>

# 4.2.2 Post Hoc Interviews

In general, students' perceptions of GenAI- powered functionalities (see Table 1) between the introductory lab sessions and post hoc interviews generally indicate a decline in the perceived effectiveness of these tools. Analysis revealed that 13 students  $(72\%)$  found the GenAI- powered visualizations beneficial to their learning in the feedback process. Specifically, 10 students  $(56\%)$  appreciated the visualizations for providing a clear overview of their strengths and weaknesses across different assignments and units. For example, one student remarked, \*I think the most fantastic thing is the visualization. Yeah, I can use that visualization, especially the graphs, to detect the areas where I need to improve. Do I need to make improvements again? Is there anything I might have ignored?" (P18). This suggests that the GenAI- powered visualizations can support the feedback literacy component of using feedback information effectively by prompting students to reflect on their performance and allowing them to monitor their progress. This, in turn, can enable them to apply feedback information to adjust their learning strategies for improvement.

Further insights from the interviews highlighted that GenAI- powered visualizations helped six students  $(33\%)$  better understand the feedback they received, suggesting understanding and application of the feedback in subsequent tasks. These

visualizations, supported by GenAI, effectively analyze and categorize complex, jumbled feedback into distinct categories of common strengths or weaknesses. P11 emphasized this potential benefit, stating, "Because some of the feedback is very jumbled, it's not laid out [distinctly]. The visualization [helps me] segregate the feedback. It gives me strengths or weaknesses separately. [This way] I know in a huge paragraph, there are like four main important points that I need to focus on." Additionally, the same feature supported three students  $(17\%)$  in managing their emotional responses to feedback. These students appreciated how GenAI- powered visualizations reduced the overwhelming nature of receiving complex feedback. By simplifying and distilling key points (i.e., common strengths and weaknesses) from the dense text provided by their educators, these visualizations helped them to maintain a positive outlook and fostered a focused approach to improvements. P08 explained the emotional benefits as follows: "I would say yes, because I'm thinking of a positive way, being able to highlight those weaknesses and to sort of analyze them as a common theme in the dashboard, is sort of turning them into like a tool or strength that we as students can use to improve."

In terms of ChatGPT explanations, eight students  $(44\%)$  found them particularly beneficial for clarifying complex feedback, as reflected in the experiences of five students  $(28\%)$  who specifically highlighted their value in making sense of feedback. For instance, student P09 said, "With the ChatGPT explanation, it analyzes the feedback and presents it in dot points. This condenses the feedback into key points, which makes it easier for me to understand when the feedback is extensive." Furthermore, three students  $(17\%)$  also reported that the ChatGPT explanations were instrumental in helping them manage their emotions during the feedback process. For example, student P06 highlighted the ChatGPT function as a valuable intermediary, providing a layer of support before they approached their instructors. This potential benefit was particularly appreciated by the student who felt hesitant about direct communication with their tutors due to cultural norms or personal anxiety: "I think this was good because it's like having a study buddy to consult with, not really to share feelings, but to have someone to talk to before I reach out to the tutor. You know, I've actually reached out to my tutor, but before that, I struggled a lot. I don't know if you understand, but in many Asian cultures, it's not really comfortable to reach out to a tutor in this way, so I struggled a lot. I think [the ChatGPT explanation] was good because then I can get another person's opinion first."

Despite the positive feedback on the GenAI- powered functionalities, some criticisms and concerns were also noted. Six students  $(33\%)$  shared negative feedback about the ChatGPT explanations, primarily because they found them repetitive and not sufficiently tailored to their specific needs. For instance, a student expressed dissatisfaction by stating, "I noticed two things. First, if I input a teacher's list of feedback into ChatGPT, it would kind of bring out the same thing. The second thing is for my maths units, when I put maths into it, it didn't seem to understand the mathematical concepts" (P07). It suggests the importance of context and discipline- specific support from GenAI, as generic GenAI tools may not be capable of handling subject- specific details, particularly in areas such as mathematics. For those students who  $(n = 6,33\%)$  neither positively nor negatively perceived the ChatGPT explanation functionality during their in- semester use, their neutrality stemmed from "not needing to use it" (P12) because the feedback they received from their teachers was already clear and easy to understand. As one student said, "I think most of my feedback was pretty straightforward and of good quality, so I didn't really need the [ask] ChatGPT function" (P02).

In summary, during the introductory lab sessions, students exhibited a high level of enthusiasm for the GenAI- powered functionalities of PolyFeed, with both the ChatGPT explanations and GenAI- powered visualizations receiving overwhelmingly positive feedback. However, this initial enthusiasm did not fully translate into sustained usage, as evidenced by the in- semester trace data, which showed that only just over half of the students continued to actively engage with these functionalities. This trend was reflected in the post hoc interviews, where a smaller number of students reported positive experiences. This was not necessarily due to dissatisfaction but rather from limited interaction with the functionalities, as some students found them unnecessary for high- quality feedback that was already easy to understand.

# 5. Discussion

This study explored how GenAI- powered functionalities within an LA tool can support students in the feedback processes through the lens of feedback literacy.

# 5.1 Summary of Findings

Responding to the overarching research question: "What may be the role of GenAI in the feedback process from a feedback literacy perspective?", findings uncovered that students initially perceived GenAI- powered functionalities as potentially beneficial in their feedback processes. During both introductory lab sessions (RQ1) and post hoc interviews (RQ2), students noted that GenAI- powered functionalities of PolyFeed could simplify complex feedback and provide actionable visualizations that offer an overview of their common strengths and weaknesses across units and assessments. These GenAI- powered functionalities, particularly GenAI- powered visualizations and ChatGPT explanations, not only clarified feedback to aid their sense- making processes but also may potentially support self- reflection and monitoring (Rad et al., 2023; Escalante et al., 2023).

In our study, some students reported that these functionalities, supported by GenAI, assisted in managing emotional responses to feedback by distilling lengthy and complex feedback from multiple educators across different assessments and units into keywords that highlight common strengths and weaknesses. This process can foster a positive outlook and a focused approach to improvement, potentially empowering students to effectively use feedback information to refine their learning strategies. Our study contributes to this research by suggesting the feasibility of supporting students to actively interact with feedback, given the immediacy and availability of AI (Escalante et al., 2023). GenAI can augment the feedback provided by teachers by adding clarifications or simplifying feedback written in a long block of text. The GenAI- powered function in this study, allowing students to ask for explanations from ChatGPT, further supports this process. The synergistic use of GenAI and effective feedback practices could support student engagement with feedback, which is crucial for enhancing feedback literacy, as it encourages students to take an active role and initiative in the feedback process (Carless & Boud, 2018). Additionally, improved clarity and specificity of the feedback explanations were valued by students in the study, which is consistent with findings by Dai and colleagues (2023, 2024) that feedback generated by GenAI often proves more readable and detailed than human- generated feedback. Not only can this clarity and specificity help students better understand and apply the feedback to their work (Escalante et al., 2023), but also these aspects are essential for developing their overall feedback literacy (Carless & Boud, 2018; Molloy et al., 2020).

There is substantial literature on the efficacy of visual LA in improving students' understanding of their learning processes and performance. Specifically, visualizations in educational settings have proven beneficial for students to better understand their learning processes and performance, thereby facilitating the development of metacognitive skills (Vieira et al., 2018). These visual LA tools allow students to reflect on their efforts and learning strategies, empowering them to make informed decisions about their learning paths (Vieira et al., 2018). In the context of our study, the integration of GenAI within an LA dashboard illustrates the potential of extending these benefits further, not merely focusing on performance metrics but enriching the feedback process by analyzing their interactions with feedback (i.e., highlighting and labelling feedback). The GenAI- powered visualizations in PolyFeed can provide students with a clear, intuitive understanding of their strengths and weaknesses across different assessments and units based on their educators' feedback. This capability extends beyond traditional performance metrics, offering insights into students' learning processes. By doing so, the support of GenAI in visualizing common strengths and weaknesses across assessments and units can help students make sense of complex feedback from multiple courses and assessments. This can facilitate self- reflection, monitoring, and planning targeted improvements (Crain et al., 2023), thus potentially supporting the development of their feedback literacy.

# 5.2 Discrepancy between Initial Perceptions and Actual Usage

5.2 Discrepancy between Initial Perceptions and Actual UsageThe results also revealed an interplay between students' initial perceptions (RQ1) and the actual usage (RQ2) of these GenAI- powered functionalities within the LA tool. While initial responses were overwhelmingly positive, sustained engagement proved to be more challenging, reflecting a reduction in actual usage of the functionalities compared to the initially perceived usefulness. The findings from post hoc interviews offered deeper insights into this decline in engagement with GenAI- powered functionalities observed during in- semester use. Three primary factors emerged that influenced the actual usage: (i) a mismatch between expectations and GenAI outputs, (ii) a lack of relevance to their learning contexts (given that the units students were taking were from multiple disciplines as well as students themselves being enrolled in different disciplinary programs), and (iii) a perception that the functionalities were unnecessary for straightforward feedback. These observations indicate that while the functionalities are appealing initially, they may not fully align with ongoing student requirements. Students often have high expectations for educational technologies, such as LA and GenAI, especially at first encounter, anticipating significant enhancements in learning experiences (Keane et al., 2023). However, the reality may fall short due to technical and practical limitations or the known inconsistencies in GenAI- generated content (Jürgensmeier & Skiera, 2024). This mismatch can undermine students' trust in and perceived usefulness of these tools, leading to lower engagement and effectiveness. This observation aligns with the findings of Li and colleagues (2021), who noted a contrast between users' initial exploration of an analytics tool and their experiences as they became more familiar with the tool, especially if it fails to provide relevant insights to meet users' needs. These insights suggest greater transparency and clearer communication regarding the operation and capabilities of GenAI tools are needed. Effectively communicating what students should expect from GenAI- powered functionalities and illustrating how they can benefit in the feedback process can help set realistic expectations. Such clarity demystifies the technology, ensuring that students understand both the potential and the limitations of GenAI, which in turn can foster more meaningful engagement with the tool. Moreover, developing discipline- specific GenAI tools that incorporate explainable AI principles—making the processes and decisions of AI understandable to users—can enhance trust and reliability (F. Jin et al., 2024; Khosravi et al., 2022). Finally, providing students with greater user control over GenAI interactions, such as the ability to adjust the level of detail or complexity in explanations, can potentially enhance their feedback literacy skills by enhancing the overall feedback experience. By enabling students to tailor the tool's output, we can encourage them to critically evaluate the feedback and make informed decisions about how to interpret and apply it in their learning. This aligns with the making judgments component of feedback literacy, as students learn to assess the relevance and applicability of the feedback

to their specific learning context and needs (Carless & Boud, 2018). Furthermore, this customization can support students in taking action based on feedback by allowing them to engage more actively with the feedback they receive. In this way, this approach can foster more consistent engagement with feedback across various learning contexts, potentially leading to improved feedback literacy skills over time.

# 5.3 Implications for Research and Practice

Exploring students' perceptions of GenAI- powered functionalities of the student- facing LA tool has several implications for the feedback process, particularly in supporting how feedback is understood, monitored, and reflected upon by students. By enabling direct interaction with feedback through GenAI- powered functionalities, such as ChatGPT explanations and visualizations of common strengths and weaknesses across assessments and units, GenAI shifts the role of students from passive recipients to active participants in feedback processes. This active engagement is crucial for the development of feedback literacy, as it encourages students to take initiative in interacting with feedback, proactively seeking clarifications to enhance their understanding, and reflecting on and monitoring their feedback interactions to adjust their learning strategies (Carless & Boud, 2018; Molloy et al., 2020). Additionally, GenAI can act as an intermediary that clarifies educators' feedback and intentions, reducing potential miscommunication and enhancing overall clarity. In this way, GenAI has the potential to support the closure of the feedback loop, ensuring that feedback is not just given and received but also understood and acted upon. Furthermore, integrating GenAI functionalities is anticipated to have several practical impacts on teaching environments. First, these functionalities can potentially enhance student engagement with feedback by providing immediate, on- demand explanations and clarifications. This is particularly valuable in large classes or resource- constrained environments where educators may not be able to respond promptly to individual requests for clarification. The ability to receive immediate clarifications through GenAI can encourage students to engage with their feedback more promptly and frequently, fostering sense- making and action- taking processes (T. Li et al., 2025). Second, integrating GenAI into an LA feedback management tool can enable students to engage with feedback more independently, empowering them to take a more active role in their feedback processes. This fosters self- regulated learning and helps students develop feedback literacy skills, such as seeking information to improve learning and processing feedback information. By promoting independence, GenAI functionalities support students in becoming more proactive and competent in managing their own feedback processes (Darvishi et al., 2024).

However, fully realizing GenAI's potential in LA feedback processes requires a more purposeful design that addresses issues that students currently face related to the relevance and depth of GenAI's outputs (Yan, Martinez- Maldonado, & Gasevic, 2024). Future research can explore the development of adaptive, personalized GenAI functionalities that respond to diverse student needs and involve learners in their iterative design processes. A shift toward discipline- specific GenAI can better meet the distinct requirements of different subject areas, providing more tailored support in feedback processes.

# 5.4 Limitations and Future Research

While this study provides valuable empirical insights into the potential of GenAI within the context of LA feedback processes, several limitations need to be acknowledged. First, the sample size was relatively small, with only 18 students participating in all activities of the study. This limited number of participants may not fully represent the diverse range of student experiences and attitudes toward GenAI across different educational contexts. Second, the students who decided to participate in this study might have had a more positive attitude toward feedback or a higher level of engagement with the feedback process than the general student population. This self- selection bias may influence the findings, potentially skewing the results toward more favourable perceptions of GenAI's role in the feedback process.

Another limitation concerns the version of ChatGPT used (i.e., version 3.5), which may not reflect the capabilities of more advanced or updated versions of GenAI tools. The output from ChatGPT might have been influenced by the technical limitations and inconsistencies inherent in this version, affecting students' perceptions and engagement. Additionally, the study did not measure long- term impacts on students' feedback literacy skills. Developing feedback literacy is a complex process that requires time and consistent practice (Carless & Boud, 2018). Our study, being exploratory in nature, focused on initial perceptions and short- term engagement with GenAI- powered functionalities. We recognize that observing significant improvements in feedback literacy skills would require a longer- term study. Finally, self- reported data provide valuable insights into students' perceptions and experiences but may not fully capture actual changes in feedback literacy skills or behaviours. Future research can develop novel trace data- based measures of feedback literacy, which could potentially capture students' actual behaviours and interactions with feedback, providing a more accurate representation of their feedback literacy skills. Additionally, future research can consider larger and more diverse student samples, explore more updated versions of GenAI, and investigate the long- term effects of GenAI on feedback literacy. Measuring academic performance could provide valuable insights into the impact of GenAI- powered feedback tools on learning outcomes and will be an important direction for future work.

# 6. Conclusion

6. ConclusionThis study highlighted the potential of GenAI within an LA tool to support feedback processes in higher education. It explored students' perceptions of GenAI-powered functionalities such as ChatGPT explanations and GenAI-powered visualizations, and how these functionalities can simplify and clarify complex feedback, thereby aiding in their sense-making processes and effective use of feedback. Despite initial positive reactions, sustained engagement with GenAI was lower than anticipated, indicating a discrepancy between students' initial dispositions and actual usage. Future research should focus on developing adaptive and discipline-specific GenAI tools tailored to meet students' expectations in the feedback process, thereby improving their engagement with feedback.

# Declaration of Conflict of Interest

The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.

# Funding

FundingThe publication of this article received financial support from Monash University educational innovation funding (The Pitch) and Advancing Women's Success Grant.

# References

ReferencesBoud, D., & Molloy, E. (2013). Rethinking models of feedback for learning: The challenge of design. Assessment & Evaluation in Higher Education, 38(6), 698- 712. https://doi.org/10.1080/02602938.2012.691462Carless, D., & Boud, D. (2018). The development of student feedback literacy: Enabling uptake of feedback. Assessment & Evaluation in Higher Education, 43(8), 1315- 1325. https://doi.org/10.1080/02602938.2018.1463354Conijn, R., Martinez- Maldonado, R., Knight, S., Buckingham Shum, S., Van Waes, L., & van Zaanen, M. (2022). How to provide automated feedback on the writing process? A participatory approach to design writing analytics tools. Computer Assisted Language Learning, 35(8), 1838- 1868. https://doi.org/10.1080/09588221.2020.1839503Crain, P., Lee, J., Yen, Y.- C., Kim, J., Aiello, A., & Bailey, B. (2023). Visualizing topics and opinions helps students interpret large collections of peer feedback for creative projects. ACM Transactions on Computer- Human Interaction, 30(3), 1- 30. https://doi.org/0.1145/3571817Dai, W., Lin, J., Jin, H., Li, T., Tsai, Y.- S., Gašević, D., & Chen, G. (2023). Can large language models provide feedback to students? A case study on ChatGPT. In M. Chang, N.- S. Chen, R. Kuo, G. Rudolph, D. G. Sampson, & A. Tlili (Eds.), Proceedings of the 2023 IEEE International Conference on Advanced Learning Technologies (ICALT 2023), 10- 13 July 2023, Oregon, Utah, USA (pp. 323- 325). IEEE. https://doi.org/10.1109/ICALT58122.2023.00100Dai, W., Tsai, Y.- S., Lin, J., Aldino, A., Jin, H., Li, T., Gašević, D., & Chen, G. (2024). Assessing the proficiency of large language models in automatic feedback generation: An evaluation study. Computers and Education: Artificial Intelligence, 7, 100299. https://doi.org/10.1016/j.caceai.2024.100299Darvishi, A., Khosravi, H., Sadiq, S., Gašević, D., & Siemens, G. (2024). Impact of AI assistance on student agency. Computers & Education, 210, 104967. https://doi.org/10.1016/j.compedu.2023.104967Escalante, J., Pack, A., & Barrett, A. (2023). AI- generated feedback on writing: Insights into efficacy and ENL student preference. International Journal of Educational Technology in Higher Education, 20(1), 57. https://doi.org/10.1186/s41239- 023- 00425- 2Filius, R. M., de Kleijn, R. A., Uijl, S. G., Prins, F. J., van Rijen, H. V., & Grobbee, D. E. (2018). Strengthening dialogic peer feedback aiming for deep learning in SPOCs. Computers & Education, 125, 86- 100. https://doi.org/10.1016/j.compedu.2018.06.004Hattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational Research, 77(1), 81- 112. https://doi.org/10.3102/003465430298487Henderson, M., Ajjawi, R., Boud, D., & Molloy, E. (2019). Identifying feedback that has impact. In M. Henderson, R. Ajjawi, D. Boud, & E. Molloy (Eds.), The impact of feedback in higher education (pp. 15- 34). Springer International Publishing. https://doi.org/10.1007/978- 3- 030- 25112- 3_2Jin, F., Maheshi, B., Martinez- Maldonado, R., Gašević, D., & Tsai, Y.- S. (2024). Scaffolding feedback literacy: Designing a feedback analytics tool with students. Journal of Learning Analytics, 11(2), 123- 137. https://doi.org/10.18608/jla.2024.8339Jin, H., Martinez- Maldonado, R., Li, T., Chan, P. W. K., & Tsai, Y.- S. (2022). Towards supporting dialogic feedback processes using learning analytics: The educators' views on effective feedback. In S. Wilson, N. Arthars, D. Wardak, P. Yeoman, E. Kalman, & D. Y. Liu (Eds.), ASCILITE 2022 Conference Proceedings: Reconnecting Relationships through Technology, 4- 7 December 2022, Sydney, Australia (e22054). ASCILITE Publications. https://doi.org/10.14742/apubs.2022.54

Jürgensmeier, L., & Skiera, B. (2024). Generative AI for scalable feedback to multimodal exercises. International Journal of Research in Marketing, 41(3), 468- 488. https://doi.org/10.1016/j.ijresmar.2024.05.005

Keane, T., Linden, T., Hernandez- Martinez, P., Molnar, A., & Blicblau, A. (2023). Digital technologies: Students' expectations and experiences during their transition from high school to university. Education and Information Technologies, 28(1), 857- 877. https://doi.org/10.1007/s10639- 022- 11184- 4

Khosravi, H., Buckingham Shum, S., Chen, G., Conati, C., Tsai, Y.- S., Kay, J., Knight, S., Martinez- Maldonado, R., Sadiq, S., & Gašević, D. (2022). Explainable artificial intelligence in education. Computers and Education: Artificial Intelligence, 3, 100074. https://doi.org/10.1016/j.caeai.2022.100074

Kim, H.- S., Cha, Y., & Kim, N. Y. (2021). Effects of AI chatbots on EFL students' communication skills. The Korean Association for the Study of English Language and Linguistics, 21, 712- 734. https://doi.org/10.15738/kjell.21. .202108.712

Knight, S., Shihani, A., Abel, S., Gibson, A., Ryan, P., Sutton, N., Wight, R., Lucas, C., Sándor, Á., Kitto, K., Liu, M., Mogarkar, R. V., & Buckingham Shum, S. (2020). AcaWriter: A learning analytics tool for formative feedback on academic writing. Journal of Writing Research, 12(1), 141- 186. https://doi.org/10.17239/JOWR- 2020.12.01.06

Lazar, J., Feng, J. H., & Hochheiser, H. (2017). Analyzing qualitative data. In Research methods in human computer interaction (pp. 299- 327). Elsevier. https://doi.org/10.1016/B978- 0- 12- 805390- 4.00011- X

Li, Q., Jung, Y., & Wise, A. F. (2021). Beyond first encounters with analytics: Questions, techniques and challenges in instructors' sensemaking. In Proceedings of the 11th International Conference on Learning Analytics and Knowledge (LAK 2021), 12- 16 April 2021, Irvine, California, USA (pp. 344- 353). ACM. https://doi.org/10.1145/3448139.3448172

Li, T., Nath, D., Cheng, Y., Fan, Y., Li, X., Raković, M., Khosravi, H., Swiecki, Z., Tsai, Y.- S., & Gašević, D. (2025). Turning real- time analytics into adaptive scaffolds for self- regulated learning using generative artificial intelligence. In Proceedings of the 15th International Conference on Learning Analytics and Knowledge (LAK 2025), 3- 7 March 2025, Dublin, Ireland (pp. 667- 679). ACM. https://doi.org/10.1145/3706468.3706559

Lim, L.- A., Dawson, S., Gašević, D., Joksimović, S., Pardo, A., Fudge, A., & Gentili, S. (2021). Students' perceptions of, and emotional responses to, personalised learning analytics- based feedback: An exploratory study of four courses. Assessment and Evaluation in Higher Education, 46(3), 339- 359. https://doi.org/10.1080/02602938.2020.1782831

Molloy, E., Boud, D., & Henderson, M. (2020). Developing a learning- centred framework for feedback literacy. Assessment & Evaluation in Higher Education, 45(4), 527- 540. https://doi.org/10.1080/02602938.2019.1667955

Pardo, A. (2018). A feedback model for data- rich learning experiences. Assessment & Evaluation in Higher Education, 43(3), 428- 438. https://doi.org/10.1080/02602938.2017.1356905

Pardo, A., Jovanovic, J., Dawson, S., Gašević, D., & Mirriahi, N. (2019). Using learning analytics to scale the provision of personalised feedback. British Journal of Educational Technology, 50(1), 128- 138. https://doi.org/10.1111/BJET.12592

Rad, H. S., Alipour, R., & Jafarpour, A. (2023). Using artificial intelligence to foster students' writing feedback literacy, engagement, and outcome: A case of Wordtune application. Interactive Learning Environments, 32(9), 5020- 5040. https://doi.org/10.1080/10494820.2023.2208170

Sedrakyan, G., Malmberg, J., Verbert, K., Järvelä, S., & Kirschner, P. A. (2020). Linking learning behavior analytics and learning science concepts: Designing a learning analytics dashboard for feedback to support learning regulation. Computers in Human Behavior, 107, 105512. https://doi.org/10.1016/j.chb.2018.05.004

Sutton, P. (2009). Towards dialogic feedback. Critical and Reflective Practice in Education, 1(1), 1- 10. https://marjon.repository.guildhe.ac.uk/id/cprint/17582/1/Towards%20dialogic%20feedback.pdf

Tsai, Y.- S. (2022). Why feedback literacy matters for learning analytics. In C. Chinn, E. Tan, C. Chan, & Y. Kali (Eds.), Proceedings of the 16th International Conference of the Learning Sciences (ICLS 2022), 6- 10 June 2022, Hiroshima, Japan (pp. 27- 34). International Society of the Learning Sciences. https://doi.org/10.48550/arXiv.2209.00879

Tubino, L., & Adachi, C. (2022). Developing feedback literacy capabilities through an AI automated feedback tool. In S. Wilson, N. Artmars, D. Wardak, P. Yeoman, E. Kalman, & D. Y. Liu (Eds.), ASCILITE 2022 Conference Proceedings: Reconnecting Relationships through Technology, 4- 7 December 2022, Sydney, Australia (e22039- 1- e22039- 5). https://doi.org/10.14742/apubs.2022.39

Vieira, C., Parsons, P., & Byrd, V. (2018). Visual learning analytics of educational data: A systematic literature review and research agenda. Computers & Education, 122, 119- 135. https://doi.org/10.1016/j.compedu.2018.03.018

Whitelock, D., Twiner, A., Richardson, J. T., Field, D., & Pulman, S. (2015). OpenEssayist: A supply and demand learning analytics tool for drafting academic essays. In Proceedings of the Fifth International Conference on Learning Analytics and Knowledge (LAK 2015), 16- 20 March 2015, Poughkeepsie, New York, USA (pp. 208- 212). ACM. https://doi.org/10.1145/2723576.2723599

Winstone, N. (2019). Facilitating students' use of feedback: Capturing and tracking impact using digital tools. In M. Henderson, R. Ajjawi, D. Boud, & E. Molloy (Eds.), *The impact of feedback in higher education* (pp. 225- 242). Springer International Publishing. https://doi.org/10.1007/978- 3- 030- 25112- 3_13Yan, L., Greiff, S., Teuber, Z., & Gašević, D. (2024). Promises and challenges of generative artificial intelligence for human learning. *Nature Human Behaviour*, 8(10), 1839- 1850. Yan, L., Martinez- Maldonado, R., Echeverria, V., Jin, Y., Alfredo, R., Milesi, M., Zhao, L., Fan, J., & Gašević, D. (2024). The effects of generative AI agents and scaffolding on enhancing students' comprehension of visual learning analytics. *arXiv preprint arXiv:2409.11645*. https://doi.org/10.48550/arXiv.2409.11645Yan, L., Martinez- Maldonado, R., & Gasevic, D. (2024). Generative artificial intelligence in learning analytics: Contextualising opportunities and challenges through the learning analytics cycle. In *Proceedings of the 14th International Conference on Learning Analytics and Knowledge* (I.AK 2024), 18- 22 March 2024, Tokyo, Japan (pp. 101- 111). ACM. https://doi.org/10.1145/3636555.3636856Yan, L., Sha, L., Zhao, L., Li, Y., Martinez- Maldonado, R., Chen, G., Li, X., Jin, Y., & Gašević, D. (2024). Practical and ethical challenges of large language models in education: A systematic scoping review. *British Journal of Educational Technology*, 55(1), 90- 112. https://doi.org/10.1111/bjet.13370Yan, L., Zhao, L., Echeverria, V., Jin, Y., Alfredo, R., Li, X., Gašević, D., & Martinez- Maldonado, R. (2024). izChat: Enhancing learning analytics dashboards with contextualised explanations using multimodal generative AI chatbots. In A. Olney, I. Chounta, Z. Liu, O. Santos, & I. Bittencourt (Eds.), *Artificial intelligence in education. AIED 2024. Lecture notes in computer science* (pp. 180- 193, Vol. 14830). Springer. https://doi.org/10.1007/978- 3- 031- 64299- 9_13Yang, M., & Carless, D. (2013). The feedback triangle and the enhancement of dialogic feedback processes. *Teaching in Higher Education*, 18(3), 285- 297. https://doi.org/10.1080/13562517.2012.719154Zheng, L., Niu, J., & Zhong, L. (2022). Effects of a learning analytics- based real- time feedback approach on knowledge elaboration, knowledge convergence, interactive relationships and group performance in CSCL. *British Journal of Educational Technology*, 53(1), 130- 149. https://doi.org/10.1111/bjet.13156