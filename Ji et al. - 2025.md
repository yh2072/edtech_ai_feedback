# The construction and implementation direction of personalized learning model based on multimodal data fusion in the context of intelligent education

Xingle Ji  $\oplus$ , Lu Sun, Kun Huang

Huzhou Vocational and Technical College, Huzhou Key Laboratory of Green Buildings Technology, Huzhou, Zhejiang, China

# ARTICLEINFO

Keywords: Data mining Intelligent education Multimodal data fusion Personalized learning Learning analytics

# ABSTRACT

The rapid development of artificial intelligence (AI) technologies, represented by computer vision, natural language processing, and speech recognition, has brought new opportunities for the advancement of personalized learning within intelligent education. This article utilizes intelligent collection devices such as cameras, electroencephalographs (EEG), eye trackers, smart bracelets, and data gloves to comprehensively collect and analyze data on learners' voices, videos, texts, breathing, heartbeats, EEG signals, and eye movements. A multimodal dataset for learners is constructed across four dimensions: behavioral representation, physiological information, human- computer interaction, and learning context. By employing natural language processing, speech recognition, computer vision, and physiological information recognition technologies, we extract and analyze the multimodal datasets. This process mines the hidden personalized information of learners, enabling data- driven, real- time, quantified evaluation of their learning states. This study constructs a personalized learning model based on multimodal data fusion within the field of intelligent education by examining the current research landscape, data types, and relevant fusion strategies of this technology. It aims to provide personalized services tailored to the needs of each learner.

# 1. Introduction

Emerging intelligent technologies such as big data, cloud computing, the Internet of Things (IoT), and artificial intelligence (AI) are rapidly developing. These advancements are increasingly driving digitization, precision, and personalization in education and teaching. These advancements provide new momentum for the innovation of teaching concepts and the transformation of talent cultivation models (Zheng et al., 2024). In this context, multimodal data fusion has gradually garnered attention from researchers and has been applied as a novel technological approach in various intelligent education scenarios. By collecting and analyzing multimodal data from learners and learning contexts, researchers leverage the complementary information among different data modalities to enhance the accuracy of data analysis, reconstruct the overall teaching process, and offer in- depth analytical insights that are challenging to obtain through traditional teaching methods. These efforts ultimately promote personalized learning research in the intelligent era (Dong et al., 2024). So, why use multimodal data? What kind of data can be called multimodal data? How to integrate multimodal data? How to use multimodal data fusion to build personalized learning systems? What impact can it bring to the development of personalized learning research in intelligent education? This article attempts to answer the above questions by reviewing the research status, system construction, and practical approaches of multimodal data fusion technology and personalized learning support services, in order to provide reference for the development of related research in the future.

# 2. Research status

# 2.1. Research status of multimodal data fusion technology

Multimodal fusion refers to the transformation and integration of information from various sources, such as video, audio, images, and text, to enhance model performance and achieve complementarity among heterogeneous data. The primary goal of multimodal fusion is to reduce disparities between different types of information while preserving the integrity of semantic details within each modality. Tamura et al. (2019)

proposed a multimodal synchronous measurement system that combines eye tracking and electroencephalography (EEG). The eye tracker captures information about the areas learners focus on, while EEG signals provide insights into users' cognitive and learning states. Wang and He (2020) utilized online learning systems to collect multimodal behavioral data across three dimensions—psychological, physiological, and behavioral—offering a more comprehensive evaluation of learners' overall status. Research indicates a coupling relationship between different modalities, where effectively integrating data from various sources can maximize the potential of each modality, thereby accurately assessing learners' true states. Nonetheless, the diversity, complexity, and unstructured nature of multimodal data present significant challenges, making the effective fusion of different modalities a pressing issue in intelligent education.

The research on multimodal data fusion has a long history. Ngiam et al. (2011) pioneered the application of multimodal data such as text, video, and audio to the field of multimodal interaction research, establishing the foundational framework for multimodal data fusion based on deep learning. The autoencoder method and cross- modal learning concepts they proposed have provided significant theoretical foundations and technical pathways for subsequent multimodal research. These methods have been widely applied in various fields, including image- text matching and speech- visual synchronization. The International Conference on Multimodal Interaction (ICMI) in 2012 addressed diverse aspects of multimodal human- computer interaction, including algorithms, system design, and application cases. It further integrated multimodal fusion technology with intelligent education research, enhancing understanding of educational and teaching mechanisms and cementing the leading position of multimodal data fusion in this domain (Morency et al., 2012). Huang et al. (2016) explored the influence of emotion recognition on students' learning performance through multimodal data fusion. Di et al. (2018) investigated the application of multimodal data in learning analytics, focusing on improving learning effectiveness by integrating diverse data sources. At the 2019 Annual Meeting of the American Educational Research Association (AERA), researchers proposed using multimodal data, including text, charts, and audio/video, to construct a research model based on multimodal narratives to advance intelligent education and teaching (Wu et al., 2019). Xu and Zhou (2020) introduced a personalized learning path recommendation system leveraging multimodal data fusion. Their system combined learners' behavioral data with learning content to deliver personalized learning services. Luo et al. (2022) designed a multimodal learning environment that integrates various data types to enhance learners' outcomes. Tang et al. (2023) examined the integration of multimodal emotional data across three dimensions—motivation, framework, and direction—to accurately perceive learners' emotional states. They also explored the potential value of multimodal emotional data to address key challenges in the evolution of intelligent education.

Defining the research scope of multimodal data fusion in the era of artificial intelligence necessitates addressing practical needs, bridging the gap between multimodal data fusion theory and AI technology. By leveraging intelligent sensing devices and advanced analytical technologies, researchers can gain a holistic understanding of educational and teaching processes, driving the digital, scientific, and intelligent transformation of education research.

# 2.2. Current status of personalized learning research

Personalized learning involves providing learners with real- time behavioral alerts, tailored learning methods, customized learning resources, and individualized learning paths based on their unique characteristics, such as interests, preferences, learning needs, learning styles, initial abilities, and cognitive levels. The goal is to enable learners to achieve truly personalized learning experiences. Currently, research on personalized learning has garnered widespread attention. Huang et al.

(2017) proposed a scenario-aware learner modeling method to develop personalized learner models incorporating contextual features. Yin et al. (2023) utilized artificial intelligence technologies to design a smart learning model that elucidates the characteristics, components, and operational mechanisms of intelligent learning. This model comprises six core elements—learning environment, educational knowledge graph, learner profile, learning path, learning evaluation, and learning community—as well as five hierarchical layers: foundation, support, service, key, and application layers. Shi et al. (2019) developed a precise personalized learning path generation and recommendation model based on learner profiles. This model utilizes personalized recommendation strategies to propose a list of learning elements tailored to a learner's current learning status. Learners can then actively select the most suitable elements to optimize their learning experience. Although personalized learning research has achieved certain quality academic results, especially in the construction of learning systems through learning theory and technology, the existing research results on how to build personalized learning systems and how to apply them to classroom practice in specific disciplines need to be further enriched. Personalized learning involves the use of data modeling to intelligently diagnose and accurately discern individual learner characteristics, and to implement precise interventions through the design of personalized learning plans.

To further understand the objectives and applications of personalized learning, a review of the literature on personalized learning reveals that the objectives of personalized learning can be summarized as "one center, three objectives" (as shown in Fig. 1). The "one center" emphasizes a learner- centered approach, while the "three objectives" are as follows:

(1) Precision academic performance diagnosis and resource recommendation:

This objective focuses on enhancing learners' academic levels. By collecting and analyzing data from assessments and learning behaviors, the learner's knowledge mastery and cognitive development are evaluated. A precise matching mechanism is developed to align learners with appropriate learning resources, considering factors such as prior knowledge, cognitive levels, learning styles, and preferences. This facilitates personalized resource recommendations and learning path planning services (Li et al., 2023).

(2) Precision-based instructional activity design to cultivate comprehensive qualities:

Learners are grouped based on characteristics such as grade level, interests, and abilities, enabling the design of inquiry- based instructional activities. These activities encourage learners to improve teamwork and problem- solving skills through collaborative learning and to apply their knowledge and skills to real- world challenges. This approach deepens their understanding of the material, fosters a spirit of exploration and innovation, and enhances their overall competencies.

(3) Comprehensive cultivation path design for career development:

Personalized cultivation models are created to support learners' career trajectories, taking into account their personality, abilities, interests, preferences, physical fitness, and family backgrounds. Precision assessments and analyses are conducted at different stages to identify and nurture learners' potential traits, ensuring tailored support for their career development (Shu et al., 2023).

# 3. Multimodal data types and fusion strategies

# 3.1. Types of multimodal data

Personalized learning, rooted in Confucius's philosophy of "teaching students according to their aptitude," emphasizes the fundamental principle of respecting the unique differences among learners. It involves tailoring learning strategies and methods to align with individual characteristics such as knowledge structure, skill level, life experiences, emotional attitudes, interests, hobbies, and motivational beliefs (Childress and Benson, 2014). The primary objective is to address the

![](images/277b6227a61dbcf356592edd31663b362eb12bcfd30558f11b48a93ff02768b6.jpg)  
Fig. 1. Objectives of personalized learning support services.

personalized learning needs of students, unlock their full learning potential, and assist them in mastering effective learning methods and strategies. Ultimately, the goal is to inspire and facilitate meaningful and effective learning experiences. The collection and analysis of learner data form the foundation of multimodal data fusion, which plays a critical role in uncovering learning behaviors and patterns within complex environments. This article examines the research status of multimodal data fusion in the field of intelligent education, categorizes data sources, and classifies multimodal data into four types, as summarized in Table 1 (Wang and Zheng, 2022).

1) Behavioral representation data

External behavioral representation data primarily encompasses learners' language, facial expressions, gestures, body posture, inertial movements, and other related information. Two common types of external behavioral representation data can be identified: 1) Multimodal data based on text, speech, and video: In the field of artificial intelligence, multimodal data refers to a data representation pattern that integrates text, speech, and video. A multimodal analysis model constructed using techniques such as natural language processing, speech recognition, and computer vision can provide comprehensive interpretations of the research object. For instance, Si and Fu (2024) developed an online learning emotion computing model that integrates multimodal data. By fusing video, speech, and text data, their emotion recognition models generate emotion recognition results for each modality. 2) Multimodal data based on facial expressions and body posture:

Table 1 Statistics of multimodal data types and sources.  

<table><tr><td>Data type</td><td>Data source</td><td>Collection devices and systems</td></tr><tr><td>Behavioral 
Representation 
Data</td><td>Language, facial expressions, gestures, body posture, habitual movements, etc.</td><td>Cameras, sound sensors, Natural Language Processing (NLP) tools, Computer Vision (CV) tools, etc.</td></tr><tr><td>Physiological 
Information Data</td><td>Breathing, heart rate, pulse, eye movement, skin conductance, EEG, blood oxygen, hormone secretion levels, etc.</td><td>Breathing sensors, ECG monitors, EEG detectors, eye trackers, skin conductance sensors, pulse oximeters, hormone level detectors, etc.</td></tr><tr><td>Human-Computer Interaction Data</td><td>Clicks, fingerprints, touch pressure, handwriting, gestures, text input, voice interaction, facial expressions, etc.</td><td>Touch screens, pressure-sensitive pens, fingerprint recognition modules, voice recognition devices, visual interaction sensors, online learning platform data, etc.</td></tr><tr><td>Learning Context 
Data</td><td>Teaching style, teaching resources, teaching activities, teaching environment, as well as learning content, behavior trajectories, learning performance, etc.</td><td>Intelligent teaching terminals (smart blackboards, trajectory tracking), learning trajectory tracking systems, environmental sensors (temperature and humidity sensors), teaching behavior capture systems (motion capture cameras), etc.</td></tr></table>

This approach, often emphasized in computer vision, utilizes multimodal data from facial expressions and body posture to analyze and uncover potential states of the research object. For example, Zhao (2023) proposed a bimodal emotion recognition model that integrates facial expressions and body posture, enabling emotional recognition of learners in the classroom. This model successfully identifies student emotions based on body posture analysis.

(2) Physiological information data

The collection of intrinsic neurophysiological information data relies on multimodal biometric recognition technology. By employing various intelligent sensing devices, data such as respiration, heartbeat, pulse, eye movement, skin conductance, electroencephalography (EEG), blood oxygen levels, and hormone secretion can be collected and integrated for analysis. These data facilitate the identification of learners' emotional states, learning engagement, and other characteristics. For instance, Castaneda et al. (2018) utilized photoplethysmography (PPG) for heart rate monitoring. PPG is a non- invasive technique that measures changes in blood volume through optical signals and is widely applied to monitor physiological parameters such as heart rate, blood oxygen saturation, and respiratory rate. Compared with evaluation methods based on external behavioral representation data, multimodal data perception grounded in neurophysiological information eliminates the influence of the subjective consciousness of research subjects on external behaviors, thereby offering a more objective reflection of their true states. Consequently, this type of data has increasingly found applications in fields such as education, psychology, and medicine.

(3) Human computer interaction data

Human- computer interaction data refers to various data generated during interactions between humans and computers or digital devices. These data can reflect users' behaviors, intentions, and emotions. Multimodal data based on human- computer interaction primarily focuses on data generated by learners during the use of human- computer interaction devices, including clicks, fingerprints, touch, pressure, handwriting, gestures, text input, voice interaction, and facial expressions. For instance, Oviatt et al. (2019) explored methods to enhance the user experience of human- computer interaction systems through emotion detection based on multimodal emotional data, such as facial expressions, speech, and body posture. Similarly, Pantic et al. (2003) developed a multimodal human- computer interaction system capable of recognizing users' emotional states by integrating multiple data sources, including visual, speech, behavioral, and physiological signals. By leveraging emotion recognition technologies, the system not only interprets user behaviors but also perceives emotional states, enabling the provision of more personalized feedback. With the advancement of intelligent technologies and the increasing maturity of intelligent education products, human- computer collaboration is expected to become a vital component of future education. Consequently, effectively utilizing multimodal human- computer interaction data to reconstruct the learning process and deliver personalized teaching guidance represents a critical direction for the future development of intelligent education.

(4) Learning context data

The aforementioned multimodal data primarily describe a single feature of an individual within a specific context. However, due to the complexity of educational settings, the issues studied by scholars often require not only the measurement of individual learner characteristics but also the integration of multi- source heterogeneous data to achieve an accurate depiction of real educational contexts. Learning context data primarily includes information related to teachers' teaching behaviors, teaching content, instructional design, teaching equipment, and teaching environment, as well as data on students' behaviors, cognition, and emotional characteristics. This data is ultimately reflected in aspects such as learning content, behavior trajectories, and learning performance. As a result, the data- driven representation of learning context information is a key focus for future research in the field of intelligent education. By intelligently perceiving and accurately evaluating the elements that constitute the educational context—such as individuals, machines, objects, and the environment—a comprehensive evaluation and analysis of the learning context can be conducted at the data perception level. This facilitates in- depth exploration and analysis of personalized learning features (Wang and Zheng, 2021).

# 3.2. Multimodal data fusion strategy

The fusion of multimodal data is designed to leverage machine learning and deep learning methodologies to integrate and analyze data from various modalities. This approach capitalizes on the complementary nature of information across different modalities, thereby enhancing the efficacy of data analysis. The advantages of multimodal fusion stem from the redundancy and complementarity of information across modalities, which enhance data analysis compared to using single- modality features. Consequently, optimal processing of multimodal information can yield a richer set of feature data. There are four prevalent strategies for multimodal data fusion: feature- level fusion, data- level fusion, decision- level fusion, and modality- level fusion. Each strategy has its unique strengths and limitations, as well as specific scenarios where it is most effective. These are detailed and compared in Table 2, which provides an analysis of their advantages, disadvantages, and suitable application contexts.

# (1) Feature-level fusion

Feature- level fusion entails the combination and weighted integration of data features from various modalities, operating on channel or spatial dimensions. This approach is frequently employed in multi- scale feature extraction and cross- modal fusion scenarios. At the feature level, a consolidated feature vector is derived from the integration of multiple modalities, which is then fed into a single classifier for signal classification. For instance, Verma and Tiwary (2014) achieved emotion classification and recognition by concatenating and fusing feature vectors from diverse signals, including respiratory rate, skin temperature, electromyography, and electrooculography, with an accuracy of up to  $81.45\%$ . Similarly, Kim and Andre (2008) examined linear and nonlinear features, as well as multi- scale entropy, across four physiological data types: electromyography, electrocardiography, skin conductance, and respiration. They proposed a method involving multilevel feature concatenation and joint feature selection based on time- - frequency analysis of multimodal data. While feature- level fusion excels at extracting multi- scale and multimodal feature information thereby enhancing the model's expressive power, it is essentially a form of loose coupling between modalities. This method necessitates temporal synchronization of multimodal features, which can be challenging. Additionally, the dimensionality of the fused features tends to be high, potentially leading to overfitting and increased computational complexity during model training. Furthermore, since the quality and significance of features can differ, straightforward concatenation or weighting might result in redundant or lost information.

Table 2 Advantages, disadvantages, and applicable scenarios of multimodal data fusion strategies.  

<table><tr><td>Strategy</td><td>Advantages</td><td>Disadvantages</td><td>Applicable scenarios</td></tr><tr><td>Feature-Level Fusion</td><td>Complete information, suitable for strongly related modalities, overall optimization</td><td>High dimensionality, sensitive to noise, requires data alignment</td><td>High correlation between features and sufficient data volume</td></tr><tr><td>Data-Level Fusion</td><td>Balanced representation, strong flexibility</td><td>Complex models, heavily dependent on the quality of feature extraction</td><td>Data is not fully aligned and interpretable fusion is required</td></tr><tr><td>Decision-Level Fusion</td><td>Modular, robust, suitable for asynchronous data</td><td>Information loss, dependent on the performance of single modality</td><td>The decision-making independence of each mode is strong and difficult to align</td></tr><tr><td>Modal-Level Fusion</td><td>Comprehensive, predictive, capable of handling complex scenarios and multi-source heterogeneous data</td><td>Computationally complex, high requirements for model design</td><td>Complex scenarios in multimodal data analysis</td></tr></table>

# (2) Data-level fusion

With the continuous development of deep learning technology, its powerful representation learning capabilities can be leveraged to combine multimodal data representations driven by specific task objective functions. This approach is referred to as data- level fusion. Neural network models, in particular, enable the flexible adjustment of the degree of fusion between different modalities, resulting in a variety of fusion methods. For example, Ranganathan et al. (2016) utilized a Convolutional Deep Belief Network (CDBN) to fuse physiological data, including facial expressions, skeletal tracking, and ECG signals, achieving enhanced multimodal feature representation and improved recognition results. Similarly, Zheng et al. (2018) employed the Bimodal Deep Auto- Encoder (BDAE) method to effectively recognize three emotional states, achieving a recognition accuracy of  $85.11\%$ . The use of deep learning networks allows for weighted fusion of representations at different levels, providing flexibility and enabling multi- branch feature extraction and fusion, making this approach well- suited for complex tasks and models. However, the fusion process often requires task- and model- specific design and adjustment, which can be labor- intensive. Additionally, weighted fusion methods may introduce challenges related to hyperparameter tuning. Furthermore, while deep learning offers strong capabilities, it is also more susceptible to overfitting.

# (3) Decision-level fusion

In decision- level fusion, models are constructed based on the data features of different modalities, and the recognition results of each model are then integrated to obtain the final recognition outcome. Commonly used decision integration strategies include voting, maximum value, average value, Bayesian rule, evidence theory, and others. For example, Chanel et al. (2009) classified positive, negative, and neutral emotions using physiological information from the central and peripheral nervous systems (including GSR signals, blood pressure, heartbeat, and RSP). By fusing three classifiers at the decision level, the accuracy was improved to  $70\%$ . Similarly, Ringeval et al. (2015) utilized the RECOLA public database to analyze data such as sound, video, and physiological information. Their findings demonstrate that multimodal emotion recognition algorithms perform better overall, with decision- level fusion algorithms outperforming feature- level fusion algorithms. Although decision- level fusion methods effectively leverage the complementarity of multimodal information and offer advantages such as strong interpretability, robustness to interference, real- time performance, and fault tolerance. This also helps reduce the risk of overfitting. However, these methods lack interaction between multimodal data at the lower levels, as they assume that each model operates

independently, which is often inconsistent with real- world scenarios. (4) Modal- level fusion

(4) Modal-level fusionModal-level fusion is a comprehensive multimodal data fusion strategy designed to combine the strengths of feature-level, data-level, and decision-level fusion. It aims to capture both low-level feature relationships and high-level semantic interactions between multimodal data. By processing and integrating data at multiple levels, it enables the effective identification of complex modal interactions. For example, Oviatt et al. (2018) developed deep learning models based on attention mechanisms to capture temporal relationships in multimodal data for evaluating learners' psychological states. They combined feature-level and decision-level fusion to enhance analysis accuracy. Similarly, Tsai et al. (2018) proposed a joint attention mechanism that disentangles the correlation and independence of multimodal features, achieving deep modality-level fusion and improving the accuracy of sentiment analysis. Modal-level fusion methods typically leverage advanced deep learning techniques, including Transformers, attention mechanisms, and graph neural networks, to address the high complexity of multimodal data. These methods have proven effective in solving problems across various domains, such as personalized learning, emotion analysis, and medical diagnosis.

# 4. Construction of personalized learning system based on multimodal fusion technology

# 4.1. Characteristics of personalized learning systems

Personalized learning is crucial for nurturing innovative talents and represents a key objective in the initial transformation of education. It contributes to improving talent cultivation models, enhancing students' learning efficiency, increasing the effectiveness of education and teaching, and creating tailored learning plans for individual learners. In the new era of educational reform, the rapid development of intelligent technologies and the increasing maturity of intelligent education products have facilitated the deeper integration of these technologies with education and teaching. This integration provides strong support for realizing personalized learning. Through multimodal data fusion technology, personalized learning systems can intelligently analyze students' academic development, offering precise learning resource recommendations and planning individualized learning paths. Such systems enable students to access the necessary resources and services promptly, avoiding the inefficiencies and resource waste associated with navigating large volumes of learning materials or struggling to address academic challenges in a timely manner.

The personalized learning system constructed in this article has seven key features: 1) Integrating students' interests and abilities into real- world activities to enhance learning motivation. 2) Teachers act as facilitators and coaches, rather than merely disseminators of knowledge. 3) Learners have control over their learning paths and are responsible for setting their own learning goals. 4) The system supports learners in selecting their learning content, paths, and styles, catering to individual preferences. 5) Personalized assessment is embedded throughout the learning process, helping both teachers and learners identify strengths and areas for improvement. 6) Learners' progress in knowledge, skills, and literacy can be accurately measured. 7) Technology is integrated through the experiences of both teachers and learners, providing enhanced support for the learning process.

Therefore, personalized learning systems have emerged as a solution to address the challenges inherent in the current education system. These systems differ significantly from traditional educational models in terms of instructional methods, learning pace, the selection of time and location, and the roles of teachers and students. The specific comparison results are presented in Table 3. It is evident that personalized learning has been seamlessly integrated and restructured throughout the entire educational process, achieving significant breakthroughs and innovations in the dimensions of time and space.

Table 3 Differences between current education system and personalized learning system.  

<table><tr><td>Dimension</td><td>Current education system</td><td>Personalized learning system</td></tr><tr><td>System Output</td><td>Mass production</td><td>Mass personalized customization</td></tr><tr><td>Autonomy</td><td>Continuous time investment, fixed-time learning</td><td>Diversified timing, learning pace based on ability and mastery</td></tr><tr><td>Teaching and Learning Model</td><td>Unified learning and teaching model based on a common pace</td><td>Autonomous learning model based on different paces</td></tr><tr><td>Course Learning Time and Evaluation</td><td>Completing courses by semester, evaluating knowledge</td><td>Continuously tracking courses, dynamically evaluating students&#x27; knowledge, skills, learning styles, and interests</td></tr><tr><td>Role Positioning Location selection</td><td>Teacher-centered Fixed location, learning at school</td><td>Student-centered Any location, mobile learning</td></tr><tr><td>Learning time</td><td>Fixed learning time</td><td>Flexible learning time and schedule</td></tr><tr><td>Teaching place Learning method Learning space</td><td>Fixed, unified place Face-to-face learning Formal learning</td><td>Multiple types of places Blended learning Formal and informal learning</td></tr></table>

# 4.2. Technical support for the construction of personalized learning systems

Currently, the development of artificial intelligence has progressed from the stage of intelligent computing, which relies on neural networks and genetic algorithms, to the stage of intelligent perception, which is driven by signal processing techniques such as text, speech, and image analysis. Artificial intelligence technologies, including computer vision, speech recognition, natural language processing, physiological information recognition, and platform data collection, provide a diverse range of technical support for the construction of personalized learning systems (as shown in Table 4).

Based on computer vision technology, image information such as students' facial expressions and movements is collected to analyze their classroom performance and identify their focus, emotional states, and

Table 4 Technical support for modeling personalized learning systems.  

<table><tr><td>Technical field</td><td>Scope of support</td><td>Content of support</td></tr><tr><td>Computer Vision Technology</td><td>Image Information Collection and Analysis</td><td>Intelligently collect and analyze image information such as students&#x27; expressions and movements, assessing classroom performance, focus, and emotional state. Capture and analyze tone, pitch, and speech content from students&#x27; classroom speeches to assess cognitive development and emotional state.</td></tr><tr><td>Speech Recognition Technology</td><td>Speech Information Processing</td><td>Language Learning, Knowledge Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graph, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs, Graphs,</td></tr><tr><td>Natural Language Processing Technology</td><td>Semantic Analysis, Deep Learning, Knowledge Graphs</td><td rowspan="3">Mine opinions and emotions from learners&#x27; expressions, extract potential cognitive development information. Collect physiological data such as eye movement and EEG from learners to provide diverse physiological information support for learning analysis research. Monitor learners&#x27; behavior on intelligent teaching platforms (such as searching, browsing, watching, testing, etc.), and analyze learning preferences and knowledge mastery.</td></tr><tr><td>Physiological Information Recognition Technology</td><td>EEG Sensing, Eye Tracking, Galvanic Skin Response, Hormone Secretion Detection</td></tr><tr><td>Platform Data Collection Technology</td><td>Online Learning Data Collection and Analysis</td></tr></table>

other behavioral indicators during the learning process. Using speech recognition technology, spoken language data from students' classroom participation is collected and analyzed to assess their cognitive development and emotional states. Through natural language processing technology, the semantic content of learners' expressions is deeply mined and analyzed to extract potential viewpoints and emotional information. Physiological information recognition technologies, such as electroencephalography (EEG) and eye tracking, enable the collection of data on learners' eye movements, EEG patterns, skin conductance, and hormone secretion levels, providing diverse physiological data support for research on learning analytics. Meanwhile, platform data collection technology monitors learners' activities on educational platforms, generating online learning datasets to analyze their learning preferences, and knowledge mastery. These intelligent analytical methods allow for accurate assessment of students' deep cognitive and emotional states, facilitating personalized learning analysis.

# 4.3. Personalized learning system framework design

This article proposes a personalized learning system based on multimodal data fusion technology. The system consists of five layers: the acquisition layer, analysis layer, fusion layer, visualization feedback layer, and evaluation layer (as shown in Fig. 2). This model leverages principles such as event monitoring, emotion recognition, and physiological feature monitoring to collect multimodal data across four dimensions: behavior, physiology, human- computer interaction, and learning context. By adopting a modal- level fusion strategy, the model integrates and analyzes heterogeneous data from multiple sources, enabling multidimensional analysis of learners' personalized feature data. This approach identifies learners' unique states and provides personalized learning services that cater to their individual needs.

(1) Acquisition layer: implementing multimodal data acquisition Data collection has shifted from subjective evaluation methods, such as manual observation and self- reporting, to objective evaluation methods enabled by smart devices that capture voice, facial expressions, and physiological signals. In various educational scenarios, the focus of multimodal data collection varies. This study comprehensively encompasses four types of multimodal data: behavioral representation, physiological signals, human- computer interaction, and learning context.

- Behavioral Data: Representative data such as voice and body posture are selected. Voice data analyze learners' states using features like tone, language, and contextual information. Body posture reflects learning status through physical expressions.- Physiological Data: Representative data such as heart rate, eye movements, and EEG are chosen. Physiological signals are collected using an extensible IoT architecture that manages lightweight physiological sensors. Data are gathered and stored in a distributed manner via wireless networks.- Human- Computer Interaction Data: Representative data include clicks and facial expressions. Clicks encompass events from mice, touchscreens, and keyboards, recorded through event listeners. Facial expressions are analyzed to infer psychological states based on dynamic muscle changes.- Learning Context Data: Representative types include learning content and academic assessment data. These data capture learners' states through textual information integrated into learning systems.

(2) Analysis layer: implement multimodal information extraction

We use denoising methods such as the Bayesian method, random forest, and principal component analysis to preprocess representative multimodal data, including facial expressions, body posture, physiological information, text, and speech, in preparation for multimodal data analysis. The technologies and collection devices for analyzing various modal data are as follows (shown in Table 5):

![](images/06bca9a695cbe5a23dfdd6a2b417848c48f6046270696e9d687ed51fedc762f3.jpg)  
Fig. 2. Framework of multimodal data fusion personalized learning system.

Table 5 Multimodal data types, technologies and collection devices.  

<table><tr><td>Data type</td><td>Specific data content</td><td>Technology</td><td>Collection devices</td></tr><tr><td>Facial Expression Information</td><td>Facial expression data for emotion classification and multimodal emotion recognition</td><td>Vision Transformer (ViT)</td><td>Cameras, RGB-D cameras</td></tr><tr><td>Body Posture Information</td><td>Students&#x27; body movements, gestures and interactions with teachers and peers</td><td>AlphaPose</td><td>Cameras, motion capture systems</td></tr><tr><td>Physiological Information</td><td>Heart rate, eye movement, brainwaves</td><td>Inside Out + 6DoF</td><td>Wearable devices: heart rate monitors, eye trackers, EEG devices</td></tr><tr><td>Text Information</td><td>Students&#x27; text input such as problem descriptions, learning notes, and feedback</td><td>Natural Language Processing (NLP) Technology</td><td>Learning Management Systems (LMS), text analysis tools</td></tr><tr><td>Speech Information</td><td>Speech content and emotion analysis, including responses, tone, and emotional changes</td><td>Speech2Vec</td><td>Microphones</td></tr></table>

Facial Expression Information: The Vision Transformer (ViT) network is used to process facial expression data. The application of ViT in facial expression recognition leverages its powerful global modeling ability and self- attention mechanism to extract rich emotional features from students' facial images. Compared with traditional convolutional neural networks (CNNs), ViT can better capture subtle changes in images when processing facial expressions, especially in tasks such as emotion classification accuracy and multimodal emotion recognition.

Body Posture Information: The AlphaPose model is used to dynamically monitor the body posture of learners. This model analyzes students' movements, gestures, and interactions with other students or teachers in the classroom by accurately locating key points on the human body.

Physiological Information: Wearable devices based on Inside Out + 6DoF technology are used to collect physiological information, such as heart rate, eye movements, and brainwaves, from learners.

Text Information: Using natural language processing (NLP) technology, we analyze and understand learners' text input, including their reactions during the learning process, problem descriptions, notes, forum discussions, homework feedback, and more. These textual data reflect important information such as students' cognitive status, emotional changes, and knowledge mastery.

Speech Information: The Speech2Vec model is used, a deep learning model that combines speech and text information. This model is designed to convert speech signals into vector representations for further tasks such as speech understanding, sentiment analysis, and speech recognition. It extends traditional text embedding models, like Word2Vec, by capturing the speaker's acoustic features through embedded representations of speech features without relying on a large amount of annotated data.

# (3) Fusion layer: implement multimodal data fusion

Data fusion refers to the integration and analysis of processed multimodal data, which is a challenging aspect of multimodal learning analysis. Data fusion can be performed in both spatial and temporal dimensions, specifically through semantic fusion and data alignment, by establishing a fusion model to enable mutual verification across multimodal data. Compared to traditional single- mode data, the value of data fusion lies mainly in improving the accuracy of measurement results and enhancing the comprehensiveness of information through mutual verification of multi- source data. This leads to more valuable conclusions, which is crucial for personalized learning modeling through multimodal data fusion.

Based on the analysis of the advantages, disadvantages, and applicable scenarios of multimodal fusion strategies presented earlier, we adopt a modal- level fusion strategy. During the data fusion phase, we utilize the Cross Attention module to dynamically capture the interaction relationships between modalities. By using the Transformer encoder for feature fusion, a multi- layer structure is established to integrate multimodal feature data. This approach captures the potential interaction relationships between modalities, achieves complementary information between them, generates unified fusion features, and facilitates joint distribution learning. Ultimately, this process provides precise guidance for personalized learning, laying the foundation for tailored learning experiences for learners.

(4) Visual feedback layer: provide personalized learning decision support

A visual analysis report is generated based on the results of multimodal data fusion analysis, offering feedback to all parties involved in the learning system. This layer assists in monitoring and managing the learning process, promotes the rational allocation of teaching resources, and provides decision support for participants in learning activities. For example, it enables personalized learning services for students throughout the entire learning process. For preschool students, it can provide personalized learning path navigation. During the learning process, it offers adaptive learning behavior warnings and personalized resource recommendations. After learning, it generates an electronic portfolio that includes subjective and objective factors such as personal information, academic performance, learning preferences, and learning attitudes, serving as a foundation for personalized evaluation and feedback. For teachers, the system provides decision- making support for differentiated teaching design, targeted learning analysis, and media application. Teachers can leverage visual feedback to deliver personalized teaching services that align with the diverse learning styles of their students.

(5) Evaluation layer: dynamic Intervention in the learning process

The results of data fusion provide insights into students' learning status, preferences, and abilities. For instance, facial expressions captured from video, conversations between teachers and students recorded through audio, and physiological data such as skin conductance and heart rate obtained via sensors collectively reflect students' emotional, cognitive, and behavioral engagement, offering a comprehensive characterization of learners' states.

Based on the decision analysis results, all stakeholders can provide personalized services to learners, facilitating the application and implementation of personalized learning systems. Online education platforms can deliver real- time behavior warnings to learners, enhancing their online learning experience and improving the effectiveness of online teaching. Educational institutions can support self- directed learning by offering learning aids, language learning applications, and customized learning content. Personalized learning recommendations can be tailored to users' needs. Teachers can leverage multimodal data analysis results to develop more targeted teaching strategies and content, thereby improving overall teaching quality. Additionally, special education departments can use these insights to design personalized learning plans for students with special needs, addressing their unique learning requirements and supporting advancements in the field of special education.

# 4.4. The implementation direction of intelligent education research driven by multimodal data

Intelligent education, as a key element of educational modernization, presents new opportunities for advancing personalized, digital, equitable, and evidence- based education. With the continuous evolution

of technology and the refinement of educational paradigms, intelligent education powered by multimodal data is poised to profoundly influence personalized learning, teaching reforms, educational equity, and the development of intelligent teaching aids.

(1) Promoting personalized learning

A core trend in intelligent education is the realization of truly personalized learning. By analyzing learners' behavioral data, emotional states, and cognitive levels throughout the learning process, a comprehensive learning profile can be constructed for each individual. This enables the dynamic generation of personalized learning paths tailored to their abilities, interests, and needs. For instance, recommendation systems powered by deep learning can suggest appropriate learning resources based on a learner's knowledge mastery, ensuring that every student progresses at their own pace in a manner best suited to them. Personalized learning provides additional support for learners facing challenges and introduces more complex tasks as their abilities improve, thereby stimulating interest in learning. It also addresses the difficulties in implementing "teaching according to students' aptitude" in traditional education systems. Additionally, intelligent education systems can dynamically adjust teaching strategies in response to learners' emotional feedback. For example, incentive mechanisms can be introduced, or content difficulty adjusted, when learners experience setbacks or display signs of fatigue. This ensures an adaptive learning environment that maximizes engagement and effectiveness.

(2) Promoting data-driven teaching reform

In the future, intelligent education will increasingly depend on data analysis technologies to enhance teaching quality and efficiency. By collecting and analyzing multimodal data, educators can gain a comprehensive understanding of students' learning behaviors, cognitive patterns, emotional changes, and other relevant aspects. These insights not only help identify learners' knowledge gaps but also uncover effective factors in teaching activities, providing a scientific basis for improving instructional strategies. For instance, data mining and machine learning techniques can reveal teaching patterns that are difficult to discern in traditional educational settings. Intelligent teaching analysis enables educational decision- makers to develop more scientifically grounded teaching plans and monitor the effectiveness of educational policies. Data- driven teaching reform will become a critical objective of intelligent education, offering robust support for achieving precision education and fostering a more evidence- based approach to teaching and learning.

# (3) Promoting educational equity

The advancement of intelligent education has opened up new possibilities for achieving educational equity. Leveraging the Internet and intelligent education platforms, students in remote areas can gain access to high- quality educational resources, fostering the sharing of knowledge and opportunities. For instance, online open courses (MOOCs) and virtual laboratories can overcome geographical barriers, enabling students worldwide to participate in renowned courses and access valuable academic resources. Moreover, multimodal data analysis technologies can tailor content to diverse regional and cultural contexts, making education more inclusive. The personalized and automated features of intelligent education systems can help educators identify and support students with special learning needs, thereby advancing the scope and reach of special education. In the future, by providing enhanced intelligent educational resources and personalized tutoring, intelligent education has the potential to address disparities in resource distribution, contributing significantly to the equitable development of education.

(4) Inspiring innovation in educational products and environments

The evolution of intelligent education products is poised to revolutionize the educational landscape. These products will extend beyond traditional textbooks and electronic screens, focusing on the development of educational agents, teaching robots, intelligent guidance systems, and smart classrooms. For instance, educational robots could assist teachers in classroom instruction while providing personalized one- on- one tutoring for students. Smart classrooms, equipped with projectors,

AR devices, smart desks, and chairs, can foster interactive and immersive learning environments, enabling students to deepen their understanding of concepts through hands- on experiences.

Furthermore, intelligent education products will facilitate multimodal data collection, capturing real- time information on learners' behavior, expressions, and body posture. These tools will empower educators to gain a comprehensive understanding of learners' states. For example, smartwatches can monitor physiological data such as heart rate and attention fluctuations, providing insights into students' learning outcomes and emotional states. By integrating such innovative products and learning environments, educational institutions can enhance teaching quality, increase student engagement, and elevate participation in learning activities.

# 5. Reflection and suggestions

This study collects and analyzes multimodal data related to learners and their learning contexts, constructing a personalized learning model using multimodal data fusion to leverage the complementary strengths of diverse data sources. The approach aims to reconstruct the complete learning process and achieve the objectives of personalized learning. Currently, while personalized learning systems based on multimodal data fusion have shown promising initial results, they face significant challenges in scaling up for widespread application. These challenges underscore the need for continuous reflection and improvement to ensure their effective integration into real- world educational systems.

(1) The contradiction between personalized learning and fairness

While multimodal data fusion aims to achieve personalized learning, over- reliance on data- driven algorithms can lead to fairness issues. For instance, variations in learner data across different regions and socioeconomic backgrounds may lead to inadequate support for vulnerable groups in personalized recommendation systems. Additionally, algorithms may inadvertently amplify biases present in the data, resulting in an unequal distribution of learning resources.

To address these challenges, system optimization is essential. Future efforts should include the implementation of fairness evaluation mechanisms and the integration of fairness constraints during the algorithm design phase. These measures can help mitigate the impact of data bias and prevent unfair recommendations, ensuring equitable access to personalized learning resources for all learners.

(2) Privacy and ethical issues

The increasing application of artificial intelligence in education raises critical concerns about data privacy and ethical implications. Monitoring learners' external behaviors and internal physiological signals may infringe upon their privacy. Additionally, using multimodal data to optimize teaching and provide personalized support can inadvertently weaken learners' opportunities for "trial and error" and self- reflection, potentially hindering their exploratory spirit and innovation.

To address these challenges, system optimization must strictly adhere to privacy protection policies to ensure the security of learners' personal information. Privacy risks can be mitigated through techniques like data anonymization and encryption. Furthermore, adopting privacy- enhancing technologies such as differential privacy and federated learning can significantly reduce the likelihood of sensitive data breaches while maintaining the system's functionality and reliability.

(3) Adaptability of technology and education integration

The success of personalized learning systems depends on the adaptability of both teachers and learners. Many educators face challenges in utilizing these systems effectively, particularly in integrating multimodal data fusion technology into their teaching strategies. Additionally, learners' attitudes toward intelligent education systems play a crucial role in the system's effectiveness.

To improve this, it's essential to enhance the digital literacy of both teachers and learners. Providing comprehensive training and guidance can equip educators with the necessary skills to implement multimodal data fusion technologies effectively. Moreover, when designing learning

systems, it's important to prioritize user- friendliness, minimize technical barriers, and encourage the adoption of these systems by making them more intuitive and accessible for both teachers and students.

# 6. Conclusion

In conclusion, the application of personalized learning systems in intelligent education represents a significant advancement towards a more intelligent and individualized approach to education. However, the widespread adoption and implementation of such systems require continuous innovation and a careful balance between fairness, privacy protection, and ethical considerations. Moving forward, it is essential to develop a more open, equitable, and efficient personalized learning system that can genuinely cater to the diverse needs of every learner, ensuring that education becomes more inclusive and tailored to individual growth.

# CRediT authorship contribution statement

Xingle Ji: Conceptualization, Writing - original draft, Data curation, Methodology, Investigation. Lu Sun: Writing - review & editing, Formal analysis, Investigation. Kun Huang: Writing - review & editing, Software, Data curation, Funding acquisition.

# Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

# Acknowledgments

This work was supported by the Zhejiang Province Vocational Education "14th Five Year Plan" Teaching Reform Project: Research and Practice on the Construction of Personalized Teaching Ecosystem Based on Artificial Intelligence Recommendation Algorithm under the Background of Intelligent Education (Project Number: jg20230191), and was supported by the "Peak Talents" program of Huzhou Vocational and Technical College.

# Data availability

Data will be made available on request.

# References

Zheng, Y., Liu, S., & Wang, Y. (2024). The realistic foundation, practical predicaments, and reform direction of educational digitalization in China. China Distance Education, (6).DOI:10.13541/j.cnki.chinade.202306.001. Dong, W., Pan, D., & Kim, S. (2024). Exploring the integration of IoT and Generative AI in English Language education: Smart tools for personalized learning experiences. Journal of Computational Science, 82, Article 102397. https://doi.org/10.1016/j. jocs.2024.102397. Tang, L., Luy, M., Kozouei, S. L., Hatemi, K., Sodan, M., Si, M., & Tsunada, Y. (2019). Integrating multimodal learning analytics and inclusive learning support systems for people of all ages. In Cross- Cultural Design. Culture and Society: 11th International Conference, (pp.469- 481).DOI:10.1100/978- 3- 030- 22580- 3_35. Wang, L., & He, Y. (2020). Online Learning engagement assessment based on multimodal behavioral data. Transactions on Eudaiment XVI, 256- 265. https://doi.org/ 10.1007/978- 3- 662- 61510- 2_25 Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., & Ng, A. Y. (2011). Multimodal deep learning proceedings of the 28th international conference on machine learning ICML'11 2011 Bellevue. Wash, USA, https://people.csail.mit.edu/khosla/ papers/icml2011_ngiam.pdf. Morency, L. P., Bohus, D., Aghajan, H., Nijholt, A., Cassell, J., & Epps, J. (2012). ICMI'12: Proceedings of the ACM SIGCHI 14th international conference on multimodal interaction. In 14th International Conference on Multimodal Interaction, ICMI 2012. Association for Computing Machinery, from https://dl.acm.org/action/ showFmPdf?doi=10.1145%2F2383676.

Huang, X., Kortelainen, J., Zhao, G., Li, X., Moilanen, A., Seppänen, T., & Pietikäinen, M. (2016). Multi- modal emotion analysis from facial expressions and electroencephalogram. Computer Vision and Image Understanding, 147, 114- 124. https://doi.org/10.1016/j.cviu.2015.09.015. Wu, B., Peng, X., & Hu, Y. (2019). Leveraging educational research in a "Post- truth" Era: meeting multimodal narratives to democratize evidence: Review of 2019 AERA annual meeting. Distance Education Journal, 37(4), 13- 23. https://doi.org/10.15881/i.cnki. cn33- 1304/g4.2019.04.002 Xu, W., & Zhou, Y. (2020). Course video recommendation with multimodal information in online learning platforms: A deep learning framework. British Journal of Educational Technology, 51(5), 1734- 1747. https://doi.org/10.1111/bjet.12951 Luo, Z., Jingying, C., Guangshuai, W., & Mengyi, S. (2022). A three- dimensional model of student interest during learning using multimodal fusion with natural sensing technology. Interactive Learning Environments, 30(6), 1117- 1130. https://doi.org/ 10.1080/10494820.2019.1710852 Tang, Y., Bu, F., & Zhao, Y. (2023). Learners multimodal emotion fusion analysis (LMEFA): Motivation, framework and direction. Open Education Research, 29(3), 96- 103. https://doi.org/10.13966/j.cnki.kfrrvj.2023.03.010 Huang, S., Yin, B., & Liu, M. (2017). Research on Individualized Learner Model Based on Context- awareness. In 2017 international symposium on educational technology (ISET) (pp. 163- 167). IEEE. DOI: 10.1109/ISET.2017.45. Yin, Y., Peng, H., & Liu, H. (2023). Research on the construction of smart teaching mode with artificial intelligence technology facilitating education informatization in colleges and universities. Applied Mathematics and Nonlinear Sciences. https://doi. org/10.2478/amns.2023.2.01409 Shi, Y., Peng, H., & Tong, M. (2019). Research on generative paths recommendation strategies of precise personalized learning based on learning profile. China Educational Technology, 5, 84- 91. doi: 10.1066/0601905- 0084- 08. Li, H., Gong, R., Zhong, Z., Xing, L., Li, X., & Li, H. (2023). Research on personalized learning path planning model based on knowledge network. Neural Computing and Applications, 35(12), 8809- 8821. https://doi.org/10.1007/s00521- 022- 07658- 8 Shu, L., Yanbin, Z., Ka, C., Zakariah, S. H., & Ali, N. M. (2023). The establishment of career development and employment guidance course evaluation system based on CIPP model. Journal of Technology and Humanities, 4(1), 41- 48. https://doi.org/ 10.53797/jthkss.v4i1.5.2023 Wang, Y., & Zheng, Y. (2022). Multimodal data fusion: The core driving force to solve the key problems of intelligent education. Modern Distance Education Research, 2, 93- 102. https://doi.org/10.3969/i.issn.1009- 5874.2022.02.011 Si, J., & Fu, Y. (2024). Affective computing for E- learning based on multimodal data fusion. Library and Information, 3, 69- 80. https://doi.org/10.11968/tsyqb.1003- 6938.2024034 Zhao, Y. (2023). Research on Learning Emotion Recognition Through Bimodal Fusion: A Case Study of STEAM Learning analytics. Doctoral dissertation, Fujian Normal University. DOI: 10.27019/d.cnki.gfjvs.2023.000194. Castaneda, D., Esparza, A., Ghamari, M., Soltanpour, C., & Nazeran, H. (2018). A review on wearable photoplethysmography sensors and their potential future applications in health care. International journal of biosensors & bioelectronics, 4(4), 195. https:// doi.org/10.15406/ijbsba.2018.04.00125 Oviatt, S., Schuller, B., Cohen, P. R., Sonntag, D., Potamianos, G., & Krüger, A. (Eds.). (2019). The handbook of multimodal multisensory interfaces. Language processing, software, commercialization, and emerging directions. Association for Computing Machinery and Morgan & Claypool. DOI: 10.1145/3233795. Wang, Y. Y., & Zheng, Y. H. (2021). Educational context awareness for smart classroom: Value positioning, characteristics model and practice framework. E- education Research, 42(11), 84- 91. https://doi.org/10.13811/j.cnki.eer.2021.11.012 Verma, G. K., & Tiwary, U. S. (2014). Multimodal fusion framework: A multiresolution approach for emotion classification and recognition from physiological signals. NeuroImage, 102, 162- 172. https://doi.org/10.1016/j.neuroimage.2013.11.007 Kim, J., & Andre, E. (2008). Emotion recognition based on physiological changes in music listening. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(12), 267- 283. https://doi.org/10.1109/TPAMI.2008.26 Rengonathan, H., Chakraborty, S., & Panchanathan, J. (2016). Multimodal Emotion Recognition using Deep Learning Architectures. In 2016 IEEE Winter Conference on Applications of Computer vision (WACV) (pp. 1- 9). IEEE. DOI: 10.1109/ WACV.2016.777679. Zheng, W. L., Liu, W., Lu, Y., Lu, B. L., & Cichocki, A. (2018). Emotionmeter: A multimodal framework for recognizing human emotions. IEEE Transactions on Cybernetics, 49(3), 1110- 1122. https://doi.org/10.1109/TCYB.2018.2797176. Chanel, G., Kierkels, J. J., Soleymani, M., & Punt, T. (2009). Short- term emotion assessment in a recall paradigm. International Journal of Human- Computer Studies, 67 (8), 607- 627. https://doi.org/10.1016/j.ijhos.2009.03.005 Ringeval, F., Eyben, F., Kroupi, E., Yuce, A., Thiran, J. P., Ebrahimi, T., & Schuller, B. (2015). Prediction of asynchronous dimensional emotion ratings from Audiovisual and physiological data. Pattern Recognition Letters, 66, 22- 30. https://doi.org/ 10.1016/j.patrec.2014.11.007 Oviatt, S., Grafsgaard, J., Chen, L., & Ochoa, X. (2018). Multimodal learning analytics: assessing learners' mental state during the process of learning. In the Handbook of Multimodal- Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition- Volume 2 (pp. 331- 374). DOI: 10.1145/ 3107990.3108003. Tsai, Y. H. H., Liang, P. P., Zadeh, A., Morency, L. P., & Salakhutdinov, R. (2018). Learning factorized multimodal representations. arXiv preprint arXiv:1806.06176. DOI: 10.48550/arXiv.1806.06176.